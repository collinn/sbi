---
output: html_document
---

# Study Design {#ch3}

> "Randomization is too important to be left to chance." - J. D. Petruccelli

[[We have a scientific question, as well as statistical question, and each of these is subtly different from what we will call hypothesis in context of statistics. Should iron out how we want to use these words]]

As mentioned in Chapter 1, statistics is the science that allows us to provide objective, evidenced-based answers to questions that we have about the world. Doing this effectively is tantamount to correctly establishing our inquiry within the statistical framework. Inquiry in hand, we now go about answering the three basic questions in statistics:

1) How should I collect my data?  
2) How should I describe and summarize the data I've collected?
3) What does my data tell me about the way the world works?  

The goal of this chapter is to address the first of these questions. Formally, **study design** is the process of identifying a relevant population, collecting a representative sample, and asking our question in such a way that the answer of this little question will ultimately give us insight to the *scientific question* at hand. Aside: for example, if we are looking at polio vaccine, our science question may be something sciency like "does this vaccine prevent polio" but that has to be translated to the *statistical question*, akin to "is the incidence of polio in control group statistically different than that of the treatment group." Maybe we should say something more about this translation process, but I don't know enough about regular science to do so. 

Also, thinking something like the study design process and challenges are

1. Sample/Population
  i. selection bias
  ii. non-response bias
  iii. Extrapolating sample beyond population
2. Asking question
  i. Confounding bias
  ii. Placebo bias
  iii. Diagnostics and Perception bias
  iv. Randomization

  
## Designing the Study

Mentioned above, the primary goal of statistics is to ultimately provide an answer to a scientific question. Before this can be done, we need to begin by translating the *scientific* question into a *statistical* one. 

The first step of this is to idenfity the collection of individuals about whom the question is asked. For example, when considering questions about the long-term effects of smoking, we will want to limit our investigation to only those people who smoke. For any scientific question that we may ask, the group of individuals we are inquiring about is known as the **population**.

It is often impractical to be able to study every single individual in a population. Instead, we concern ourselves with collecting a **random sample** of the population, chosen in such a way that the random sample is *representative* of the population from which it was collected. By representative, we mean that anything that we can learn about the random sample will very likely also be true about the entire population (of course, we can never be *entirely* certain unless we ask each individual within a population, in which case we are no longer doing statistics).

Sample in hand, we now proceed to the process of *asking* the question. In particular, we want to be confident that we are answering the question that we are intending to ask. Often in medical studies and biostatistics, we concern ourselves with evaluating a new treatment or therapy, and the question is, "Is this treatment effective *within our population of interest*?" This further begs the question: "effective *within our population of interest*, relative to what?" Frequently, the only meaningful way to answer this is to compare treatment or therapy to *something else*. 

For our puposes, this *something else* will be members of the population who have not been exposed to the treatment or therapy. This natural dichotomy suggets two groups into which we may divide our random sample:

1. The **treatment group**, representing the portion of our sample who receives the treatment or therapy
2. The **control group**, representing the portion of our sample who is not treated. This group makes up the *something else* to which we will compare

Just as our sample was collected randomly as to be sure that it was representative of the population, care is needed to be sure that subjects in our sample are **randomly assigned** to either the treatment group or the control. This is critical to be sure that the makeup of each group continues to be representative of the population at large. 

[[maybe only briefly introduce here, then go into more detail in the bias section]]

Having ensured that we have collected a sample that is representative of the population and randomly assigned subjects in this sample to either a treatment or control group, we are well on our way to conducting a successful study. There is, however, one more variable that we need to account for: human psychology. In particular, there are two ways in which human bias (define bias before) can affect the outcome of an experiment:

1. The first issue is with subject bias. Subject bias can result from stuff
2. also experimenter bias

We can effectively control for each of these by concealing from both subjects and experimenters whether subjects were assigned to either the treatment or control group through a process known as **blinding**. something something placebo effect.

Taken together, these procedures help ensure that we are correctly asking the question that we intend to ask. However, as we will see in the next section, there are a number of ways in which [things can still go wrong (havent' introduced bias formally yet)].


[[Illustration?]]  Population -> Sample -> (Treatment, Control) -> experiment 

<div class="definition-container">
<div class="definition"> Definitions </div>
**Population: ** <em> The collection of individuals or objects about which the scientific question is addressed. </em>

**Random Sample: ** <em> A collection of individuals or objects selected at random from a population in such a way that the sample is *representative* of the population </em>

**Treatment Group: ** <em> The portion of our sample who receives the treatment or therapy </em>

**Control Group: ** <em> The portion of our sample who is not treated, used as our baseline comparison </em>

**Random Assignment: ** <em> The process by which members of our sample are randomly assigned to either a treatment or control group </em>

**Double Blind: ** <em> A procedure whereby neither the subject or researcher knows the treatment status of the subject </em>

**Placebo: ** <em> A form of treatment or therapy that is designed to mimic treatment while having no therapeutic value, i.e., sugar pills </em>

</div>

## Bias

What is bias? Like many terms in statistics, the concept of bias has meaning that is not what we mean, etc.,. For our purposes here, we define bias to be any process or technique whereby the actual outcome is systematically different from the true value(s) for which it is intended to estimate. [[words]]

In the context of this textbook, there are primarily two sources of bias, each of which has various types/flavors. The first type of bias is known as **sampling bias**, which describes the various ways in which our collected sample is *not* representative of the population from which it was sampled. The second type, known as **study design bias**, is concerned with the ways in which the study itself can bias the answer to the question we are trying to ask.

### Sampling Bias

Let's begin by considering an ideal world. In such a case, the sampling stage of our study design process might look a little bit like this: 

1. We start with a list of everyone in our population of interest. 
2. Each person in this population is equally easy to sample, and the cost of each is the same
3. We randomly sample this population
4. Everybody sampled is eager to help the statistician answer their question, and nobody refuses to participate

Unfortunately, things are rarely perfect (with a rare exception being the authors of this textbook), and the consequence is that we may end up with a sample that is *not* representative of our population. The ways and magnitudes with which these difference exist are known as **sampling biases**. Fortunately, the most common biases occur in rather predictable ways. Broadly speaking, the two most common forms of sampling bias are **selection bias** and **non-response bias**.

#### Selection Bias

Often, a population of interest is not made up of a single homogenous group of individuals, but rather a heterogenous collection of smaller *sub-populations*. For example, if our population of interest was citizens of the United States, we could easily identify a number of sub-populations that make it up. This could include men and women, children and adults, people in rural or urban environments, etc.,. In order for our sample to be representative of the population, caution must be taken to ensure that these sub-populations are taken into account. **Selection bias** is the phenomenon in which this is not what happens.[[transition sentence to next paragraph]]

The year was 1936, and the United States was still in the midsts of the Great Depression, with nine million people unemployed and real income only two-thirds of it's 1929 equivalent. Franklin Delano Roosevelt was just completing his first term of office of president, and was up for re-election against Republican candidate Alfred Landon. Roosevelt and Landon had different views about how active the government should be in enacting policies to bring the country out of the depression. 

The *Literary Digest* magazine had a custom of anticipating the outcome of the election, having successfully predicted the winner in every presidential election since 1916. Having collected samples from 2.4 million people for the upcoming 1936 election, the *Digest* predicted a landslide victory for Landon: 57% to 43%. The actual election was a landslide indeed, but the victory was to Roosevelt, having collected 62% of the vote to Landon's 38%. Having had such an enormous sample [[can we mention relationship of size and variability?]], how could this poll have been so incredibly far off?

In conducting it's poll, the *Digest* mailed 10 million questionnaires to addresses that it had collected from telephone books and club membership rosters. At a time when only one in four households owned a telephone, and club membership was a luxury few could afford, the *Digest* inadvertently tended to screen out of their survey those who were poor. In other words, the sample selected was disproportionately wealthy, resulting in a classic case of selection bias.  

1. Slides mention 10 million surveys and only 2.4 responses, but we don't have any information on the non-response bias other than the fact that some didn't respond.
2. A similar thing happened in the 1948 election with [Thomas Dewey vs Harry Truman](https://medium.com/@ODSC/dewey-defeats-truman-how-sampling-bias-can-ruin-your-model-f4f67989709e) 

#### Non-response Bias

In the previous example, we noted that 10 million questionnaires had been sent out, but only 2.4 million responses were returned. While it would certainly be nice if everybody complied with our requests for information, it is often unrealistic to expect that each inquiry will be returned. In and of itself, this is not a problem. Indeed, so long as non-response is random and the samples collected continue to be representative of the population, our sample population will still be valid. However, what if non-response is not random? In other words, what happens if one sub-group of the population is more likely to respond to a survey than another? More specifically, **non-response bias** is the phenomenon in which willingness to respond to a survey (or selection) is related to the scientific question [[i don't like this paragraph at all]]

In 2017, the University of Iowa conducted the Speak Out Iowa campus climate survey as part of a comprehensive strategy to respond to sexual misconduct, dating violence, and stalking on our campus. All degree-seeking, undergraduate, graduate, and professional students (N=30,458) at the Iowa City and off-campus centers, including those completing online degrees, received an invitation to participate in the Speak Out Iowa survey through an email message sent to their university email address. A total of 6,952 students completed the survey, making up the entirety of our sample. Of those, 68% identified as female, while 32% identified as male.

[[not happy with concluding paragraph either. maybe ask a question, maybe phrase it differently, idk]]

Given that 54% of the total student body idenfies as female, does it seem that our collected sample is representative of the population? In what ways might a student's willingness to respond be related to the outcome of interest? In your opinion, does it appear as if we are dealing with a case of non-response bias?

### Study Design Bias

[[maybe could also move placebo/observer effect here? even though we mention double blind above. perhaps here we can go into more detail]]



#### Extrapolation Bias

What do we call this section?

The previous two examples demonstarted ways in which we might incorrectly move from a target population to a non-representative sample. This final case describes movement in the opposite direction: from a specified sample to a more general population. Nonetheless, the cause of the bias is the same.

The motivation here can be most readily illustrated by considering the issue of pharmaceutical trails and the use of children. For practical, ethical, and economic reasons, clinical trials usually only involve adults - indeed, only about 25% of drugs are subjected to pediatric studies. Physicians, however, are allowed to use any FDA-approved drug in any way that they think is beneficial are are not required to inform parents if the therapy has not been tested on children. 
By way of example, we consider here the drug propofol, a sedative that has consistently proven safe for use in adults. In 1992, however, after several children who received propofol in the ICU died, the British government recommended against using it on patients under the age of 16. Despite this recommendation, propofol continued to be widely used in the U.S.. It wasn't until 2001 that the manufacturers of propofol conducted a randomized, controlled trial, finding that 9.5% of children on propofol died, compared with 3.8% of children on a different sedative. Although the FDA has now added a warning indicating this, the administration of propofol to children in the ICU is still legal, and subject to controversy.

As we can see, the safety of propofol, which had been consistently demonstrated to be effective in adults, failed to generalize to the wider population, which in this case included children under the age of 16. 


#### Confounding

In 1916, a polio epidemic hit the United States, resulting in hundreds of thousands of people, especially children, falling victim to the disease over the next forty years. By the 1950s, several vaccines had been developed. One in particular, developed by Jonas Salk, seemed especially promising based on laboratory studies. In 1954, the Public Health Service organized an experiment to see whether the Salk vaccine would protect children from polio outside of the laboratory. Two million children were selected from the most vulnerable age groups, aged 6 to 9. Of these children, some were vaccinated, some refused treatment, and some were deliberately left unvaccinated.

This, of course, raises the issue of medical ethics, which are always a consideration in medical studies: is it ethical to deliberately leave some children unvaccinated? In other words, as some families ultimately chose to refuse vaccination, would a more ethical design not offer the vaccine to all children, leaving those who refused to serve as controls?

Thinking back to some of the issues raised at the beginning of this chapter, what are some issues that could arise in letting the subjects determine their group assignment? Would we expect the treatment and control groups to both still be representative samples of our population? Or is it possible that that these groups might be different in important ways?

[[I need to consider past/present tense here, and in general]]

As it turns out, higher-income parents were generally more likely to consent to treatment, *and* their children were more likely to suffer from polio. The reason for this latter point is that children from poorer backgrounds were more likely to contract mild cases of polio early in childhood, while still protected by antibodies from their mothers. Consequently, the difference in infection between the treatment and control groups could possibly have been related to parental income, rather than treatment. Family background here is said to be a **confounding** factor, a frequent and potentially major source of bias. 

A *confounder*, also known as a *lurking variable*, is a third variable which is related to both exposure and outcome. Because of this relationship, confounders distort the observed relationship between exposure and outcome.

[[Image]]

In the case of the polio example, then, we have the following pieces from the diagram above

- Exposure = whether the child gets the vaccine or not
- Outcome = whether the child gets polio or not
- Confounder = child's socioeconomic status

Example 2

Many epidemiologic studies have shown that coffee drinkers have an increased risk of lung cancer. However, researchers also noticed that smokers are more likely to drink coffee

[[Image]]

Once researchers *controlled* for smoking status, they no longer found a change in lung cancer risk due to [drinking coffee](https://cebp.aacrjournals.org/content/25/6/951)

---

While there are many complicated analysis techniques that exist to control for confounding, **matching** is a technique used in the study design process, which can be done at either the individual or the group level. 

In a matched study at the individual level, for each subject in the treatment group, a subject matching on potentially confounding variables is also placed. For example, age and sex are both common confounders. If, during the process of enrolling patients, a 40 year old male is assigned to the treatment group, the next 40 year old male would then be assigned to the control, or placebo, group.

When used at the group level, matching seeks to ensure that the *distributions* of the counfoundning variables are the same in each group. Consider again the coffee/cancer example, matching at the group level would ensure that a similar proportion of smokers are included in the coffee drinking group as in the non-coffee drinking group.

#### Maybe cognitive bias? placebo stuff?

<div class="definition-container">
<div class="definition"> Definitions </div>

**Bias: ** <em> definition </em>

**Sampling Bias: ** <em> A type of bias in which the collected sample *is not* representative of the study population </em>

**Selection Bias: ** <em> A type of sampling bias in which subgroups of the population were more likely to be included than others </em>

**Non-response Bias: ** <em> A type of sampling bias in which the probability of being sampled is related to the scientific question. In this case, the sampling is no longer completely random </em>

**Study Design Bias: ** <em> definition </em>

**Extrapolation Bias: ** <em> definition </em>

**Confounding: ** <em> definition </em>

</div>

## Review

We began this chapter with the goal of answering the question, "How should I collect my data?" As this relates to  our research/scientific question, this consists of something

1. Correctly identifying the population and collecting a random, representative sample
2. Conducting our study so as to be free from bias

