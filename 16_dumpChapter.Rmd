---
output: html_document
---

# Dump Chapter {#ch16}


::: {.infobox .hazard data-latex="hazard"}
PRACTICE SAFE STATISTICS

Don't drink and derive
:::



## Introduction dump {-}


learn more statistics [here](https://google.com)


```{tikz example, fig.cap = "Funky tikz", fig.ext = 'png', echo = F}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\definecolor{zzffzz}{rgb}{0.6,1,0.6}
\definecolor{ffcctt}{rgb}{1,0.8,0.2}
\definecolor{yqyqdz}{rgb}{0.5019607843137255,0.5019607843137255,0.8509803921568627}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-4.505289256198347,-6.316528925619829) rectangle (21.02198347107439,13.083471074380155);
\draw [line width=2pt,pattern color=yqyqdz,fill=yqyqdz,fill opacity=0.25] (2,0) circle (3cm);
\draw [line width=2pt,pattern color=ffcctt,fill=ffcctt,fill opacity=0.25] (6,0) circle (3cm);
\draw [line width=2pt,pattern color=zzffzz,fill=zzffzz,fill opacity=0.25] (4,3.46415) circle (3cm);
\end{tikzpicture}
```

After index.Rmd, bookdown loads subsequent Rmd files in alphanumeric order, so 01 comes before 02, etc

Here we have a chapter, we can have sections that are just like standard markdown

<p style="color:red; border-style:inset; border-color:powderblue"> I'm not sure how to load css classes yet, but it can't be that hard </p>


<!-- # Introduction {.para1 -} <- this adds para1 class to introduction--> 

First note - Having `{-}` in the heading skips numbering assignment (which can be customized in _bookdown.yml), so that `Chapter 1` doesn't read  as `2 Chapter 1`

<p class="para1">this is a test</p>
<div class="para1"> hello </div>

This is a cool book that we are going to make, and it's going to have information about statistics

index.Rmd is the first file loaded, similar to HTML use of index.html

Do note that

- This book uses markdown, so we can use *markdown* **notation** when writing, or <u> html tags</u> 
  + That includes sublists too
    - forever
      * but not too many
- It can be published in a lot of ways, but [this book](https://bookdown.org/yihui/bookdown/) suggests we focus on HTML first since pdf can be wonky and change format frequently
- We can use math too if we want. sex  $\rightarrow$ $\int e^X$

Since this is done in Rstudio with markdown, we can build changes by just pushing Knit at the top (Ctrl+Shift+K !). However, if we add a new file/chapter, we have to, from the console, input

```{r, eval = FALSE}
library(magrittr)
bookdown::render_book('index.Rmd', 'bookdown::gitbook')
```
(or whichever format we want. When closer to finshed, we can make a script to compile finished versions in multiple formats)

Things you'll need:

```{r, eval=FALSE}
install.packages(c("knitr", "bookdown", "servr", "shiny", "rmarkdown"))
```


https://www.w3schools.com/css/css_selectors.asp

https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html




### What is a simulation? {#ch15_s5}

There are three basic questions in statistics:  

1) How should I collect my data?  
2) How should I describe and summarize the data I've collected?
3) What does my data tell me about the way the world works?  

Throughout this book, we will cover different ways of analyzing data and making 
conclusions about the way the world works. Depending on the type of data, there 
are different methods of analysis.



Many times in this book, we will illustrate or investigate a concept with a **simulation**. A simulation is a computer generated experiment where the user gets to specify the true mechanism for generating data. For example, we *could* flip a fair coin over and over again and record whether each flip results in heads or tails. OR, we could tell a computer to simulate a pre-specified number of flips with the probability of heads being 50% and get our results in a fraction of a second.

```{r}
flipCoin <- function(nFlips) {
    dat <- rbinom(nFlips, 1, 0.5)
    dat <- ifelse(dat == 1, 'Heads', 'Tails')
    dat
}
flipCoin(5)
```

Simulations are great tools for understanding the concept of randomness. For example, every time we simulate coin flips using the tool above, we will get a different data set:

```{r}
flipCoin(5)
flipCoin(5)
flipCoin(5)
flipCoin(5)
flipCoin(5)
```

This is exactly what happens when scientists conduct a research study. If they were carry out a study in exactly the same way - sampling individuals from the same population and measuring the same quantities, they would get different results every time. This is because while each sample is from the same population, each sample will contain different individuals. Each sample is random!



## Data summaries dump

### Outline

- Introduction
- Summaries (this maybe unnecessary?)
  - What is a plot/figure?
  - What is a table?
- Types of Data
  - Categorical
  - Continuous
- Categorical
  - visual summary
  - numerical summary 
- Continuous
  - Visual summary
  - Numerical Summary
- Summary of Info

Headers in this document need a LOT of work


### Summaries

Before getting into some of the summaries themselves, it will be helpful to ask, "What does it mean to summarize our data?" By analogy, we may consider the process of summarizing a chapter from a book. While a reader will surely know every minute detail there is to know by reading the chapter themselves, the most salient aspects can be recovered by recounting the list of characters and major events. So it is with data that often 



We can also have R code

```{r sampHist, include = FALSE}
normPop <- rnorm(1e5, mean = 10, sd = 20)

x <- sample(normPop, size = 5, replace = TRUE)
y <- sample(normPop, size = 100, replace = TRUE)

hist(normPop)
abline(v = 10, col = "blue", lwd = 2)
abline(v = mean(x), col = 'red', lty = 2, lwd = 2)
abline(v = mean(y), col = 'green', lty = 2, lwd = 2)
```

But that's boring and dumb when we can also have Shiny, though a few things of note, I guess

- While rmarkdown can host a self contained shiny app, bookdown requires a url. This is not ideal, and I will investigate if there is something that can be done about this
- Ok, whatever, I'm sure the library will host it for us. That would require internet access, though.
- We can still include the actual directories in our repo, or I think we can embed shiny if it is created as a single document instead of chapters
- This is dumb, but the shiny.rstudio website stopped hosting all of their shiny examples (?), and everything else is too complicated
- Oh, I know, we can just use the shiny server for public health!
- "To protect your security, ph-ivshiny.iowa.uiowa.edu will not allow Firefox to display the page if another site has embedded it. To see this page, you need to open it in a new window."
- Here is a way complicated one that takes too long to load. Things I don't like:
  - Small window (maybe can adjust?)
  - Scrolly window
  - Yihui recommends miniUI for embedded shiny apps, but still
  - Not yet sure what other options might exist
  
  

We can also compute the counts and percents for the other variables in our data. 

Sex:

```{r}
data(HairEyeColor)
haireyecolor <- as.data.frame(HairEyeColor)

haireyecolorExpanded <- haireyecolor[rep(1:nrow(haireyecolor), haireyecolor$Freq),]

haireyecolorExpanded <- haireyecolorExpanded[,-4]


# shuffle around
set.seed(1)
haireyecolorExpanded<- haireyecolorExpanded[sample(nrow(haireyecolorExpanded), nrow(haireyecolorExpanded)),]
rownames(haireyecolorExpanded) <- 1:nrow(haireyecolorExpanded)

haireyecolorExpanded$SubjectID <-  1:nrow(haireyecolorExpanded)
haireyecolorExpanded <- haireyecolorExpanded[,c('SubjectID','Sex', 'Hair', 'Eye')]

dt <- haireyecolorExpanded

tab <- table(dt$Sex)
prop <- prop.table(tab)
nn <- names(tab)
tab <- as.numeric(tab)
prop <- as.numeric(prop)
names(tab) <- nn
tab <- rbind(tab, prop*100)
toPrint <- data.frame(t(tab))
colnames(toPrint) <- c('Count', 'Percent')
toPrint$Percent <- paste0(round(toPrint$Percent, 2), '%')
toPrint %>% kable %>% kable_classic(full_width = F)
```

Eye color:

```{r}
tab <- table(dt$Eye)
prop <- prop.table(tab)
nn <- names(tab)
tab <- as.numeric(tab)
prop <- as.numeric(prop)
names(tab) <- nn
tab <- rbind(tab, prop*100)
toPrint <- data.frame(t(tab))
colnames(toPrint) <- c('Count', 'Percent')
toPrint$Percent <- paste0(round(toPrint$Percent, 2), '%')
toPrint %>% kable %>% kable_classic(full_width = F)
```


This kind of counting, as opposted to the raw numbers, is known as **relative frequency**. 



#### Center

Numbers designed to reflect the center of a dataset are called *Measures of Central Tendancy*. There are three popular arithmetic measures used to caputre the center of a ((distribution)), namely

  - Mean
  - Median
  - Mode
  
Before moving onto these in detail, let's take a moment for a brief review of Summation Notation (((Would be cool to have a div element for "Review" or "Btw". We could also include an appendix where all of this is explained in more detail that they can click to visit, with an actual "quick review" done here)))

[[Probably ought to start with N then move to n]]

In general, we will let lowercase $n$ represent the total number of observations that we have collected. We can express these observations as 

$$
x_1, x_2, \dots, x_n
$$
where the subscript values are used to identify each observation. We can represent the sum of all of these observations using the abbreviated *Sigma* notation, 


$$
x_1 + x_2 + \dots + x_n = \sum_{i=1}^n x_i
$$
On the right hand side, note that the value of $x$ has a subscript $x_i$, while the Sigma notation has $i=1$ on the bottom and $n$ on the top. This is to indicate that we are to go through each value of $i = 1, 2, \dots, n$, for each value of $x_i$. In other words, where we might say $x_1$ is the first observation, we would say that $x_i$ is the $i$th observation. This is convenient when we want to describe any $x$ value, but it is not important which one we choose. 

[[Yeah, but what do we mean by 'center' of a distribution?]]

Now, in statistics, we often talk about the mean in two different ways, which we will illustrate here. Consider, for example, the case in which we want to know the mean height of each individual living in the state of Iowa. We will present the number of people in Iowa, also known as the $population$, as $N$. That is, there are $N$ people living in Iowa. The mean of this entire population, known as the *population mean* is designated by the Greek later $\mu$ (called *mu*). Mathematically, we would write this as 

$$
\mu = \frac1N \sum_{i=1}^N x_i
$$
Now, finding the height of each person in Iowa is likely to be very difficult, if not impossible. Often, we can get a reasonable estimate of the population mean by taking a sample of people and treating them as an approximation of the entire population. This sample will represent the actual observations we collect, and we will represent this as we did above with the lower case $n$, where generally, $n < N$ (often by quite a bit). The *sample mean*, which we denote $\bar{x}$ (called x-bar), can be expressed similarly

$$
\bar{x} = \frac1n \sum_{i=1}^n x_i
$$
There are two major differences here between these two values that are worth giving a second consideration to

- The population mean has $N$ observations, which is always larger than the $n$ observations in the sample
- While there is only one way to select $N$ observations from $N$ subjects, there are many ways to select $n$ observations from $N$ subjects. For example, if you have five different cookies, there is only one way to select all five cookies, while there are ten different ways to select only three (I know childish example, maybe we could have a picture? Though this also introduces some benefit to non-rigorous introduction of combinations, which could be useful in explaining variance in samples?)

In general, we have that $\bar{x} \approx \mu$, with this approximation being better the closer $n$ is to $N$. This is a topic we will return to shortly. 



As the data that we can collect can come in a variety of different shapes and 
sizes, it is important that we use tools and definitions that will mean the same
thing, regardless of what the data is, or where it comes from. Often, it is 
useful to know how large or small an observation is, relative to all of the others.
We might say, for example, "Captain Public Health is the fourth tallest person 
in the class." Notice how this can mean one thing if Captain Public Health is 
in a class of 500 students, but something entirely different were he in a class 
of only 5.









## Study design dump



[[We have a scientific question, as well as statistical question, and each of these is subtly different from what we will call hypothesis in context of statistics. Should iron out how we want to use these words]]

As mentioned in Chapter 1, statistics is the science that allows us to provide objective, evidenced-based answers to questions that we have about the world. Doing this effectively is tantamount to correctly establishing our inquiry within the statistical framework. Inquiry in hand, we now go about answering the three basic questions in statistics:

1) How should I collect my data?  
2) How should I describe and summarize the data I've collected?
3) What does my data tell me about the way the world works?  

The goal of this chapter is to address the first of these questions. Formally, **study design** is the process of identifying a relevant population, collecting a representative sample, and asking our question in such a way that the answer of this little question will ultimately give us insight to the *scientific question* at hand. Aside: for example, if we are looking at polio vaccine, our science question may be something sciency like "does this vaccine prevent polio" but that has to be translated to the *statistical question*, akin to "is the incidence of polio in control group statistically different than that of the treatment group." Maybe we should say something more about this translation process, but I don't know enough about regular science to do so. 


## Designing the Study

Mentioned above, the primary goal of statistics is to ultimately provide an answer to a scientific question. Before this can be done, we need to begin by translating the *scientific* question into a *statistical* one. 

The first step of this is to idenfity the collection of individuals about whom the question is asked. For example, when considering questions about the long-term effects of smoking, we will want to limit our investigation to only those people who smoke. For any scientific question that we may ask, the group of individuals we are inquiring about is known as the **population**.

It is often impractical to be able to study every single individual in a population. Instead, we concern ourselves with collecting a **random sample** of the population, chosen in such a way that the random sample is *representative* of the population from which it was collected. By representative, we mean that anything that we can learn about the random sample will very likely also be true about the entire population (of course, we can never be *entirely* certain unless we ask each individual within a population, in which case we are no longer doing statistics).

Sample in hand, we now proceed to the process of *asking* the question. In particular, we want to be confident that we are answering the question that we are intending to ask. Often in medical studies and biostatistics, we concern ourselves with evaluating a new treatment or therapy, and the question is, "Is this treatment effective *within our population of interest*?" This further begs the question: "effective *within our population of interest*, relative to what?" Frequently, the only meaningful way to answer this is to compare treatment or therapy to *something else*. 

For our puposes, this *something else* will be members of the population who have not been exposed to the treatment or therapy. This natural dichotomy suggets two groups into which we may divide our random sample:

1. The **treatment group**, representing the portion of our sample who receives the treatment or therapy
2. The **control group**, representing the portion of our sample who is not treated. This group makes up the *something else* to which we will compare

Just as our sample was collected randomly as to be sure that it was representative of the population, care is needed to be sure that subjects in our sample are **randomly assigned** to either the treatment group or the control. This is critical to be sure that the makeup of each group continues to be representative of the population at large. 

[[maybe only briefly introduce here, then go into more detail in the bias section]]

Having ensured that we have collected a sample that is representative of the population and randomly assigned subjects in this sample to either a treatment or control group, we are well on our way to conducting a successful study. There is, however, one more variable that we need to account for: human psychology. In particular, there are two ways in which human bias (define bias before) can affect the outcome of an experiment:

1. The first issue is with subject bias. Subject bias can result from stuff
2. also experimenter bias

We can effectively control for each of these by concealing from both subjects and experimenters whether subjects were assigned to either the treatment or control group through a process known as **blinding**. something something placebo effect.

Taken together, these procedures help ensure that we are correctly asking the question that we intend to ask. However, as we will see in the next section, there are a number of ways in which [things can still go wrong (havent' introduced bias formally yet)].


[[Illustration?]]  Population -> Sample -> (Treatment, Control) -> experiment 

<div class="definition-container">
<div class="definition"> Definitions </div>
**Population: ** <em> The collection of individuals or objects about which the scientific question is addressed. </em>

**Random Sample: ** <em> A collection of individuals or objects selected at random from a population in such a way that the sample is *representative* of the population </em>

**Treatment Group: ** <em> The portion of our sample who receives the treatment or therapy </em>

**Control Group: ** <em> The portion of our sample who is not treated, used as our baseline comparison </em>

**Random Assignment: ** <em> The process by which members of our sample are randomly assigned to either a treatment or control group </em>

**Double Blind: ** <em> A procedure whereby neither the subject or researcher knows the treatment status of the subject </em>

**Placebo: ** <em> A form of treatment or therapy that is designed to mimic treatment while having no therapeutic value, i.e., sugar pills </em>

</div>

  
### Bias

What is bias? Like many terms in statistics, the concept of bias has meaning that is not what we mean, etc.,. For our purposes here, we define bias to be any process or technique whereby the actual outcome is systematically different from the true value(s) for which it is intended to estimate. [[words]]

In the context of this textbook, there are primarily two sources of bias, each of which has various types/flavors. The first type of bias is known as **sampling bias**, which describes the various ways in which our collected sample is *not* representative of the population from which it was sampled. The second type, known as **study design bias**, is concerned with the ways in which the study itself can bias the answer to the question we are trying to ask.


Often, a population of interest is not made up of a single homogenous group of individuals, but rather a heterogenous collection of smaller *sub-populations*. For example, if our population of interest was citizens of the United States, we could easily identify a number of sub-populations that make it up. This could include men and women, children and adults, people in rural or urban environments, etc.,. In order for our sample to be representative of the population, caution must be taken to ensure that these sub-populations are taken into account. **Selection bias** is the phenomenon in which this is not what happens.[[transition sentence to next paragraph]]


The year was 1936, and the United States was still in the midst of the Great Depression, with nine million people unemployed and real income only two-thirds of it's 1929 equivalent. Franklin Delano Roosevelt was just completing his first term of office of president, and was up for re-election against Republican candidate Alfred Landon. Roosevelt and Landon had different views about how active the government should be in enacting policies to bring the country out of the depression. 

The *Literary Digest* magazine had a custom of anticipating the outcome of the election, having successfully predicted the winner in every presidential election since 1916. Having collected samples from 2.4 million people for the upcoming 1936 election, the *Digest* predicted a landslide victory for Landon: 57% to 43%. The actual election was a landslide indeed, but the victory was to Roosevelt, having collected 62% of the vote to Landon's 38%. Having had such an enormous sample [[can we mention relationship of size and variability?]], how could this poll have been so incredibly far off?

In conducting it's poll, the *Digest* mailed 10 million questionnaires to addresses that it had collected from telephone books and club membership rosters. At a time when only one in four households owned a telephone, and club membership was a luxury few could afford, the *Digest* inadvertently tended to screen out of their survey those who were poor. In other words, the sample selected was disproportionately wealthy, resulting in a classic case of selection bias.  

1. Slides mention 10 million surveys and only 2.4 responses, but we don't have any information on the non-response bias other than the fact that some didn't respond.
2. A similar thing happened in the 1948 election with [Thomas Dewey vs Harry Truman](https://medium.com/@ODSC/dewey-defeats-truman-how-sampling-bias-can-ruin-your-model-f4f67989709e) 

In 2017, the University of Iowa conducted the Speak Out Iowa campus climate survey as part of a comprehensive strategy to respond to sexual misconduct, dating violence, and stalking on our campus. All degree-seeking, undergraduate, graduate, and professional students (N=30,458) at the Iowa City and off-campus centers, including those completing online degrees, received an invitation to participate in the Speak Out Iowa survey through an email message sent to their university email address. A total of 6,952 students completed the survey, making up the entirety of our sample. Of those, 68% identified as female, while 32% identified as male.

[[not happy with concluding paragraph either. maybe ask a question, maybe phrase it differently, idk]]

Given that 54% of the total student body identifies as female, does it seem that our collected sample is representative of the population? In what ways might a student's willingness to respond be related to the outcome of interest? In your opinion, does it appear as if we are dealing with a case of non-response bias?

By way of example, we consider here the drug propofol, a sedative that has consistently proven safe for use in adults. In 1992, however, after several children who received propofol in the ICU died, the British government recommended against using it on patients under the age of 16. Despite this recommendation, propofol continued to be widely used in the U.S.. It wasn't until 2001 that the manufacturers of propofol conducted a randomized, controlled trial, finding that 9.5% of children on propofol died, compared with 3.8% of children on a different sedative. Although the FDA has now added a warning indicating this, the administration of propofol to children in the ICU is still legal, and subject to controversy.

As we can see, the safety of propofol, which had been consistently demonstrated to be effective in adults, failed to generalize to the wider population, which in this case included children under the age of 16. 

In 1916, a polio epidemic hit the United States, resulting in hundreds of thousands of people, especially children, falling victim to the disease over the next forty years. By the 1950s, several vaccines had been developed. One in particular, developed by Jonas Salk, seemed especially promising based on laboratory studies. In 1954, the Public Health Service organized an experiment to see whether the Salk vaccine would protect children from polio outside of the laboratory. Two million children were selected from the most vulnerable age groups, aged 6 to 9. Of these children, some were vaccinated, some refused treatment, and some were deliberately left unvaccinated.

This, of course, raises the issue of medical ethics, which are always a consideration in medical studies: is it ethical to deliberately leave some children unvaccinated? In other words, as some families ultimately chose to refuse vaccination, would a more ethical design not offer the vaccine to all children, leaving those who refused to serve as controls?

Thinking back to some of the issues raised at the beginning of this chapter, what are some issues that could arise in letting the subjects determine their group assignment? Would we expect the treatment and control groups to both still be representative samples of our population? Or is it possible that that these groups might be different in important ways?

[[I need to consider past/present tense here, and in general]]

As it turns out, higher-income parents were generally more likely to consent to treatment, *and* their children were more likely to suffer from polio. The reason for this latter point is that children from poorer backgrounds were more likely to contract mild cases of polio early in childhood, while still protected by antibodies from their mothers. Consequently, the difference in infection between the treatment and control groups could possibly have been related to parental income, rather than treatment. Family background here is said to be a **confounding** factor, a frequent and potentially major source of bias. 

A *confounder*, also known as a *lurking variable*, is a third variable which is related to both exposure and outcome. Because of this relationship, confounders distort the observed relationship between exposure and outcome.

[[Image]]

In the case of the polio example, then, we have the following pieces from the diagram above

- Exposure = whether the child gets the vaccine or not
- Outcome = whether the child gets polio or not
- Confounder = child's socioeconomic status

Example 2

Many epidemiologic studies have shown that coffee drinkers have an increased risk of lung cancer. However, researchers also noticed that smokers are more likely to drink coffee

[[Image]]

Once researchers *controlled* for smoking status, they no longer found a change in lung cancer risk due to [drinking coffee](https://cebp.aacrjournals.org/content/25/6/951)

While there are many complicated analysis techniques that exist to control for confounding, **matching** is a technique used in the study design process, which can be done at either the individual or the group level. 

In a matched study at the individual level, for each subject in the treatment group, a subject matching on potentially confounding variables is also placed. For example, age and sex are both common confounders. If, during the process of enrolling patients, a 40 year old male is assigned to the treatment group, the next 40 year old male would then be assigned to the control, or placebo, group.

When used at the group level, matching seeks to ensure that the *distributions* of the confounding variables are the same in each group. Consider again the coffee/cancer example, matching at the group level would ensure that a similar proportion of smokers are included in the coffee drinking group as in the non-coffee drinking group.


In the previous example, we noted that 10 million questionnaires had been sent out, but only 2.4 million responses were returned. While it would certainly be nice if everybody complied with our requests for information, it is often unrealistic to expect that each inquiry will be returned. In and of itself, this is not a problem. Indeed, so long as non-response is random and the samples collected continue to be representative of the population, our sample population will still be valid. However, what if non-response is not random? In other words, what happens if one sub-group of the population is more likely to respond to a survey than another? More specifically, **non-response bias** is the phenomenon in which willingness to respond to a survey (or selection) is related to the scientific question [[i don't like this paragraph at all]]



## Probability dump

If statistics is the science of uncertainty, then probability is the mechanism that allows us to quantify our uncertainty. In other words, probability tells us how likely something is to happen. People talk loosely about probability all the time. For example, "what's the chance of rain tomorrow?" or "how likely is it that drug A is better than drug B?" However, for scientific purposes, we need to be more specific in terms of defining and using probabilities. 

First, let's consider the simple example of flipping a coin. When a coin is flipped, there are two possible outcomes: heads or tails. We can assume the coin is fair, which means we are just as likely to observe heads as we are tails.

<div class="exercise"> Exercises </div>
<div class="exercise-container">

The applet below is designed to use the computer to simulate flipping a fair coin. In the top right, the flip results are shown with a blue "T" indicating the flip resulted in tails and a pink "H" indicating the flip resulted in heads. In the bottom left, the total number of heads and tails across all flips is tallied. In the bottom right, the running total of the proportion of heads is plotted. For example, if we flip a coin three times and observe THH, the running proportion of heads is 0/1=0 after the first flip (T), 1/2=0.5 after the second flip (TH), and 2/3=0.67 after the third flip (THH). The dotted red line on this plot falls at 0.5, which translates to half of the flips resulting in heads. You can set the number of flips you would like to do, and repeat your experiment over and over by clicking the "Flip Coins" button.

</div>
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/singleCoinFlip" width = "100%" height = "800"> 
</iframe>

<div class="exercise-container">
  
<ol>
 <li>Set the number of flips to 3 and click the &quot;Flip Coins&quot; button 10 times. Each time you simulate 3 coin flips, record the ordered outcome (e.g., HHT) in the following table: <br><br>
 <table>
 <tbody>
 <tr>
 <td>1.
 <td>2.
 <td>3.
 <td>4.
 <td>5.
 </tr>
 <tr>
 <td>6.
 <td>7.
 <td>8.
 <td>9.
 <td>10.
 </tr>
 </tbody>
 </table>
 <ol>
 <li>Assuming the order is important (i.e. HHT is not the same as HTH), how many different outcomes did you observe?</li>
 <li>Still assuming the order is important, how many different outcomes are possible? List all possible outcomes you can think of. (This may include outcomes you did not observe in your 10 flips.)</li>
 </ol>
 </li>
 <li>Now set the number of flips to 10 and click the &quot;Flip Coins&quot; button 10 times. Each time you perform an experiment, record the number of heads you observed in the following table:<br><br>
 <table>
 <tbody>
 <tr>
 <td>1.
 <td>2.
 <td>3.
 <td>4.
 <td>5.
 </tr>
 <tr>
 <td>6.
 <td>7.
 <td>8.
 <td>9.
 <td>10
 </tr>
 </tbody>
 </table>
 <ol>
 <li>On any given experiment where the coin is flipped 10 times, how many flips would you expect to result in heads? Why?</li>
 <li>How many times did you see <em>exactly</em> four heads? What about at least two heads?</li>
 </ol>
 </li>
 <li>Now set the number of flips to 100 and click the &quot;Flip Coins&quot; button.
 <ol>
 <li>What happens to the running proportion of heads as the coin is continually flipped?</li>
 <li>Focusing your attention on the plot of the running proportion of heads, click the &quot;Flip Coins&quot; button several times. What do you notice about the plot? What characteristics of the plot stay the same and what differ?</li>
 </ol>
 </li>
 <li>For each of the following number of coin flips, perform one experiment and record the final <em>proportion&nbsp;</em>of heads to observed (e.g., 12 heads out of 20 flips = 12/20 =0.6).<br><br>
 <table>
 <tbody>
 <tr>
 <td>3:
 <td>5:
 <td>10:
 </tr>
 <tr>
 <td>20:
 <td>100:
 <td>500:
 </tr>
 <tr>
 <td>1,000:
 <td>5,000:
 <td>10,000:
 </tr>
 </tbody>
 </table>
 <ol>
 <li>What happens to the proportion of heads you observed as the number of flips increased?</li>
 <li>ANOTHER QUESTION??</li>
 </ol>
 </li>
</ol>

</div>

The previous exercises illustrate several important points about randomness and probability. First, we saw that when we repeated the same experiment, i.e. flipping the coin three times, several different outcomes were possible. And, if you compare your results from 10 experiments to anyone else's, it is unlikely you will have the exact same results. This is because each time we flip the coin, the outcome is random. Since we know the coin is fair, there is a 50% chance of observing heads. We don't always observe exactly 50% of the flips yielding a heads, especially if we only flip the coin a few times. However, when the coin is flipped many, many times (like, 10,000 times), we see the proportion of the flips that result in heads becomes very close to 50%. 

Now that we have some intuition, let's introduce some definitions to help us talk about these concepts. First, a **random process** is any act or process that results in an outcome that cannot be predicted with certainty. The **sample space** of a random process is the set of all its possible outcomes and is often denoted $\mathcal{S}$. Finally, an **event** is an outcome or collection of outcomes from a random process and will always be contained in the sample space. 

To illustrate, consider flipping a fair coin three times. Each act of flipping the coin is random process - the coin might land on heads and it might land on tails. Letting $H$ be shorthand for the flip resulting in heads and $T$ be shorthand for the flip resulting in tails, the sample space can be enumerated as 

$$\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\},$$ 

giving us eight possible results from the three coin flips. Finally, an event could be any single outcome, or any collection of outcomes, based on this experiment. For example, we may have:

* The event of obtaining *exactly* two heads: $\{HHT, HTH, THH\}$  
* The event of obtaining heads on the first toss: $\{HHH, HHT, HTH, HTT\}$    
* The event of obtaining three tails $\{TTT\}$.

We would all agree that the probability of heads when flipping a fair coin is 50% and the probability of rolling a 2 on a 6-sided die is 1/6, but why is that true? Well, if we were to flip a coin many, many times, we would expect half of the flips to result in heads. Similarly, if we roll a 6-sided die over and over, 1/6 of the rolls should result in a value of 2. The big idea is that when we talk about probability, we are thinking about a *long-run frequency* or what will happen if the random process is repeated over and over again under the same conditions. If we think about probabilities in terms of the long-run frequencies, we can define and quantify **probability** as the fraction of time an event occurs if a random process is repeated indefinitely. This means that probabilities are always between 0 and 1, since we can never observed more events than the number of times the process is repeated, e.g. we can never observed 12 heads on 10 coin flips. Clearly, an event with probability 0 is an event that can never occur.

This leads us to some important properties of probabilities:

1) The sum of probabilities for all outcomes in the sample space, $\mathcal{S}$, must equal 1
2) For any event, the probability of that event is the sum of the probabilities for all the outcomes in that event

For the coin flipping example, there are eight possible outcomes. Since each outcome is equally likely (why?) The first property tells us that the probability of any specific outcome (say, $HHH$) is 1/8. The second tells us that the probability of heads on the first toss is $4/8 = 1/2$, since four of the eight possible outcomes have heads on the first toss. These properties underlie a lot of the more complicated formulas and concepts we will cover in this chapter, although we don't always think about them explicitly. 



<div class="definition-container">
<div class="definition"> Definitions </div>
**Random Process: ** <em> An act or process that results in an outcome that cannot be predicted with certainty </em>

**Sample Space: ** <em> The set of all possible outcomes from a random process </em>

**Event: ** <em> An outcome or collection of outcomes from a random process </em>

**Probability: ** <em> The fraction of times an event occurs when a random process is repeated continuously</em>

</div>




We can explore this concept of long-run frequency more with a simulation. For this simulation, we will simulate rolling a 6-sided die 100 times, recording the result of each roll, and then tabulating the proportion of rolls where we observed each side of the die.

```{r}
rollDie <- function(nRolls, seed) {
    set.seed(seed)
    sample(1:6, nRolls, replace = TRUE)
}
barplot(table(rollDie(100, 1)))
prop.table(table(rollDie(100, 1)))
```

As we see, with just 100 rolls, we don't observe exactly the same proportion of rolls landing on each side of the die. The proportions are not exactly 1/6 = 16.7%, because of randomness. However, if we do 1,000,000 rolls, we see things even out:

```{r}
round(prop.table(table(rollDie(1e6, 1))), 3)
```

Now let users play with a similar app illustrating coin flips (or something) and have associated exercises. Maybe a plot of the proportion over time.



## Shiny test


### Regular UI
<!-- frameBorder="0" -->
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/confidenceIntervals/" width = "800" height = "750"> 
</iframe>

### Editing text

```diff
Here are some things that were written that seem fine
- this is maybe something we're not sure about
+ this is an idea or proposal we can discuss?
```

But the actual code above looks like this


    ```diff
    Here are some things that were written that seem fine
    - this is maybe something we're not sure about
    + this is an idea or proposal we can discuss?
    ```


### Idea

Not sure where else to write this - for running a simulation at home. Consider heating a pan, throwing on 10 corn kernels, and figuring out how many have popped after certain number of time. Pretend 3 minutes is 50%. Well, have them do ten for 3 minutes, then count how many popped. Repeat.


## CLT Stuff

https://iwant2study.org/lookangejss/math/ejss_model_GaltonBoardwee/GaltonBoardwee_Simulation.xhtml

Ideas to make things CLT interesting - The Wall TV show.



Questions we can answer at the end of this chapter

1. What is a statistic? What is a parameter? What is the relationship between the two? (Draw a picture would be a cool exercise. Idk how you'd grade it)
2. Why are sampling distributions random? Suppose we wanted to ask the state of Iowa which 
university had the best football team, and we only ask in Iowa City. Would this be a 
random sample? Could we use this sample to make reliable predictions about the entire state? Why or why not?
3. Prove the Central Limit Theorem using characteristic functions for the iid case, assuming finite mean and variance.


Another good way to address the $\bar{X}$ vs $\bar{x}$ is to have something like (&= isn't making them align?)

<div class="latex">
$$
\begin{align*}
\bar{X} &= \{\text{Mean height of} \textit{ Sex and the City } \text{protagonists}\} \\
\bar{x} &= \{\text{Mean height of Carrie and Samantha}\}
\end{align*}
$$
</div>


## Probability distributions dump 


<div class="objective-container">
<div class="objectives"> Learning Objectives </div>
1. Understand how a distribution represents a random process that creates data that is then observed
2. Understand how the parameters of a distribution govern how the data is generated [and with what probability]
3. Be able to identify which distributions underlying a given real world random process.
</div>

In the previous chapter, we introduced the idea of random processes, which are situations where the outcome can not be determined perfectly in advance. We encounter random processes all the time in our lives, from the exact amount of time it takes to get to class from your home, to determining the winner of a football game. In any case, the random process is defined in terms of the *collection of possible events* and their *associated probabilities*. While the number of unique outcomes of a random process may be impossible to count, we will find that many of them have a very similar underlying structure dictating how these events occur. Formally recognizing the properties of these structures, as well as understanding how they can be used to make predictions, motivates the goals of this chapter. 

### Introduction to Probability Distributions 

Most simply, a **probability distribution** (often just called a distribution) is a method for taking a possible event as input, and giving us the corresponding probability as output. The corresponding probability tells us how likely it is that the specific event will occur, out of all of the possible events. A helpful metaphor is to consider a machine that produces these events at random frequencies corresponding to their associated probabilities. That is, if our machine only creates green marbles and red marbles, and the probability of producing a green marble is 75%, then *on average*, our machine will make 3 green marbles for each red one. In this sense, we can think of distributions as "data generating mechanisms" - the distributions govern how the machine which generates data works. 

To continue with the machine metaphor, it will be important for us to distinguish between two machines that are completely different, and two machines that are the same but tuned to different settings. These settings, which we call *distribution parameters*, dictate many aspects of how the machine will generate data, including the range of likely events, how likely these events are to occur, and how much variability we might expect in the events that we observe. To briefly illustrate, we might first consider a *normal distribution*, which is governed by two parameters: the mean value, $\mu$, and the amount of variability, $\sigma^2$. A plot of two normal distributions is given below:

```{r, fig.align='center', echo = FALSE}
curve(dnorm(x, 0, 1), from = -4, to = 13, lwd = 4,
      col = 'steelblue', ylab = 'Probability', xlab = 'X',
      main = "Two Normal Distributions")
curve(dnorm(x, 5, 3), add = TRUE, lwd = 4, col = 'tomato')
legend(x = 6.5, y = 0.35, legend = c(expression(N(mu == 0, sigma^2 == 1)),
                                     expression(N(mu == 5, sigma^2 == 3))),
       lwd = 4, col = c("steelblue", "tomato"))
```

As we can see, these two curves are quite similar, and indeed, the underlying process that created each of them is the same. What is different, however, are the parameters governing how the data were generated. With this in mind, we have three goals for the present chapter

1. Understand how a distribution represents a random process that creates data that is then observed
2. Understand how the parameters of a distribution govern how the data is generated [and with what probability]
3. Be able to identify which distributions underlying a given real world random process.

<div class="definition-container">
<div class="definition"> Definitions </div>
**Probability Distribution: ** <em> A method for assigning probabilities to all possible events </em>

**Distribution Parameters: ** <em> Values associated with a probability distribution that determine how the data is generated </em>

</div>

### Flipping Coins

In the previous chapter, we examined the possible events and associated probabilities with flipping a fair coin three times. In particular, we noted the collection of possible events was given by 

$$\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\},$$
and the respective probabilities for the number of heads were

```{r, echo = FALSE}
## Why does this need cbind?
knitr::kable(cbind.data.frame('# Heads' = 0:3, 'Probability' = c('1/8', '3/8', '3/8', '1/8')), 
             align = 'c')
```

In what ways might we formalize this as a process? We might start by identifying a few details about this experiment. First, we know that we are interested in coin flips, where each flip could be either one of two outcomes. We might also  recognize that in this experiment we flipped a coin three times, with each flip having an equal probability of being Heads as it did Tails. In light of our previous discussion, which of these properties seem characteristic of a more general underlying process, and which of these could be changed while retaining the more general form? If we had flipped the coin 50 times, would the process be fundamentally different? Would this process be different if the probability of Heads was twice that of Tails?

What we have identified above is a data generating mechanism, or distribution, known as the  *binomial distribution*, in which each outcome is one of two states. In this example, the two states were Heads and Tails, but it could just as easily be described as success vs failure, adverse reaction vs non-adverse reaction, death vs non-death, etc.,. Most generally, we will consider the outcome to be either an "event" or a "non-event". 

We next turn our attention the parameters of the distribution. As you may have guessed, there are two parameters associated with the binomial distribution, namely the number of observations (or flips), denoted $n$, and the probability of a particular outcome being classified as an event, denoted $p$. In our coin flipping example, flipping the coin three times gives us a parameter value of $n = 3$. As we were interested in counting the number of Heads, we will call this our "event", and note that it occurs with probability $p = 0.5$. Together, these pieces define everything we need to know about a random process that follows a binomial distribution. Notionally, we write

$$ X \sim Bin(n, p)$$

or, "the random variable $X$ follows a binomial distribution with $n = 3$ and probability of event $p = 0.5$". Our coin flipping example would then be expressed as $X \sim Bin(n = 3, p = 0.5)$, where $X$ is our experiment.

### Functional Representation of Probability Distributions

We turn our attention now to the practical problem of determining how we might relate the idea of a probability distribution to determine the actual probabilities of given outcomes. From our definition above, we see that at a minimum, all we need is a method to assign a probability to a given event; within these bounds, we have a number of options available. 

Perhaps the most direct method of doing so consists of counting each of the possible outcomes by hand and determining their probabilities, which is precisely what was done when we identified $\mathcal{S}$ and then counted the frequency in which different numbers of Heads occurred. This, of course, can become cumbersome quite quickly: with only $n = 3$, we identified a total of 8 separate outcomes. If $n$ were 4, this would increase to 16. One can quickly see the issue when considering an experiment in which the total number of coin flips was equal to $n = 50$. This example was further made easier by the fact that each of the outcomes was equally likely; if the value of $p$ was anything but $0.5$, our task of assigning probabilities to these outcomes would have been significantly more challenging (see Chapter 4).

Another possibility involves the use of simulation, the justification for which is covered in more detail in another chapter. By simulating this experiment a large number of times, we can determine the relative probabilities by counting the relative frequency of each outcome, which saves us the trouble of having to compute them mathematically. Here, we simulate this experiment $N = 10,000$ times, and record the total number of heads in each experiment, dividing by the total number of experiments to get our desired probability

```{r, echo = FALSE}
library(knitr)
library(magrittr)
flipCoin3 <- function() {
  # perform binomial experiment, which returns either 1 or 0
  dat <- rbinom(n = 3, 1, p = 0.5) 
  # Add these up to determine number of events
  sum(dat)
}
nSims <- 10000
simRes <- replicate(nSims, flipCoin3())
matrix(c(0:3, 
         paste0(format(table(simRes), big.mark = ","), "/10,000"), 
         prop.table(table(simRes)),
         c(1,3,3,1)/8), ncol = 4) %>% 
  kable(col.names = c("Number of Heads", "Fraction", "Observed Probability", "True Probability"))
```

Of course as we can see, simulations have their own limitations: they often require a large number of replications, and because of randomness, the observed probabilities will rarely be exactly equal to the true probabilities. Nonetheless, simulations prove to be exceedingly useful when our experiment is complicated, or if a known distribution function for our problem does not exist. 

Our final method for specifying a probability distribution is with the use of a mathematical function, often referred to as a **probability distribution function** (pdf) or **probability mass function** (pmf), with the former reserved for continuous variables and the latter for those that are discrete. The binomial distribution, consisting of discrete outcomes, is specified with a pmf. For $X \sim Bin(n, p)$, we have the following formula:

$$
P(X = x) = \binom{n}{x} p^x (1-p)^{n-x}
$$

where $n$ and $p$ are our distribution parameters, and $X$ can take any of the values $x = 0, 1, \dots, n$. Perhaps new to us here is the leading term in the expression above, $\binom{n}{x}$, called the *binomial coefficient*, which can be written as 

$$
\binom{n}{x} = \frac{n!}{x!(n-x)!}
$$
where $n! = n \times (n-1) \times \dots \times 2 \times 1$ (known as a factorial). In words, we might say $\binom{n}{x}$ as "$n$ choose $x$". While this may seem daunting at first, the need for it is quite reasonable. Consider again our coin flipping experiment, where the possible outcomes were listed as 

$$
\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\}.
$$

If we are interested in determining the probability of observing two Heads, it is of note that there are a number of instances above in which two Heads occurs. We might ask ourselves, "if we have $n = 3$ flips, how many ways might we *choose* $x = 2$ heads?" Writing this with our binomial coefficient, we find that

$$
\binom{3}{2} = \frac{3!}{2!(3-2)!} = \frac{3 \times 2 \times 1}{2 \times 1 \cdot(1 \times 1)} = \frac62 = 3,
$$
and indeed, 3 is precisely the number of outcomes in $\mathcal{S}$ in which two Heads occur. Finally, using the pmf above, we can substitute in our distribution parameters $n = 3$ and $p = 0.5$ to find the distribution function that describes our experiment:

$$
P(X = x) = \binom{3}{x} (0.5)^x (1-0.5)^{3-x}
$$
[Exercise: verify that the probabilities returned by the binomial pmf match the true probabilities in the table above by plugging in values for $X = \{0, 1,2,3\}$. ]

### Plotting the PMF

It is often useful to create a visual representation of a pmf as well. Doing so quickly gives us an idea of where data  tend to aggregate and how the data are dispersed. Below are two plots representing two different sets of parameters for the binomials distribution. What do you notice in how they differ? How are they similar? What impacts do the different parameters have on the distribution of the data?

```{r, fig.align = 'center', echo = FALSE, fig.width=10}
par(mfrow = c(1, 2))
x <- 0:3
y <- dbinom(x, 3, 0.5)
barplot(y, names.arg = x, xlab = 'Number of Heads Observed on Three Flips', ylab = 'Probability', yaxt='n', 
        main = "X ~ Bin(n = 3, p = 0.5)")
axis(side = 2, at = seq(0, 0.45, by = 0.05), pos = c(0, 0), labels = seq(0, 0.45, by = 0.05))

x <- 0:10
y <- dbinom(x, max(x), 0.8)
barplot(y, names.arg = x, xlab = 'Number of Heads Observed on Six Flips', ylab = 'Probability', 
        main = "X ~  Bin(n = 6, p = 0.8)", yaxt='n')
axis(side = 2, at = seq(0, 0.45, by = 0.05), pos = c(0, 0), labels = seq(0, 0.45, by = 0.05))
```

Of particular note here, we recall from the previous chapter that the sum of all possible probabilities must be equal to one. Visually, this is represented by the total area of the bars in our plot. Given that our bars our rectangles, we can find the area by considering that the width of each bar is equal to 1, and it's height is given by the probability of $X = x$, which can be found using the PMF. On the left hand side, for example, from left to right, we have

$$
\begin{align}
\text{Total Area } &= \text{ \{Area of Heads = 0\} + \{Area of Heads = 1\} + } \\
& \quad \ \ \text{\{Area of Heads = 2\} + \{Area of Heads = 3\}} \\
&= (1 \times P(X = 0)) + (1 \times P(X = 1)) + (1 \times P(X = 2)) + (1 \times P(X = 3)) \\
&= 0.125+0.375+0.375+0.125 \\
&= 1
\end{align}
$$
If, say, we are interested in the probability that $X = 2$ or $X = 3$, we can add the area of the two corresponding bars. In this case, we find $P(X = 2, 3) = 0.75$. 

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/binomial/" width = "100%" height = "800"> 
</iframe>

[Include binomial app here. See that clicking bars adds to something. Explain changing settings, clicking bars, selecting different things. Come up with exercises below, i.e., prob that even if n = 8, prob if even n = 4, etc.]

### Measuring Heights

Often, our data will not fit nicely into a finite number of discrete categories, leaving us with *continuous data* that are described with a *probability distribution function*, or *pdf*. As our data does not fit neatly into categorical bins, many of the techniques described above will not work in the same way here. Rest assured, the idea is exactly the same. While we will spare the technical details here, interested readers may consider what follows to be analogous to the cases presented above when the number of "bins" becomes infinite. 

Our motivating example here will consider the process of determining the height of individuals within a population. Unlike the binomial distribution, where the underlying process and associated parameters were easily teased apart, the components here are less obvious, and some care will be needed to identify them. Height data, like many things in the natural world, tend to follow a symmetric distribution, where most observations tend to gather around a mean value, with observations deviating from the mean being equally likely to fall some distance above the mean as below, their frequencies becoming smaller as this distance increases. That is to say, if the mean height of a population is 68 inches, an individual is equally likely to be 67 inches tall as they are 69 inches. Similarly, an individual is equally likely to be 64 inches as they are 82. However, given their proximity to the mean value, an individual is far more likely to be either 67 or 69 inches (1 inch from the mean) than they are to be either 64 or 82 inches (4 inches from the mean).

The process described above describes what is known as a *normal distribution*, colloquially referred to as a "bell curve". There are a number of properties that together characterize a normal distribution

1. There are two parameters for the normal distribution, the mean $\mu$ (pronounced "myu") and the variance $\sigma^2$ ("sigma squared")
2. $\mu$ is the mean, or expected value, and represents the most probable value of the distribution. That is, observations from a normal distribution are more likely to be close to $\mu$ than away from it
3. Observations are equally likely to be the same magnitude above $\mu$ as they are below it. In other words, the distribution is centered around $\mu$. We see this concept expressed in everyday language when we offer estimates of some value: "The cost is 'x', plus or minus 'y'"
4. The second parameter, $\sigma^2$, describes how concentrated values are around the mean. The smaller the value of $\sigma^2$, the more observations that will be close to $\mu$. Likewise, larger values of $\sigma^2$ result in higher dispersion, or more values further away from  $\mu$. 

Notationally, if a random variable $X$ follows a normal distribution with mean $\mu$ and variance $\sigma^2$, we write $X \sim N(\mu, \sigma^2)$. A special case of this that will be explored in following chapters is known as a *standard normal distribution*, which arises when the mean value is $\mu = 0$, and the variances is $\sigma^2 = 1$. This distribution is often written with its own letter $Z$, as in $Z \sim N(0, 1)$. 

Another critical difference between a continuous and discrete random variable is the way in which we determine probability. In the discrete case, we could enumerate all of the events and determine their relative frequency. Alternatively, we could run a simulation and simply count how often each outcome occurred. In the continuous case, however, there is no finite set of possibilities (i.e., somebody could be 68" tall, 68.1" tall, 68.01", ...), and any attempts to enumerate these will only terminate in frustration; we will determine the implications of this below. In the meantime, however, we will rejoice in knowing that a normal distribution can be mathematically represented by it's probability function:

$$
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}}
$$
As we can see, the pdf contains both of the distribution parameters, $\mu$ and $\sigma^2$. As we saw at the beginning of the chapter, different values for these parameters gives us different curves:

```{r, fig.align='center', echo = FALSE}
curve(dnorm(x, 0, 1), from = -4, to = 13, lwd = 4,
      col = 'steelblue', ylab = 'Probability', xlab = 'X',
      main = "Two Normal Distributions")
curve(dnorm(x, 5, 3), add = TRUE, lwd = 4, col = 'tomato')
legend(x = 6.5, y = 0.35, legend = c(expression(N(0, 1)),
                                     expression(N(5, 3))),
       lwd = 4, col = c("steelblue", "tomato"))
```

Of particular interest above, notice how the value of $\mu$ changes where the data are aggregated, and similarly, note how larger values of $\sigma^2$ results in a greater amount of dispersion.

Let's now return to the issue of determining the probability of a particular event. In the discrete case, we saw that we could examine the plot of the pmf and multiply the width of each bin (which was equal to 1), with the height of the bin, given by the pmf While the pdf here does indeed give us the "height", we quickly run into an issue when considering the width: the only way we can have an "infinite" number of bins for each outcome is to assign each bin a width of 0. As such, the probability of any particular event is unintuitively assigned a probability of zero. 

This apparent shortcoming can thankfully be remedied with the tools of calculus. Where discrete observations allow us to take a sum, the analogous case for continuous intervals is satisfied by the use of the integral. As it no longer makes sense to consider the probability of specific events (all of which will be zero), we instead consider the probability that an observation falls within a *range* of events. For example, if we have a random variable with $X \sim N(\mu, \sigma^2)$, the probability that $X > 3$ can be written 

$$
P(X > 3) = \int_3^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}} \ dx
$$
Similarly, if we were curious to know the probability that $X$ was between a range of values, say $2 < X < 5$, we would write

$$
P(2 < X < 5) = \int_2^{5} \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}} \ dx
$$
Fortunately for us today, this no longer need be computed by hand. A number of computational resources are able to compute this for us with minimal effort. 

[Using app below, explore different parameter values. Use slider to select a range of probabilities. Note that the area of interest is highlighted. Do exercises with it]

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/normal/" width = "100%" height = "800"> 
</iframe>


<div class="definition-container">
<div class="definition"> Definitions </div>
**Binomial Distribution: ** <em> A discrete distribution in which there are two possible outcomes, "events" and "non-events". There parameters are $n$, which dictate the number of trials, and $p$, determining the probability of an event </em>

**Normal Distribution: ** <em> A continuous distribution with two parameters that is symmetric about a mean value, $\mu$, with a variance $\sigma^2$. Many real world processes follow a normal distribution.  </em>

**Standard Normal Distribution: ** <em> A special case of the normal distribution, $Z \sim N(0, 1)$ </em>

**Probability Mass Function: ** <em> A probability function used for discrete random variables. The probability of outcomes is given as a sum </em>

**Probability Distribution Function: ** <em> A probability  function used for continuous random variables. The probabilities of outcomes are taken over a range, given as an integral. </em>

</div>

### Other Common Distributions

Having examined in detail both discrete and properties distributions, demonstrated with the binomial and normal distributions, respectively, we consider below a brief overview of other common distributions and their properites. 


### Poisson Distribution

The Poisson distribution, like the binomial, is a *discrete* distribution, in that it concerns itself with count data. Specifically, a Poisson distribution describes the number of independent events that may occur within a fixed interval of time. For example, we may be interested in the number of cars that pass through a busy intersection from noon to 1pm every day, or the number of major floods that occur in an area every 100 years. Perhaps the most famous example of the Poisson distribution comes courtesy of Ladislaus Bortkiewicz, a Russian statistician who, in 1898, showed that the number of Prussian soldiers killed by being kicked by a horse in a twenty year period followed a Poisson distribution (also child suicides, but that's less fun).

The Poisson distribution has a single parameter, $\lambda$, which describes the rate at which events occur, and a random variable following a Poisson distribution may be expressed as $X \sim Pois(\lambda)$ (People who write $X \sim Po(\lambda)$ are heathens). A random variable following a Poisson distribution has the following assumptions:

1. The value of $X$, being a count, can be any non-negative integer, i.e., $0, 1, 2, \dots$ with no upper bound
2. The occurrence of one event in a time interval is independent of another event. One soldier being kicked by a horse has no impact on the probability of another solider being kicked by a horse. 
3. $\lambda$, which may be any number greater than $0$, describes the rate at which events occur 
4. [Two events cannot occur at the exact same time, though they probably don't need this]

The distribution function of a Poisson random variable with rate $\lambda$ can be expressed

$$
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
One surprisingly detail about the Poisson distribution is the relationship between the mean and the variance. For both, we have that $E(X) = Var(X) = \lambda$.

#### Plots for Poisson

As we look at the plot for the Poisson, we will notice one aspect in particular that distinguishes it from the plots of both the binomial and normal distributions: it is no longer symmetric. This is a consequence of the range of values that a Poisson random variable can take on. Whereas a binomial random variable was bounded between $0$ and $n$, the number of trials conducted, and where the normal distribution allowed any real number, the Poisson is bounded below by $0$, while having no theoretical upper bound. Given below is a plot of the distribution with $\lambda = 2$ and $\lambda = 4$ (it's obvious here that choosing a specific value is inadequate. We can replace these plots with distribution exploration apps)

```{r, fig.align = 'center', fig.width = 10, echo = FALSE}
par(mfrow = c(1, 2))
x <- 0:9
y <- dpois(x, lambda = 2)
barplot(y, names.arg = x, xlab = 'Number of Events Observed in Interval', ylab = 'Probability', main = expression(paste("Poisson distribution with ", lambda, " = 2")))

x <- 0:15
y <- dpois(x, lambda = 4)
barplot(y, names.arg = x, xlab = 'Number of Events Observed in Interval', ylab = 'Probability', main = expression(paste("Poisson distribution with ", lambda, " = 4")))
```

Just as with the binomial distribution, we can determine the probability of an event or collection of events by determining the area of the bars in our plot. Below is an interactive app to do stuff. Exercises


<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/poisson/" width = "100%" height = "800"> 
</iframe>



## Regression dump

[[https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/]]

