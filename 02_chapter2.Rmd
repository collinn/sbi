---
output: html_document
---
# Data Summaries and Presentation {#ch2}

> "Numerical quantities focus on expected values, graphical summaries on unexpected values." 
>
> --- John Tukey

<div class="objective-container">
<div class="objectives"> Learning objectives </div>
1. Define data and categorize and identify different types of data
2. Understand and calculate numerical summaries of different data types
3. Learn about different types of graphs and how they can be interpreted
</div>

```{r, include = FALSE, echo = FALSE}
library(magrittr)
library(plyr)
library(dplyr)
library(knitr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(scales)
library(truncnorm)


theme_set(theme_bw())

knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')
```


## Introduction to Data {#ch2_s1}

Data can be virtually anything that is observed and recorded, including how 
tall you are, the color of all the cars in a town, or the time it takes to drive to work. 
Just as the different types of data can vary considerably, so can the amount. 
There is a natural tension between the quantity of data available and our 
abilities to make sense of it. It is difficult to sort through large streams of 
data and make any meaningful conclusions. Instead, we can better understand data 
by condensing it into human readable mediums through the use of *data summaries*,
often displayed in the forms of *tables* and *figures*. However, in doing so,
information is often be lost in the process. A good data summary will seek to 
strike a balance between clarity and completeness of information. The focus of 
this chapter will be on descriptive statistics, utilizing both numerical and 
graphical summaries of various types of data. 

The optimal summary and presentation of data depends on the data's type. There
are two broad types of data that we may see in the wild, which we will call 
**categorical data** and **continuous data**. As the name suggests, categorical data 
(sometimes called *qualitative* or *discrete* data) are data that fall into 
distinct categories. Categorical data can further be classified into two distinct types:

* **Nominal data**: data that exists without any sort of natural or apparent ordering,
  e.g., colors (red, green, blue), gender (male, female), and type of motor vehicle (car, truck, SUV).  
* **Ordinal data**: data that does have a natural ordering, e.g., education 
  (high school, some college, college) and injury severity (low, medium, high)

Continuous data (sometimes called *quantitative* data), on the other hand, are 
data that can take on any numeric value on some interval or on a continuum. 
Examples of continuous data include height, weight, and temperature. Categorical
and continuous data are summarized differently, and we'll explore a number of 
ways to summarize both types of data.



<div class="definition-container">
<div class="definition"> &nbsp; </div>
**Categorical data: ** <em> Data that takes on a distinct value (i.e., falls into categories) </em>

**Continuous data: ** <em> Data that takes on numeric values </em>

**Nominal data: ** <em> A type of categorical data where the categories do not have any apparent ordering </em>

**Ordinal data: ** <em> A type of categorical data where there is a natural ordering </em>
</div>


## Categorical Data {#ch2_s2}

### Basic Categorical Summaries {#ch2_s2_ss1}

Let's begin by considering a dataset of survey responses for 592 students 
responding with their sex, hair color, and eye color. This data includes 
responses from male and female students, with hair colors that are black, brown, 
red, or blond, and eyes that are brown, blue, hazel, or green. Note that these 
are *qualitative* measures, suggesting that are dealing with categorical data. 
Let's take a look at the data for the first 10 subjects.

```{r}
data(HairEyeColor)
haireyecolor <- as.data.frame(HairEyeColor)

haireyecolorExpanded <- haireyecolor[rep(1:nrow(haireyecolor), haireyecolor$Freq),]

haireyecolorExpanded <- haireyecolorExpanded[,-4]


# shuffle around
set.seed(1)
haireyecolorExpanded<- haireyecolorExpanded[sample(nrow(haireyecolorExpanded), nrow(haireyecolorExpanded)),]
rownames(haireyecolorExpanded) <- 1:nrow(haireyecolorExpanded)

haireyecolorExpanded$SubjectID <-  1:nrow(haireyecolorExpanded)
haireyecolorExpanded <- haireyecolorExpanded[,c('SubjectID','Sex', 'Hair', 'Eye')]

dt <- haireyecolorExpanded

kable(haireyecolorExpanded[1:10,], align = 'c')
```

Each row indicates a subject possessing the indicated sex, hair, and eye color. 
For example, the first row indicates a male with brown hair and blue eyes. Trying 
to make sense of 592 such observations is a daunting task, so we can begin by
taking the data we have and summarizing it in a useful way. For categorical data, 
like we have here, summarizing the data is pretty straightforward - you just 
count how many times each category occurs. For example, we can count how many
of each hair color was observed in our data.

```{r}
tab <- table(dt$Hair)
nn <- names(tab)
tab <- as.numeric(tab)
names(tab) <- nn
t(tab) %>% kable %>% kable_classic(full_width = F)
```

This kind of counting is known as **absolute frequency**, which gives us a single 
value indicating the total number of observations. In looking at the table above, 
it is clear that there are far more observations with brown hair than black, 
blond, and red. We gather this information by looking at the total number of 
observations with brown hair, and noticing that this number is quite a bit larger 
than any of the other observations.

However, suppose somebody asks you how common brown hair is relative to other 
colors. Does it make sense to respond, "Oh, there are 286 individuals with 
brown hair"? Without knowing the values for the other hair colors, this number 
alone doesn't carry much meaning. Is 286 observations a lot? It depends. Were 
300 people examined? 3,000? Without knowing anything about the rest of the data, 
the absolute frequency may not be very useful.

In addition to actual counts of observations in categorical data, we may often 
be interested in **rates**. A rate can be as simple as taking the total number 
of a single category observed, and stating it in terms of the total number of 
observations. For example, instead of saying, "286 subjects who were
observed had brown hair," we might instead say, "286 of 592 subjects surveyed
had brown hair." Rates are also known as **relative frequencies**, because
they are relative to a specific number of observations. More commonly, we use
**percentages**, also known as **proportions**, which are a special type of rate/
relative frequency - the count per 100 observations.
We calculate percentages using division. That is, 286 of 592 subjects becomes
$286/592 = 0.4831 = 48.31\%$. We can now show the same table above, this time
in terms of percentages.

```{r}
t(scales::percent(prop.table(tab))) %>%  kable() %>% kable_classic(full_width = F)
```


By considering all observations as rates per one hundred, we can quickly compare 
the relative counts of our observations. For example, we can quickly note that 
about half of the observations collected had brown hair, and almost twice as many 
had blond hair compared to red. 

In addition to tables, we can also summarize categorical data visually. The most
common figure used to represent categorical data is the **bar plot**. Below is a 
demonstration of a bar plot for the percentages of hair color in our data.

```{r}
ggplot(dt, aes(Hair, fill = Hair)) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  geom_text(aes(label = scales::percent((..count..)/sum(..count..)),
                   y= (..count..)/sum(..count..) ), 
            stat= "count", vjust = -.5) +
  scale_y_continuous(labels=scales::percent, limits = c(0, 0.55)) +
  ylab("Percent") + 
  ggtitle("Percentages by Hair Color") +
  scale_fill_manual(values = c('black', 'saddlebrown', 'brown3', 'lightgoldenrod1'))
```


### Advanced Categorical Summaries {#ch2_s2_ss2}

Numerical and visual summaries become even more useful as our data becomes more 
complicated. Let's continue with the data we've been using, but now let us also 
break down observations with each hair color by sex as well. This process is 
known as **stratification**.

```{r}
table2 <- with(dt, table(Sex, Hair))
addmargins(t(table2) , FUN= list(Total = sum), quiet = T) %>% 
  kable() %>% kable_classic(full_width = F)
```

First, we notice that by taking sums across the rows, we arrive at the same 
numbers that we had when only hair color was considered. If we sum vertically 
down the columns, we also get the total number of observations of each sex. Note
that the sum of both margin totals add up to 592, the total number of observations,
as indicated by the bottom right corner. In other words, y stratifying the hair 
color counts by sex, we haven't lost any information related to the hair color,
but we have *added information* to our summary about sex. 

Getting stratified counts is straightforward, however, we now have several ways 
in which we might compute the percentages. For the table above, there are three
ways we could compute percents.

- How many in each category, relative to the entire sample

```{r}
# by population
tt <- prop.table(t(table2)) 
tt <- apply(tt, c(1,2), percent, accuracy = 0.1)
kable(tt, caption = "Relative to population") %>% kable_classic(full_width = F)
```

Here, the percentages are computed by diving the count in each inner cell by the
total sample size, 592. For example, there were 56 male respondents with black 
hair. Relative to the total sample, this means 56/592 = 0.095 = 9.5% of the sample
consists of black-haired males. Adding up all of the percentages gives us $\approx$ 100% (due to rounding to the first decimal place we actually get 100.1% here).

- How many of each hair color, within sex

```{r}
# by hair color
tt <- prop.table(t(table2), margin = 2) %>%
  addmargins(1, FUN= list(Total = sum), quiet = T) 
tt <- apply(tt, c(1,2), percent, accuracy =0.1)
kable(tt, caption = "Proportion of hair color, by sex") %>% kable_classic(full_width = F)
```

Now our table looks very different. The percentages in the inner cells to not add up to 100%. Instead, the percentages have been computed relative to the total number of respondents in each sex. For example, we still have 56 male subjects with black hair, but relative to the total number of male subjects (279), this is 56/279 = 0.201 = 20.1%. There are 52 black-haired female respondents out of 313 total females, which gives us 53/313 = 0.166 = 16.6%. Since the percentages are computed relative to sex, we now see the percentages in *each column* add up to 100%. In other words, we have information about distribution of hair colors within sex.

- How many in each sex, within hair color

```{r}
# by eye color
tt <- prop.table(t(table2), margin = 1) %>% 
  addmargins(2, FUN= list(Total = sum), quiet = T) 
tt <- apply(tt[, -5], c(1,2), percent, accuracy = 0.1)
kable(tt, caption = "Proportion of sex, by hair color") %>% kable_classic(full_width = F)
```

Similarly, we can also look at the relative frequencies of each sex within the 
four hair color categories. We have 56 black-haired males and 52 black-haired 
females, so 56/108 = 0.519 = 51.9% of the black-haired respondents are male and
53/108 = 0.481 = 48.1% are female. Since hair color is given in the rows of our
table, we now have percentages that add up to 100% in each *row*.

All three of these tables are correct and informative. When deciding the best 
way to calculate percentages for tables like this, it will depend on the 
research question and the data at hand. We can also use stratification graphically,
by creating multiple bar plots for each category of the stratification variable.



```{r, fig.width = 10}
ggplot(dt, aes(x=Hair, fill=Sex))+
  geom_bar(aes( y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge" ) +
  geom_text(aes( y=..count../tapply(..count.., ..x.. ,sum)[..x..],
                 label=scales::percent(..count../tapply(..count.., ..x.. ,sum)[..x..]) ),
            stat="count", position=position_dodge(0.9), vjust=-0.5)+
  ggtitle('Percent of Sex within Hair Color') +
  ylab('Percent') +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = c('steelblue2', 'coral'))


ggplot(dt, aes(x=Sex, fill=Hair))+
  geom_bar(aes( y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge" ) +
  geom_text(aes( y=..count../tapply(..count.., ..x.. ,sum)[..x..],
                 label=scales::percent(..count../tapply(..count.., ..x.. ,sum)[..x..]) ),
            stat="count", position=position_dodge(0.9), vjust=-0.5)+
  ggtitle('Percent of Hair Color within Sex') +
  ylab('Percent') +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = c('black', 'saddlebrown', 'brown3', 'lightgoldenrod1'))

```




<div class="definition-container">
<div class="definition"> &nbsp; </div>
**Absolute frequency: ** <em> The number of observations in a category </em>

**Rate/Relative frequency: ** <em> The number of observations in a category relative to any other quantity </em>

**Percent/Proportion: ** <em> The number of observations per 100 </em>

**Bar plot: ** <em> Visualization of categorical data which uses bars to represent each category, 
with counts or percents giving the height of each bar </em>

**Stratification: ** <em> The process of sorting data into categories prior to summarizing </em>
</div>



## Continuous Data {#ch2_s3}

To explore graphical and numerical summaries of continuous data, let's consider
a dataset which contains daily air quality measurements in New York from May to
September 1973. Let's look at the first 10 rows of the data:

```{r}
data("airquality")
aq <- airquality[,-which(colnames(airquality) %in% c('Solar.R'))]
aq <- aq[,c('Month', 'Day', 'Temp', 'Wind', 'Ozone')]
kable(head(aq, 10), align = 'c')
```

This data contains continuous measurements on the air quality:

- maximum daily temperature in degrees Fahrenheit at La Guardia Airport (Temp)  
- average wind speed in miles per hour (mph) at La Guardia Airport (Wind)
- average ozone in parts per billion (ppb) from 1:00pm - 3:00pm at Roosevelt Island (Ozone)

The tables and bar charts we introduced in Section \@ref(ch2_s2) are great summaries when
the data can be categorized and counted. However, with continuous data, we can 
have a potentially infinite number of values and counting each unique value would
not be very informative. Instead, one way we can visualize the data is by first 
creating *bins* which cover an interval of possible values and then counting the
number of observations that fall into each bin. Plotting the absolute or relative
frequency of observations in each bin gives us a **histogram**. A histogram
of the maximum daily temperature from our New York data looks like this:

```{r}
hist(aq$Temp, main = 'Histogram of Maximum Daily Temperature at La Guardia',
     xlab = 'Temperature (degrees F)', ylab = 'Count',
     col = 'salmon1')
```

Histograms provide a nice picture of the *distribution* of the observed data. In
the histogram of temperature, we can readily see that on most days between May and 
September, the maximum temperature at La Guardia was between 75 - 85 degrees. Some
days were particularly chilly, with temperatures below 60 degrees and some days
were quite hot, with temperatures above 90 degrees. When creating a histogram,
it is important to be careful in selecting the size of each bin. Bins that are 
too large can over-simplify the data, but bins that are too small can make the 
figure unnecessarily complex. 

```{r}
par(mfrow = c(1, 2))
hist(aq$Temp, main = 'Too few bins',
     xlab = 'Temperature (degrees F)', ylab = 'Count',
     col = 'salmon1', breaks = 2)
hist(aq$Temp, main = 'Too many bins',
     xlab = 'Temperature (degrees F)', ylab = 'Count',
     col = 'salmon1', breaks = 150)
```


Histograms can come in a variety of shapes and are often described by the number
of peaks shown and by their *skewness*. In
our temperature histogram, we see only one peak, so we call the distribution 
**unimodal**. When there are two peaks, we say the distribution is **bimodal**, 
and for any number of peaks of three or more, we call the histogram **multimodal**.
Looking at the histogram of temperature, we also see that the observed temperatures
are fairly evenly distributed around the peak. When a histogram has equal tails on
both sides of the peak, it is considered to be **symmetric**. Otherwise, the 
histogram is considered to be skewed. We can further describe the skew by the side
of the histogram with a longer tail - if there is a large tail to the right of the 
histogram we call it **skewed right** and if there is a large tail to the left of
the histogram we call it **skewed left**. Let's look at a few different histogram
shapes to illustrate these definitions.

```{r, echo = FALSE, fig.height=5, fig.width = 7}
set.seed(2)
x1 <- rbeta(10000,5,5)
x2 <- rbeta(10000,2,5)
x3 <- rbeta(10000,5,2)
bimodal <- c(rtruncnorm(7500, a=0, b=1, mean=.2, sd=.1),
                 rtruncnorm(4000, a=0, b=1, mean=.75, sd=.1))

histCol <- 'plum1'

par(mfrow= c(2, 2))
hist(x1, col = histCol, 
     main = 'Unimodal and Symmetric', xlab = '', 
     ylab = '', axes = F)
hist(x2, col = histCol, 
     main = 'Unimodal and Skewed Right', xlab = '', 
     ylab = '', axes = F)
hist(x3, col = histCol, 
     main = 'Unimodal and Skewed Left', xlab = '', 
     ylab = '', axes = F)
hist(bimodal, col = histCol, 
     main = 'Bimodal', xlab = '', 
     ylab = '', axes = F)

```

While nothing can replace a picture, sometimes it is preferable to summarize our
data with one or two numbers which characterize the most important information 
about the distribution. Often, we are most interested in the location of the
peak (center) and how spread out the data is around that peak. There are two 
popular methods to describe the center of a distribution:

- Mean
- Median

The **mean** is the most commonly used measure of the center of a distribution. 
Simply enough, the mean can be found by taking the sum of all of the observations,
and dividing by the total number. With $n$ observations, $x_1, x_2, ..., x_n$, 
we can express the mean mathematically in the following way:

$$
\frac{x_1 + x_2 + \dots + x_n}{n} = \frac1n \sum_{i=1}^n x_i
$$
The **median** is another common measure of the center of a distribution. In 
particular, for a set of observations, the median is an observed value that is 
both larger than half of the observations, as well as smaller than half of the 
observations. Formally, we say that the median is the 50th percentile of the 
data. Informally, we say it's the middle value. To find the median, we begin by 
arranging our data from smallest to largest. If the total number of observations,
$n$, is odd, then the median is simply the middle observation. If $n$ is even, 
it is the mean of the middle two.

Examples:

- $1, 2, 2, 3, {\color{red} 5}, 7, 9, 10, 11 \quad \Rightarrow \quad \text{Median} = 5$
- $1,2,2,3, {\color{red} 5}, {\color{red} 6}, 7, 9, 10, 11, \quad \Rightarrow \quad \text{Median} = \frac{(5+6)}{2} = 5.5$

For the New York temperature data, the mean is `r round(mean(aq$Temp), 1)` and 
the median is `r round(median(aq$Temp), 1)`. These values are very similar to 
each other and fall near the peak in the histogram.

```{r}
hist(aq$Temp, main = 'Histogram of Maximum Daily Temperature at La Guardia',
     xlab = 'Temperature (degrees F)', ylab = 'Count',
     col = 'salmon1')
abline(v = mean(aq$Temp), lty = 2, lwd = 2)
abline(v = median(aq$Temp), lty = 3, lwd = 2)
legend('topright', 
       c(paste0(c('Mean', 'Median'), ' = ', round(c(mean(aq$Temp), median(aq$Temp)), 1))),
       lwd = 2, lty = c(2, 3))
```

However, it is not always the case that the mean and median will be similar. Let's
consider an example where we collect $n = 10$ samples of salaries for University 
of Iowa employees:

```{r, echo = FALSE}
sals <- c("$31,176", "$50,879", "$34,619", "$103,000", "$36,549", "$130,000", "$37,876", "$144,600", "$48,962", "$5,075,000")
kable(matrix(sals, ncol = 2), format = "html")
```

For our sample, we find that the mean is \$569,266, but the median is
(48,962 + 50,879) / 2 = \$49,921. It turns out, our sample included the highest
paid university employee - the head football coach. This extremely high salary 
has caused the mean to be very large - larger than 9/10 of the salaries in our 
sample. On the other hand, the median is not impacted by the football coach's 
salary and is thus a much better reflection of the typically University employee's
salary.

This one high salary, which is not representative of most of the salaries 
collected, is known as an **outlier**. From the example above, we have seen that 
the mean is highly sensitive to the presence of outliers, while the median is not. 
Measures that are not sensitive to outliers are known as **robust**. We see, then,
that the median is a robust estimator of the center of the data. 

We have seen an example where the mean and median are quite close, and an example
where they are wildly different. This begs the broader question - when might we 
expect these measures of central tendency to be the same, and when might we expect 
them to differ?


```{r, echo = FALSE, fig.height=5, fig.width = 7}

set.seed(2)
x1 <- rbeta(10000,5,5)
x2 <- rbeta(10000,2,5)
x3 <- rbeta(10000,5,2)
bimodal <- c(rtruncnorm(7500, a=0, b=1, mean=.2, sd=.1),
                 rtruncnorm(4000, a=0, b=1, mean=.75, sd=.1))

histCol <- 'plum1'
lineCols <- c('grey20', 'grey20')
ltys <- c(5, 3)

par(mfrow= c(2, 2))
hist(x1, col = histCol, 
     main = 'Unimodal and Symmetric', xlab = '', 
     ylab = '', axes = F)
abline(v = mean(x1), col = lineCols[1], lwd = 2, lty = ltys[1])
abline(v = median(x1), col = lineCols[2], lwd = 2, lty = ltys[2])

hist(x2, col = histCol, 
     main = 'Unimodal and Skewed Right', xlab = '', 
     ylab = '', axes = F, breaks = 20)
abline(v = mean(x2), col = lineCols[1], lwd = 2, lty = ltys[1])
abline(v = median(x2)-0.04, col = lineCols[2], lwd = 2, lty = ltys[2])

hist(x3, col = histCol, 
     main = 'Unimodal and Skewed Left', xlab = '', 
     ylab = '', axes = F)
abline(v = mean(x3), col = lineCols[1], lwd = 2, lty = ltys[1])
abline(v = median(x3), col = lineCols[2], lwd = 2, lty = ltys[2])

hist(bimodal, col = histCol, 
     main = 'Bimodal', xlab = '', 
     ylab = '', axes = F)
abline(v = mean(bimodal), col = lineCols[1], lwd = 2, lty = ltys[1])
abline(v = median(bimodal), col = lineCols[2], lwd = 2, lty = ltys[2])

legend(-.5,3200, legend = c("Mean", "Median"),
       col = lineCols, lty = ltys, lwd = 2,
       title = "Measure of Center",xpd=NA)

```

When the data is unimodal and symmetric, the mean and median are indistinguishable.
However, when there is skew or multiple peaks, we see the mean and median start 
to differ. When the distribution is skewed, the mean is pulled towards the tail. 
On the other hand, the number of very large/small observations is relatively small,
so the median remains closer to the peak where the majority of data lies. When
the distribution is bimodal, neither the mean or median can capture the distribution
well. In that case, it might be better to summarize each peak individually.


<div class="definition-container">
<div class="definition"> &nbsp; </div>
**Histogram: ** <em> Visualization of continuous data which divides the continuous 
  variable into bins and plots the number of observations in each bin</em>

**Unimodal: ** <em> Characterization of a distribution with one peak</em>
  
**Bimodal: ** <em> Characterization of a distribution with two peaks</em>
  
**Multimodal: ** <em> Characterization of a distribution with three or more peaks</em>
  
**Symmetric: ** <em> Characterization of a distribution with equal tails on both  
  sides of the peak</em>
  
**Skewed right: ** <em> Characterization of a distribution with a large tail to 
  the right of the peak</em>
   
**Skewed left: ** <em> Characterization of a distribution with a large tail to 
  the left of the peak</em>
  
**Mean: ** <em> </em>

**Median: ** <em> </em>

**Mode: ** <em> </em>

**Outlier: ** <em> </em>

**Robust: ** <em> Measures that are not sensitive to outliers </em>
</div>



[[Do we need discussion on mode? Skipping for now]]


### Measures of Spread
This is just getting these on paper. Serious reorganization needs to occur

Suppose that we have two datasets, each with 1,000 observations, and each with both a common mean and median of 100:

```{r, echo = FALSE}
n <- 1e3
set.seed(69)
x1 <- rnorm(n, mean = 100, sd = 10)
x2 <- rnorm(n, mean = 100, sd = 25)
par(mfrow = c(1, 2))
aa <- hist(x1, main = "Dataset 1", breaks = seq(-25, 200, by = 5), ylim = c(0, 210))
abline(v = mean(x1), col = 'blue')
abline(v = median(x1), col = 'green')
bb <- hist(x2, main = "Dataset 2", breaks = seq(-25, 200, by = 5), ylim = c(0, 210))
abline(v = mean(x2), col = 'blue')
abline(v = median(x2), col = 'green')
```


From the histograms above, we visually confirm that both datasets have a mean and median of 100. Despite this, however, it would appear as if these datasets are not quite as similar as one might have thought

- First, we see that for the first dataset, nearly all of the values fall in the range of (50, 150), while for the second dataset, this range appears to be closer to (0, 200)

- Recalling that each dataset has the same number of observations, it appears as if the first dataset has nearly twice as many observations near the mean value of 100 than the second dataset

Suppose, then, you are approached by a researcher who wants to select a single observation from each dataset at random, and you are asked to anticipate what value that observation will have. Without having seen them, the best guess you can offer is our measure of center, which is 100. From which dataset do you suspect the random observation will have a value closer to 100? Which do you suspect will be further away?

What we are observing here is two datasets with a common center that have different *spread* or *dispersion*.

Next to the center, this is the second most important piece of information informing us on the shape of our data. Roughly speaking, the spread of the data informs us of the degree to which our observations tend to lie close to the center or scattered away from it. Just as we have multiple ways to determine the center of a dataset, so we have several methods for quantifying the variability of the data. These numeric summaries are known as *Measures of Dispersion*. Measures that we will investigate here include

- Variance
- Standard deviation
- Range
- Interquartile Range (IQR)

##### Variance and Standard Deviation

In mathematical terms, we describe the **variance** of a dataset to be *the average of the squared differences between each data value and the sample mean*. The **standard deviation** is defined as the *square root of the variance*. 

[[something something clever way of inuiting what variance is]]
[[https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/]]


As before, consider a population with $N$ observations denoted as 

$$ 
x_1, x_2, \dots, x_N
$$
To tie into our definition, note that $(x_i - \mu)$ represents the difference between an observation and the population mean, while $(x_i - \mu)^2$ represents the squared difference. The **population variance**, defined as the average of these squared differences, is denoted as $\sigma^2$, where

$$
\sigma^2 = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2
$$

The **population standard deviation**, being the square root of the population variance, is denoted $\sigma = \sqrt{\sigma^2}$.

Like our measure for the mean, we can have both a population variance, as well as a **sample variance**. That is, suppose we have a sample of $n < N$ observations, 

$$
x_1, x_2, \dots, x_n
$$
The sample variance, denoted $s^2$, is nearly identical to the population variance, with the exception of the fact that we now subtract the sample mean, rather than the population mean

$$
s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
$$
Similarly, the **sample standard deviation** is written as the square root, $s = \sqrt{s^2}$. A few things to be on the lookout for:

- See that the sample variance finds the average by multiplying by $\frac{1}{n-1}$ rather than $\frac1n$. This is because there is no God, and ironically, the world is doomed to end in eternal darkness after being engulfed by our only source of light, the sun

- Be extra careful when finding standard deviations. It is the *square root* of the sum, rather than the square root of terms inside of the sum. that is, 

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2} \not= \frac{1}{N} \sum_{i=1}^N \sqrt{(x_i - \mu)^2} = \ ?? \ _{_{(\text{mean absolute deviation})}}
$$
[[Maybe we should describe high level properties of sd/var before their formulas]]

As we are discussing measures of "spread", it is important to consider the point in our data about which the observations are dispersed. In examining the mathematical definitions of both variance and standard deviation, we see that this point is the mean, as indicated by the $(x_i - \mu)$ term within the expression. Somewhat implicit in this definition [[why?]], we assume that the observations fall symmetrically on both sides of the mean. It is this assumption that gives informative value to expressions like $\bar{x} \pm s$, which implies that an observation is just as likely to be in the interval $(\bar{x} - s)$ as it is in $(\bar{x}+ s)$. I bet a visual would be nice here. Another fun fact to be worked in here is that sd > 0, and only equal zero when all observations are  equal to the mean. This might be a good place to start? If, for example, we assume each $x_i = \bar{x}$, then we can see simultaneously how there is no variation in our data AND how this would result in zero in the expressions above. Also, SD is not robust.

[[shiny app showing sd/var with symmetric/nonsymmetric and with and without outliers?]]

[[appendix entry for difference and use between sd/var]]

Just as we had multiple methods for determining the center of a dataset, we have multiple ways with which to determine the spread of our data. The simplest method of spread, the **range**, gives us the difference between the largest and smallest values in the data, computed simply as 

$$
\text{maximum value} - \text{minimum value}
$$
We may also report the range as an interval. That is, we may say that all of our observations fall in the interval $(\text{min val}, \ \text{max val})$.

The range, however, should be used with care; a single outlier in either maximum or minimum direction can give a false sense of spread for our observations. [[example]]. In other words, the range is not robust.

[[Introduce quartiles, percentiles, etc.]] <- this is actually done later in text. I will put it down there and we can rearrange things later. Man, this chapter is tricky

As an alternative to the range, which determines the difference between the largest and smallest values, we also have the **interquartile range (IQR)**, which determines the differnce between the 75th percentile (third quartile, Q~3~) and the 25th percentile (first quartile, Q~1~), i.e., 

$$
\text{IQR} = \text{Q}_3 - \text{Q}_1
$$

In other words, the IQR is the length of the interval containing the middle 50% of observations, where the smallest 25% and largest 25% are not included. As this helps protect against a handful of outliers in either direction, we have that the IQR is a robust measure of spread. [[Not sure how to nicely tie in here, but good place to note that the median is also included in this (and in fact is the center of it).]]

[[May be interesting to show to histograms of data and their summaries. For one, use mean and sd, for others, use median and IQR. Compare and contrast]]

### Percentiles and Quartiles

As the data that we can collect can come in a variety of different shapes and sizes, it is important that we use tools and definitions that will mean the same thing, regardless of what the data is, or where it comes from. Often, it is useful to know how large or small an observation is, relative to all of the others. We might say, for example, "Captain Public Health is the fourth tallest person in the class." Notice how this can mean one thing if Captain Public Health is in a class of 500 students, but something entirely different were he in a class of only 5.

A useful way around this is to use a concept known as a **percentile**, which allows us to determine where in a dataset an observation would fall relative to all other values. More precisely, the $p$th percentile is a value $V_p$ such that

- $p\%$ of observations are below $V_p$
- $(100 - p)\%$ of observations are greater than $V_p$


[[chose to use 20 instead of ten, since I think it allows us to "see" observations on the other side of the percentile. If we chose 10, there would be nothing to the left of it]]

Suppose, for example that we are looking for the $20$th percentile of a dataset, which we denote $V_{20}$. This will be the number such that it is greater than or equal to 20% of the data, but smaller than the remaining 80%. If our sample data consisted of ten observations, 

$$
2,\ 3,\ 5,\ 5\ ,\ 6 ,\ 7,\ 10,\ 23,\ 26,\ 28.
$$
Then the $20$th percentile would be $V_{20} = 3$, as 20% of the observations $\{2, \ 3\}$ are less than or equal to it, while the remaining 80%, $\{5,\ 5\ ,\ 6 ,\ 7,\ 10,\ 23,\ 26,\ 28 \}$ are greater than it. 

There are, however, a few difficulties that may arise with this definition. The largest of these are

- The sample size is too small
- There may be tied values
- The percentiles may not be unique 

For example, what if we had been asked to find the $20$th percentile of an altered version of the dataset above, where 3 is repeated several times?

$$
2,\ 3,\ {\color{red} 3},\ {\color{red} 3}\ ,\ 6 ,\ 7,\ 10,\ 23,\ 26,\ 28.
$$
There are a number of different methods for defining and calculating percentiles in these cases, but for the intent of this course, we will focus more on the interpretation of the percentile, rather than it's definition. 

There are a number of specific percentiles that are especially useful to us. For example, we have already seen the median, which can also be referred to as the $50$th percentile of a dataset. In addition to the median, we are also often interested in determining the $25$th and $75$th percentile as well. These three percentiles make up the **quartiles** of the data, denoted

$$
\begin{align*}
Q_1 &= 25^{th} \text{ percentile} = 1^{st} \text{ or lower quartile} \\
Q_2 &= 50^{th} \text{ percentile} = 2^{nd} \text{quartile or median} \\
Q_3 &= 75^{th} \text{ percentile} = 3^{rd} \text{ or upper quartile}
\end{align*}
$$
Here is a helpful thing to note: suppose that we begin with the median $M$, such that 50% of observations are less than this value and 50% are greater than it. $Q_1$ can be said to represent the median of the smaller 50% of all observations, while $Q_3$ can be said to be the median of larger 50%. With some effort, I can maybe find a way to articulate this better. 

It is the difference between the upper and lower quartile that we define as the interquartile range, or $\text{IRQ} = Q_3 - Q_1$.

[[Thinking about box plots here, used to depict information on skewness, percentiles, outliers, etc. It might be good to lead this chapter with all of the different characteristics we might want to know about (i.e., including center, spread, etc)]]

[[Numeric example with UI salaries]]

While it is common to report the mean and standard deviation as a two-number summary of continuous data, as we have seen in this chapter, neither of these measures are robust to extreme observations or outliers. An alternative approach, known as a five-number summary, makes use of the percentiles of the data, consisting of: (1) the minimum, (2) the first quartile, (3) the median, (4) the third quartile, (5) and the maximum.

In addition to the numerical presentation of the five-number summary, we can also present important percentiles with the use of a plot. The **box plot**, also known as a **box-and-whisker plot**, is one such visual summary. In addition to information regarding the quartiles, the box plot also depicts information about skewness and the presence of outliers. 


[[reproduce plot from slides, which was nicely labeled]]

In general, we can expect a box plot to consist of the following elements:

- A box, indicating the Interquartile Range (IQR), bounded by the values $Q_1$ and $Q_3$
- The median, or $Q_2$, represented by the line drawn within the IQR
- The "whiskers", extending out of the box, which can be defined in a number of ways. In the plot above, the length of the whiskers is determined to be 1.5 times the length of the IQR from either $Q_1$ or $Q_3$. Variations may include (but are not limited to) the minimum and maximum of the data, the length of one standard deviation above and below the mean (not median) of the data, or pre-specified percentiles in addition to those given by $Q_1, \ Q_2, \ \text{and} \ Q_3$.
- Outliers. These are typically presented as small circles or dots, and are values in the data that are not present within the bounds set by either the box or whiskers.

Box plots also allow us to determine if the distribution of the data is skewed or symmetric. We say a distribution is **symmetric** if the left side of the box plot is identical to the right, mirrored around the median. We say that a distribution is **skewed left** if the majority of the area of the box plot is on the right, and the area gradually trails off on the left. Finally, we say that a distribution is **skewed right** if the majority of the area for the box is on the left, gradually tailing to the right. 

[[Depending on how this is used in the course, I almost prefer positive and negative skew because this shit is confusing. Sort of how f(x - c) is a right shift of the plot compared to f(x)]]

[[Wait, do those definitions above match what is presented in the slides for the box plots on the next page?]]


[[Introduce questions to guide learning. i.e., where is median, is this skewed, are there outliers, etc]]

[[ Box plots to compare multiple distributions]]

[[Data presentation in the wild + common errors]]

[[review]]


```{r}
plot(aq$Ozone, aq$Temp, pch = 16)
```



















