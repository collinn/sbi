---
output: html_document
---

# Probability Distributions {#ch6}

> "Statistics is the grammar of science." 
>
> --- Karl Pearson

<div class="objective-container">
<div class="objectives"> Learning Objectives </div>
1. Understand how a distribution represents a random process that creates data that is then observed
2. Understand how the parameters of a distribution govern how the data is generated [and with what probability]
3. Be able to identify which distributions underlying a given real world random process.
</div>

## Introduction to Probability Distributions 

In Chapter 4, we introduced the idea of random processes, i.e. situations
in which the outcome can not be determined perfectly in advance. Random processes
are defined in terms of the *collection of possible events* (sample space) and their
*associated probabilities*. In that chapter, we saw three methods for calculating
probabilities - the enumeration method, the probability function method, and the
simulation method. In this chapter we will expand on the probability function 
method, which uses a known function called a **probability distribution** to
determine the probability of each event. 

Probability distributions are closely related to **random variables**, a numeric 
variable that can take on different values depending on the outcome of a random
process. In previous mathematics courses you may have seen variables such as
$x$ or $y$ used as placeholder values which are then solved for. For example, 
you can solve for the variable $x$ in $4x + 5 = 25$, to determine $x = 5$. By 
contrast, the outcome of a *random* variable cannot be predetermined. Instead, 
we talk probabilistically about the likelihood of observing each possible outcome.
Random variables are typically denoted with capital letters, e.g., $X$ or $Y$, 
whereas the observed outcome of the random process is denoted with lowercase letters
$x$ or $y$. For example, flipping a coin three times is a random process. We can
define the random variable $X$ to represent the number of heads we observe 
between the three flips. $X$ can take on four possible values: 0, 1, 2, or 3. If
we observe 2 heads, we have $x = 2$.

Most simply, a probability distribution (often just called a distribution) 
is a method for taking a possible event as input, and giving us the corresponding
probability as output; the corresponding probability tells us how likely it is 
that the specific event will occur, out of all of the possible events. We often 
say random variables have or follow a probability distribution, as the 
distribution quantifies the probability of observing the possible values a random
variable can take on. We can denote a probability distribution as $P(X = x)$, or 
the probability that the random variable, $X$, takes on a generic value, $x$.

There are many useful probability distributions that have
been defined by mathematicians and statisticians to describe a variety of scenarios:

* Counting the number of successes in a fixed number of trials that can results 
in either success or failure

* Counting the number of failures before the first success in a series of success/failure trials

* Describing the length of time between events that occur at a constant rate

* Describing the blood pressure of adults

One can represent a probability distribution visually using a **probability histogram**.
On the x-axis, we have the possible outcomes of the random process - the values
the random variable could take on. For each outcome, the bar height represents 
the probability of observing that value. For the coin flipping example, the 
probabilities of observing each possible number of heads can be represented as:


```{r, echo = F}
x <- 0:3
y <- dbinom(x, size = 3, prob = 0.5)
scaleFac1 <- sum(y) * 0.2
scaleFac2 <- sum(y) * 0.04
my_bar <- barplot(y, names.arg = x, col = 'palevioletred1',
                      ylim = c(0, max(y) + scaleFac1),
                  xlab = '# of heads',
                  ylab = 'Probability')
text(my_bar, y + scaleFac2, y, cex = 2)
```


The beauty of using probability distributions to describe the likelihood of all
outcomes of a random process is its simplicity. Probability distributions rely 
on a small number of **parameters** which determine the distribution's "shape".  
In the coin flipping example, one of the parameters is the probability of obtaining
heads. We assume that we have a fair coin and that this probability is 50%. If,
instead, we had a weighted coin with a 60% chance of landing on heads, the 
probability distribution would change.

```{r, echo = F}
x <- 0:3
y <- dbinom(x, size = 3, prob = 0.6)
scaleFac1 <- sum(y) * 0.2
scaleFac2 <- sum(y) * 0.04
my_bar <- barplot(y, names.arg = x, col = 'palevioletred1',
                      ylim = c(0, max(y) + scaleFac1),
                  xlab = '# of heads',
                  ylab = 'Probability',
                  main = '60% chance of heads')
text(my_bar, y + scaleFac2, y, cex = 2)
```

With a higher chance of the coin resulting in heads, we now see that the three 
flips are more likely to result in 2 or 3 heads, and much less likely to result
in 0 heads. Because these probabilities can be described by a distribution 
function, which depends on the probability of heads, we can easily compute and 
compare the probabilities of each outcome depending on the coin's true chance at
turning up as heads.

Another key concept related to probability distributions and random variables, is
the idea of the **expected value**. The expected value of a random variable, often
denoted as $E(X)$, is a weighted average which provides a measure of the central
mass of the probability distribution. The expected value averages over all possible
outcomes of the random variable with each outcome weighted according to its 
probability. Returning to the coin flipping example, with a fair coin we have 
seen that the probability distribution is:

```{r}
x <- 0:3
y <- dbinom(x, 3, 0.5)
tmp <- data.frame(x, y)
knitr::kable(tmp, col.names = c('x', 'P(X = x)'), align = 'c')
```

So the expected value is 

$$(0 \times 0.125) + (1 \times 0.375) + (2 \times 0.375) + (3 \times 0.125) = 1.5.$$

Looking at the probability histogram, this value should make sense as it falls
right in the center of the distribution.

Expected values are more easily conceptualized in terms of a game or bet. For 
example, consider the following game. You flip a fair coin; if the coin lands on 
heads, you win \$20 and if the coin lands on tails, you lose \$1. Would you play 
this game? Assuming you have a spare dollar, the answer is probably yes. Since
you have equal chances of the coin landing on heads or tails, you are just as 
likely to win \$20 as you are to lose \$1. In terms of the expected value, it 
would be

$$(20 \times 0.5) + (-1 \times 0.5) = 9.5$$

The expected value tells us that if you were to play this game over and over, 
you would be expected to win \$9.5 per game. If instead the game was that for 
heads you won \$1 and for tails you lost \$1, would you still want to play?



<div class="definition-container">
<div class="definition"> &nbsp;  </div>

**Probability distribution: ** <em> A method for assigning probabilities to all possible events </em>

**Random variable: ** <em> A method for assigning probabilities to all possible events </em>

**Probability histogram: ** <em> Values associated with a probability distribution that determine the distributions shape </em>

**Parameters: ** <em> Values associated with a probability distribution that determine the distributions shape </em>

**Expected value: ** <em> The weighted average of the outcomes of a random variable, with weights determined by their probability. </em>

</div>


## Binomial Distribution

One common probability distribution is the **binomial distribution**, which 
describes the number of successes in a fixed number of independent trials
that can result in one of two outcomes (success or failure), when each trial has 
the same probability of success. We have already seen one example of the
binomial distribution - flipping a coin three times counting the number of flips
that result in heads (success). Each flip has two possible outcomes (heads or 
tails), the same probability of heads (50%), and we have predetermined the number
of trials (3 flips). Another example of the binomial distribution is rolling a 
six-sided die 10 times and counting the number of rolls that result in a 5 or 6.
In this example, each die roll has a 1/3 chance of turning up a 5 or 6 and the 
die will be rolled 10 times.

As you may have noticed in these binomial distribution examples, the distribution
can be used for various probabilities of success and numbers of trials. In fact,
these quantities define the two parameters of the binomial distribution. These
parameters are denoted as $n$ = the number of trials and $p$ = the probability of 
success. The binomial distribution is written as a function of these parameters

$$
P(X = x) = \binom{n}{x} p^x (1-p)^{n-x}.
$$

While this may look like a nasty formula, it can be easily computed by any 
statistical software so there is no need to calculate the probability by hand.

For any valid value of $n$ and $p$, we can use the binomial distribution
to compute the probability of observing any possible outcome. To illustrate this,
we will consider a binomial distribution with $n = 10$ trials. The following 
figure shows the probability histograms for four different values of $p$.


```{r, fig.height=6, fig.width=8, echo = F}
n <- 10
p <- c(0.2, 0.5, 0.75, 0.9)

par(mfrow = c(2,2))
for (i in 1:length(p)) {
  x <- 0:n
y <- dbinom(x, size = n, prob = p[i])
barplot(y, names.arg = x, col = 'palevioletred1',
                  xlab = '# of successes',
                  ylab = 'Probability',
        ylim = c(0, 0.4),
        main = paste0('p = ', p[i]))
}


```

When $p$ is 0.2, we are more likely to observe a lower number of successes, than
to observe 8 or more successes. When $p$ is 50%, we are most likely to observe
5 of the 10 trials resulting in a success, with the probability decreasing as the
number of successes gets closer to 0 or 10. For a large success probability of 90%,
there is a very small chance we observe less than 6 successes and a much higher
probability of observing 9 or 10 successes.

The expected value of the binomial distribution is given by $E(X) = np$


<div class="exercise"> &nbsp;  </div>
<div class="exercise-container">

The applet below is designed to help you get familiar with the parameters of the
binomial distribution, and how they impact the probability distribution.
You can change the values of the number of trials $n$, and
the probability of success $p$, and the app will display the associated distribution
in a probability histogram.

</div>

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/binomial/" width = "100%" height = "500"> 
</iframe>

<div class="exercise-container">

Use the applet to answer the following questions:

1. Set $n = 10$, and change the value of $p$ (note: you can press the triangular 
"play" button to have the app vary $p$ automatically). What happens to the shape of the
distribution as $p$ gets closer to 0? What about when $p$ gets closer to 1?

2. Now, set $p = 0.4$ and vary $n$ over the range of possible inputs. What do
you notice about the x-axis as $n$ is changing? Explain this trend by referring
back to what $n$ represents.

3. Keeping $p$ constant, as $n$ changes between 20 - 50, does the shape of the 
distribution change? What about the location of the distribution?

</div>



<div class="exercise"> Exercises </div>
<div class="exercise-container">

The applet below is designed to familiarize you with data generated from the 
binomial distribution. You can change the values of the number of trials $n$, and
the probability of success $p$ to specify the parameters of the population 
distribution. Then, 

</div>

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/binomialData/" width = "100%" height = "600"> 
</iframe>


<div class="definition-container">
<div class="definition"> &nbsp;  </div>

**Binomial Distribution: ** <em> A probability distribution that characterizes the
  probabilities of observing some number of successes in a fixed number of 
  trials, each with two possible outcomes and the same probability of success. </em>

**Discrete Distribution: ** <em> Any probability distribution that depicts the 
  occurrence of countable values.</em>
</em>

</div>



## Normal Distribution

Included below are histograms of the depth of Lake Huron from 1875-1972, the annual flow of the Nile river in cubic meters from 1871-1970, and the height in feet of 31 black cherry trees. What do these histograms seem to have in common?

```{r, echo = FALSE, fig.align='center', fig.width=10}
par(mfrow = c(1, 3))
hist(LakeHuron)
hist(Nile)
hist(trees$Height)
```

What we see here are examples of a *normal distribution* (also known as a bell curve), one of the most ubiquitous distributions in all of statistics. The normal distribution is characterized by the "bell shape" that is symmetric about it's mean [but maybe don't say mean]. 

Like the binomial, the normal distribution is characterized by two parameters, $\mu$ and $\sigma^2$, representing the mean and the variance, respectively. The mean value, $\mu$, indicates the location of the peak on the x-axis, whereas the variance, $\sigma^2$, indicates the amount of dispersion about the mean. A random variable $X$ that follows a normal distribution can be expressed $X \sim N(\mu, \sigma^2)$, or, "The random variable $X$ follows a normal distribution with mean $\mu$ and variance $\sigma^2$." The formula for the normal distribution is given as 

$$
\begin{align*}
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}}.
\end{align*}
$$

Consider the two normal distributions below, with different values for $\mu$ and $\sigma^2$. Although they are centered at different locations and have different amounts of dispersion around the mean, they are both bell-shaped curves characteristic of the normal distribution:

```{r, fig.align='center', echo = FALSE}
curve(dnorm(x, 0, 1), from = -4, to = 13, lwd = 4,
      col = 'steelblue', ylab = 'Probability', xlab = 'X',
      main = "Two Normal Distributions")
curve(dnorm(x, 5, 3), add = TRUE, lwd = 4, col = 'tomato')
legend(x = 6.5, y = 0.35, legend = c(expression(N(0,1)),
                                     expression(N(5,3))),
       lwd = 4, col = c("steelblue", "tomato"))
```

Given that the normal distribution appears so frequently in statistics, it is common practice to *standardize* a normal distribution so that it has a mean value of $\mu = 0$ and variance $\sigma^2 = 1$. A normal random variable that has been standardized is called a *standard normal distribution* and is often written $Z \sim N(0,1)$. We can consider again the histograms above, once they've been standardized:

```{r, echo = FALSE, fig.align='center', fig.width=10}
par(mfrow = c(1, 3))
mkstd <- function(x) (x-mean(x))/sd(x)
hist(mkstd(LakeHuron), freq = FALSE)
curve(dnorm(x), add = TRUE, col = 'steelblue', lwd = 4)
hist(mkstd(Nile), freq = FALSE)
curve(dnorm(x), add = TRUE, col = 'steelblue', lwd = 4)
hist(mkstd(trees$Height), freq = FALSE)
curve(dnorm(x), add = TRUE, col = 'steelblue', lwd = 4)
```

Unlike the binomial distribution, in which there are $n$ possible values that our random variable can take, the normal distribution represents a random variable that is *continuous* over a range of values. Instead of asking the probability of a specific value, say, $Z = 0$, probabilities are given as the area under the curve for a certain interval. We might ask, "What is the probability that $Z$ is one standard deviation ($\sigma$) away from 0?" pr perhaps, "What is the probability that $Z < 0$?"

```{r, fig.align='center', echo = FALSE, width = 12}
par(mfrow = c(1, 2))
curve(dnorm(x, 0, 1), from = -3, to = 3, lwd = 3, col = 'steelblue',
      ylab = 'density', main = "Standard Normal")
x <- seq(-2, 2, 0.01)
y <- dnorm(x)
z <- 1
#axis(1, at = c(-z, 0, z), labels = c(expression(-z[alpha]), 0, expression(z[alpha])))
polygon(c(-z, x[x > -z & x < z], z ),
        c(0, y[x > -z & x < z], 0), col = 'pink',angle = 45, density = 55,
        border = FALSE)

text(2, 0.325, cex = 1.15,
     labels = expression(paste("P(-", 1, " < Z < ", 1, ") = ", 0.683)))
lines(x = c(-1, 1), y = c(dnorm(1,0,1), dnorm(1,0,1)))
lines(x = c(0,0), y = c(0, dnorm(0, 0, 1)), lty = 2)
text(x = 0.5, y = 0.265, labels = expression(sigma), cex = 1.5)
text(x = -0.5, y = 0.265, labels = expression(sigma), cex = 1.5)

###

curve(dnorm(x, 0, 1), from = -3, to = 3, lwd = 3, col = 'steelblue',
      ylab = 'density', main = "Standard Normal")
lines(x = c(0,0), y = c(0, dnorm(0, 0, 1)), lty = 2)
x <- seq(-3, 0, 0.01)
y <- dnorm(x)
z <- 0
#axis(1, at = c(-z, 0, z), labels = c(expression(-z[alpha]), 0, expression(z[alpha])))
polygon(c(-z, x[x < z], z ),
        c(0, y[x < z], 0), col = 'pink',angle = 45, density = 55,
        border = FALSE)
text(2, 0.325, cex = 1.15,
     labels = expression(paste("P( Z < ", 2, ") = ", 0.978)))


```

Because probabilities for continuous distributions are described as areas under the curve, their values are computed with integrals. [do we introduce probability tables or just go with the app?]

also maybe include

- subtract  cdf?
- probability that |Z| > z?
- what else?



[Using app below, explore different parameter values. Use slider to select a range of probabilities. Note that the area of interest is highlighted. Do exercises with it]

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/normal/" width = "100%" height = "500"> 
</iframe>

<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/normalData/" width = "100%" height = "600"> 
</iframe>


<div class="definition-container">
<div class="definition"> &nbsp;  </div>
**Normal Distribution: ** <em> A continuous bell-shaped distribution with two parameters that are the mean value, $\mu$, with a variance $\sigma^2$.  </em>

**Continuous Distribution: ** <em> A special case of the normal distribution, $Z \sim N(0, 1)$ </em>

**Standard Normal Distribution: ** <em> A special case of the normal distribution, $Z \sim N(0, 1)$ </em>

</div>














