---
output: html_document
---

# Introduction to Inference {#ch8}

```{r, echo = FALSE, warning=F, message=F}
library(markdown)
```

> " A useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in his experiment."
>
> --- Gertrude Mary Cox


!!! Recurring issue in this chapter -- be extra cautious in wording interpretation of confidence intervals

<div class="objective-container">
<div class="objectives"> Learning objectives </div>
1. Understand conceptually how confidence intervals are constructed
2. Know correct interpretation of confidence intervals
3. Hypothesis testing null and alternative hypothesis
4. Type 1 and type 2 error
5. Definition of p-value and how to interpret
</div>

It may be helpful at this point to quickly review our goals as statisticians, as well as some of the key concepts that we have covered so far:

1. First, let's consider again the primary goal of statistical analysis. We often begin with some population of interest, and in particular, we are generally interested in determining some numerical quantity that describes this population. This may include things such as average monthly spending, or the proportion of individuals who respond positively to some treatment. As it is typically impractical to measure some quantity within an entire population, we are often limited to collecting a *random sample* of the population from which we hope to make generalizations that apply to the population as a whole.

2. The data that we collect is usually *random*, and we can quantify the amount of uncertainty associated with a particular outcome with a numerical quantity known as a probability. The entire relationship between possible values of our data and their associated probabilities is known as a *distribution*, two of the most widely used being the normal and binomial distributions.

3. The act of collecting a sample and computing a statistic is itself a random process and, as such, also follows a probability distribution known as a *sampling distribution*. As we saw in the previous chapter, as the number of individuals collected in our sample increases, the computed statistic will approximately follow a normal distribution, thanks to the Central Limit Theorem. In our case, the sampling distribution will depend on the value of the sample mean, as well as the size of the standard error

Our goal now is to determine how we might use these ideas to go from information in our random sample to statements about the broader population. This process is known as statistical inference and is the focus of the present chapter. 

## Central Limit Theorem {#ch8_s0}

[[This may be either moved to previous chapter or merged in with next section]]

We ended the previous chapter with an introduction of the Central Limit Theorem (CLT), which stated that as the size of our sample, $n$, increased, the sampling distribution of the sample mean $\overline{X}_n$ would become approximately normal. That is, we had

$$
\overline{X}_n \sim N(\mu, \sigma^2/n)
$$
where $\mu$ is the true value of the population mean and $\sigma^2$ is the variance; $n$, of course, is the size of our sample. Just as the size of our sample increases, so does the amount of variability in our sampling distribution *decrease*, giving us more precise estimates for the true value $\mu$. 

To see how the CLT will be of benefit to us, it may be helpful to explore a little bit more closely how the different portions(?) of the theorem will prove helpful [[hate the wording here. this whol esection needs work]]. In particular, recall that

1. The CLT tells us that as the size of our sample increases, the sampling distribution of $\overline{X}_n$ will become approximately normal. One of the defining characteristics of a normal distribution is that it is unimodal and symmetric about the mean. This means that random values tend to be closer to the mean than not and that values are equally likely to be just above the mean as they are to be just below it. 

2. We saw that $\overline{X}_n$ is an *unbiased estimator* of the mean. In otherwords, given our data, this is the most likely estimate for the true value of $\mu$.

3. The variance of a normal distribution tells us how dispersed these values are about the center This should be the second point.

```{r, echo = FALSE, fig.align='center'}

```

## Point Estimation and Confidence Intervals { #ch8_s1}

In the previous chapter, we concerned ourselves with calculating a particular value of a statistic, in our case, the sample mean. The process of computing a single statisic from a sample that will be used to estimate a paramter in the population is known as **point estimation**. As we saw, however, the value of a sample statitic is random, meaning that a sample mean computed from two separate samples from the same population are unlikely to be identically equal to each other. To account for both this random variation and the fact that we are often only able to obtain a single point estimate, we might consider instead determining an *interval* of likely values that our point estimate may fall within. We might consider this our *margin of error* for the point estimate. This process is known as **interval estimation**, and it is a critical part of performing statistical inference. 

<!-- To begin, let's first note that the interval we construct will be determined by the point estimate that we have collected. As this point estimate is itself a random variable, it follows that the interval itself will be random as well. The primary consequence of this fact is that there is some probability that the true -->

We can begin by considering some desireable properties of a good interval. For example, it would be nice if our constructed interval had a high probability of containing the true population parameter. Intuitively, the larger the interval, the more likely that the true parameter falls within this range. However, if an interval is *too* large, it ceases to contain much useful information. If when planning what to wear this weekend we reported that the temperature was supposed to be somewhere between absolute zero and the heat found at the center of the sun, this would mostly likely capture the actual temperature, but it would not be terribly useful.

Perhaps, then, we want our intervals to be very small. whatever...

What we see here is a tension between two competing goals: we would like our intervals to be large enough to capture the true parameter value with high probability, yet small enough to give us a reasonable range of estimates of what this value might be. Fortunately, we have statistical tools available to [[do this right]]

[[this section might be a bit abstract]]

In the above discussion, the quality of an interval estimate with which we are concerned is known as the **coverage probability**, the probability that the interval that we construct contains or covers the parameter of interest. The coverage probability is often experssed as the term $1 - \alpha$, where $\alpha$ is some term between 0 and 1. Just as $1 - \alpha$ represents the probability that our interval does cover the true value, the value of $\alpha$ represents the probability that it *does not*. In other words, $\alpha$ represents the probability that we are incorrect. It is often convenient to convert these decimal values to percentages, which can be done by multiplying the value by 100. This value, expressed as $100(1 - \alpha)\%$, is called the **confidence interval**. As we can see, the smaller the value of $\alpha$, the more likely our interval estimate contains the true value of the parameter of interest. 

[[need to add either here or above this paragraph information relating mean, SE, and sampling distribution to show interval idea]]

<!-- We will first turn our attention to parameter estimation, of which there are two general approaches. These approaches are **point estimation**, which is a single statistic computed from the sample data used to estimate the parameter and **interval estimation**, which is an interval computed from the sample data to give a range of plausible values that is likely to contain the true population parameter. Point estimation is straightforward and is as simple as using the sample mean as an estimate of the population mean. While straightforward, this approach is problematic. In the last chapter we saw that every time we take a sample from a population and compute the sample mean, it changes. And we saw an example in which no matter how we took a sample, the sample mean could never possibly equal the population mean. This is because the act of taking a sample is a random process, which means that for any estimate from our sample it is highly unlikely (or straight up impossible) that we got the exact correct answer. Instead, it would be nice to have an interval that we could be reasonably confident contained the true parameter. -->

<!-- What properties would make a good interval estimate? Well, on one hand we desire an interval that we can have a high degree of confidence of including the true parameter, but on the other hand, a narrow interval is much more informative. I can tell you with 100% confidence that the interval from $-\infty$ to $\infty$ covers the true population parameter, and I don’t even need to know anything about what you’re studying or what numerical quantity is being estimated and I don’t even need any data! I’ve achieved the first goal of having a high degree of confidence, but this interval is clearly not informative. I can make the interval smaller (I will need some data to do that), but the probability that the interval contains the true parameter will decrease. As a compromise, scientists and statisticians decided that intervals with a 95% probability of containing the true parameter was a satisfactory balance between the desire for high confidence and narrow intervals. This gives way to the most common confidence interval – the 95% confidence interval. Although we are getting a little bit ahead of ourselves here (we will have plenty of chances to calculate 95% confidence intervals). Let’s back up with some important definitions and notation. -->

<!-- With an interval estimate, we are concerned with **coverage probability**, the probability that the interval contains or covers the parameter of interest. Typically, coverage probability is denoted by $1 - \alpha$. As the interval covers the parameter with probability $1 - \alpha$, the interval *does not* cover the parameter with probability $\alpha$ (because covering and not covering the parameter are complements and cover the entire sample space). In that sense, $\alpha$ is the probability that we are wrong. We will discuss $\alpha$ in more specific terms later in the chapter, but for now let’s just note that $\alpha$ is a small number, maybe 0.05 or 0.01. If we convert from decimal format to percentage format (multiplying by 100), we have $100(1 - \alpha)$%, which is called the **confidence level**. -->

---

The mathematical details of constructing a confidence interval depends on the parameter of interest and how the study was designed, and we will see various formulas for constructing confidence intervals throughout the rest of this book. In this chapter, we will focus on conceptual understanding and interpretation. To construct a $100(1 - \alpha)$% confidence interval, we use sample data and sampling distributions as we know that sampling is a random process. For example, the Central Limit Theorem tells us the sampling distribution of the mean is normally distributed and gives us the mean and standard deviation of that distribution. Once we have a normal distribution for which we know the mean and standard deviation, we can calculate probabilistic quantities using software.

<div class="definition-container">
<div class="definition"> &nbsp; </div>
**Point Estimation: ** <em> def </em>

**Interval Estimation: ** <em> def </em>

**Coverage Probability: ** <em> def </em>

**Confidence Interval: ** <em> def </em>

</div>

[[strong dislike this section below]]

It’s extremely important that we remember what is random and what the probability distribution is specifying when we are interpreting confidence intervals. The sampling distribution describes which values of the sample statistic are likely and which are not likely. The random process is drawing a sample. The population parameter is a fixed quantity – we don’t know what it is, but it does not change. Confidence intervals are constructed from the information we do know – the observed data from the sample. Because sampling is a random process and confidence intervals are constructed using the sample data, the confidence interval will vary from sample to sample. However, we formulate confidence intervals mathematically to ensure that the probability that the interval contains the true probability is $1 - \alpha$. And as we’ve seen probabilities are long-run frequencies. So any given confidence interval may or may not cover the true parameter, but in the long run if we take samples over and over and compute a confidence interval based on the data from each sample, we are guaranteed that $100(1 - \alpha)$% of the confidence intervals constructed will cover the truth. This is quite appealing. Lets explore this concept further by using the following applet.



<div class="exercise-container">
<div class="exercise"> &nbsp; </div>

The applet below is designed to help you get familiar with some of the important concepts related to confidence intervals. The inputs to the simulation are the sample size, number of experiments, and the confidence level. The simulation proceeds by taking a sample of the specified size and constructing a confidence interval based on that sample with the specified level of confidence. This process is one experiment. Each sample is drawn from the same population. For a specified number of experiments, the sampling and confidence interval construction is repeated either 10, 50, 100, or 1000 times. The plot shows all the confidence intervals constructed from each experiment - the number of intervals shown is equal to the "Number of Experiments." The true population parameter in this case is 0 (only for simplicity). Intervals are shaded in blue if they cover the true population parameter and red if they do not contain the true parameter. The applet also reports the "observed coverage" from all experiments, this is the proportion of all the intervals shown that did contain the truth. Clicking "Run Simulation" will redo the specified number of experiments using the inputs provided.

</div>

<!-- frameBorder="0" -->
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/confidenceIntervals" width = "100%" height = "550"> 
</iframe>

<div class="exercise-container">
<ol>
  <li>Set the sample size to 30, the confidence level to 95%,
      and simulate running 100 experiments (100 intervals total).
  <ol style="list-style-type: lower-alpha; ">
  <li>How many intervals did not contain the true population parameter? In other words, how many red intervals are there? </li>
  <li>What was the observed coverage? Show how this was computed based on the number of intervals that did/did not contain the truth. </li>
  <li>Given the specifications use, What would you expect the coverage to be? </li>
  <li>Re-run the simulation under the same conditions (click "Run Simulation" again). What was the observed coverage for the second set of 100 experiments?</li> 
  <li>Did you get the same observed coverage for both simulations? What does this indicates about the simulation? </li>
  <li>Re-run the simulation several times and compare the observed coverage from each run. What do you notice?</li>
  </ol>
  </li>
  <li>Keep the confidence level at 95%, but change the sample size to 50. Simulate 100 experiments.
  <ol style="list-style-type: lower-alpha; ">
  <li>How do the intervals differ from those constructed from experiments using a sample size of 30?  </li>
  <li>What is the observed coverage? What would you expect the coverage to be based on the specifications used?</li>
  <li>Across the 100 confidence intervals shown, how do the widths of the confidence intervals compare? Is there any difference in the confidence interval widths between intervals that do contain the true parameter and those that do not? </li>
  <li>Now change the sample size to 100. How do these intervals compare to those using sample sizes of 30 or 50? </li>
  <li>Change the number of experiments to be 1000. Play around with various sample sizes. How does changing the sample size effect the observed coverage? </li>
  </ol>
  </li>
  <li>Return to a sample size of 30. Now we will simulate 1000 experiments. Simulating more experiments makes things harder to visualize (we can no longer easily count the intervals that did not contain the truth), but the coverage is more stable when the simulation is re-ran.
  <ol style="list-style-type: lower-alpha; ">
  <li>With a confidence level of 95%, what is the observed coverage? </li>
  <li>Re-run the simulation with a sample size of 100. How do these intervals compare to those using sample sizes of 30 or 50? </li>
  <li>Re-run the simulation using a confidence level of 50%. 
        How do these intervals compare to those using a confidence level of 95%? What is the observed coverage? </li>
  </ol>
  </li>
  <li>Change the parameters of the simulations as needed to answer the following true/false questions. Explain your answers. 
  <ol style="list-style-type: lower-alpha; ">
  <li>As the sample size increases, the coverage probability increases.</li>
  <li>As the confidence level decreases, the width of the confidence intervals decreases,</li>
  <li>As the sample size increases, the width of the confidence intervals increases.</li>
  <li>If a researcher wants a narrower confidence interval, they should obtain a larger sample.</li>
  <li>If a researcher wants a narrower confidence interval, they should decrease the confidence level. </li>
  </ol>
  </li>
</ol>
</div>

## Hypothesis Tests { #ch8_s2}

Often, the motivation for performing a particular experiment is to answer a scientific question about a population.

We have just seen that, along with the CLT, the collection of a sample mean along with a standard error allows us to construct an intervals of values representing likely values of the true parameter. 

Along with parameter estimation, a cornerstone of statistical inference is significance testing. Confidence intervals allow us to construct a range of plausible values for the parameter, and significance tests allow us to determine the likelihood of a parameter taking a certain value. A **hypothesis test** uses sample data to assess the plausibility of each of two competing hypotheses regarding an unknown parameter (or set of parameters). A **statistical hypothesis** is a statement or claim about an unknown parameter. The **null hypothesis** generally represents what is assumed to be true before the experiment is conducted. This hypothesis is typically denoted $H_0$. The **alternative hypothesis** represents what the investigator is interested in establishing. This hypothesis is typically denoted $H_A$. Oftentimes when people refer to the "scientific hypothesis," this in reference to the alternative hypothesis - it is what the investigators think will happen or what they want to show. The goal of a hypothesis test is to determine which hypothesis is the most plausible - the null or the alternative. 

As an example, consider researchers that have developed a new drug to treat cancer. In order for the drug to be approved for use, the investigators must prove that it is more effective in treating cancer than the current gold standard treatment. To do this, the investigators gather a  sample of cancer patients and randomize half of them to receive their new drug and half to receive the current treatment. Then they determine how many patients improved in both groups. In this scenario, the null hypothesis would be that the new drug and the current drug result in the same improvement. Why? Well, the null hypothesis is what is believed before the data was collected. The key is *whose* beliefs we are talking about. While the scientists that developed the drug most likely believe that their new drug is more effective, the rest of the scientific community remains in a state of uncertainty. The null hypothesis reflects the general beliefs of the scientific community. The alternative hypothesis in this scenario is that the drugs differ in their effectiveness on treating cancer. This is what the researchers hope to show, specifically, they hope to show that their drug is more effective, however, when the study is conducted there is no evidence in either direction, so we leave the alternative hypothesis to also encompass the possibility of the new drug being worse.

In general, we can think about the null hypothesis as being the "baseline," "boring," "nothing to see here" hypothesis. The exact specification will depend on the study context and the type of data being measured (categorical or continuous):

* $H_0$: the average cholesterol for hypertensive smokers' is no different than the general population
* $H_0$: no difference between the treatment and control groups
* $H_0$: men and women have identical probabilities of colorectal cancer
* $H_0$: observing a "success" in a population is identical to flipping a coin

The hypothesis testing procedure uses probability to quantify the amount of evidence against the null hypothesis. Since the null hypothesis is the baseline, we start by assuming that it is true. Then, we conduct the study and collect data to quantify the likelihood that the the null is true. The reason for this approach is rooted in the scientific method. As we introduced in Chapter 1, the scientific method has 7 steps:
 
1) Ask a question
2) Do background research
3) Construct a hypothesis
4) Test your hypothesis with a study or an experiment
5) Analyze data and draw conclusions
6) How do the results align with your hypothesis?
7) Communicate results

We are really focusing on steps 3-5. In step 3, we construct the "scientific hypothesis" and in step 4, we test that hypothesis. In order to produce rigorous scientific results we cannot assume that the scientific hypothesis is true, as that is the goal of our study. We must assume the current state of knowledge (null hypothesis) and then if we are to prove that our hypothesis is correct, we would show that if the current knowledge was true, it would be really unlikely that our experiment would have ended up how it did. 

A great analogy to the concept of hypothesis testing is our judicial system. In court, the legal principle is that everyone is "innocent until proven guilty" and the prosecution must prove that the accused is guilty beyond a reasonable doubt. In many cases, there may not be definitive evidence if the defendant is actually innocent or guilty. But, if the prosecution can show that the likelihood of the accused individual being guilty is high (or equivalently, that the likelihood of the accuwsed individual being innocent is low), then the defendant will be convicted. In hypothesis testing researchers are like the prosecution and must use data to prove that the null hypothesis (current state of knowledge) is false beyond a reasonable doubt. The next several chapters will go through how we can use different types of data to quantify our evidence against the null hypothesis.

With this set up in mind, we have two possible outcomes of a hypothesis test. Either we conclude that we do not have a lot of evidence against the null hypothesis, i.e. the null hyptohesis looks reasonable, and we **fail to reject the null** *or* we conclude that we have enough evidence against the null and we **reject the null**. It is extremely important to note here that we NEVER accept the null or accept the alternative. Many people find this annoying because we can never say anything with 100\% certainty. But this is exactly the point! Remember that statistics is all about quantifying our uncertainty. Think back to our drug development example. There will never ever ever be a drug that works exactly the same in every person that takes it. People are too variable and many aspects of a person's life impacts how a drug works in their body. So it would be completely unreasonable to say that a new drug works all the time. However, there can be a drug that improves outcomes for the average person or that this drug is likely to improve outcomes in a randomly selected person who takes it.

We can create a two by two table for the results of any hypothesis test. In the rows we have the two possible outcomes from our test - fail to reject $H_0$ and reject $H_0$. In the columns we have the true underlying state of nature - either $H_0$ is true or false. 

```{r, echo = FALSE}
library(kableExtra)
col1 <- c('Correct (1 - $\\alpha$)', 'Incorrect $\\alpha$')
col2 <- c('Incorrect ($\\beta$)', 'Correct (1 - $\\beta$)')

toOutput <- cbind.data.frame(col1, col2)
colnames(toOutput) <- c("$H_0$ true", '$H_0$ false')
rownames(toOutput) <- c('Fail to reject $H_0$', 'Reject $H_0$')

kbl(toOutput) %>%
  kable_paper(full_width = F) %>%
  column_spec(2, width = "4em") %>%
  column_spec(3, width = "6em")
```

In the upper-left and bottom-right cells of the table we are making the correct decision based on our test. When the null hypothesis is true, failing to reject $H_0$ is the correct decision and when the null hypothesis is false, rejecting $H_0$ is the correct decision. However, the other two cells correspond to a mistake being made. Because statisticians are not creative, these mistakes are referred to as **type 1 error** and **type 2 error**. A type 1 error is equivalent to a false alarm or a false positive - the null hypothesis was rejected, when in fact it was true. A type 2 error can be thought of as a missed opportunity or a false negative - the null hypothesis was false, but it was not rejected. Typically, the type 1 error rate is symbolically denoted with $\alpha$ and the type 2 error rate is denoted by $\beta$ (again the statisticians of the past were not creative).

If we were looking to create a good hypothesis test, we would want to minimize type 1 and type 2 errors. In other words, if we were to conduct our study over and over again and there was something to be found, we would want to reject the null hypothesis (find something) at a high rate and fail to reject the null hypothesis at a low rate. If there was truly nothing to be found, we wouldn't want to find anything and if there is something to be found we want to find it. However, there is a trade-off between type 1 and type 2 error. Let us illustrate this point with a simulation.



<div class="exercise-container">
<div class="exercise"> &nbsp; </div>

In this experiment, we are looking to determine if a coin is fair, i.e. whether or not the probability of heads is 50%. A type 1 error in this context would be concluding that the coin is not fair, when it actually is. A type 2 error in this context would be concluding that the coin is fair when it is actually not. Each experiment consists of flipping a coin 20 times and observing the proportion of the 20 flips that result in heads. If the coin is fair, we would expect around 10 of the 20 flip to result in heads. However, if we observed 11 or 12 heads in one experiment, would you be convinced the coin isn't fair? What about if you observed 19/20 of the flips resulting in heads?

This applet allows the user to specify the "Rejection Threshold" to determine the cutoff where the app will reject the null hypothesis. This is specified in terms of how far off the proportion of heads in the experiment is from 50%. Since we have no way to determine if the coin is more prone to heads or tails, we consider this distance in both directions. If the observed proportion of the 20 flips that result in heads is beyond the specified threshold, the null hypothesis will be rejected. For example, if you choose a threshold of 10%, then any experiment where there are 60% (12/20) or more flips resulting in heads **OR** 40% (8/20) or less flips resulting in heads, then the null hypothesis is rejected and the coin is determined to not be fair. 

The simulation involves replicating the 20 flip experiment 10,000 times. For each experiment, a coin is flipped 20 times and the proportion of heads is calculated. If that proportion exceeds the specified threshold it is counted as "Reject." If the proportion does not exceed the threshold, that experiment is considered a "Fail to Reject." The barchart shows the proportion of 10,000 experiments in which the null hypothesis was and was not rejected. Note: Since we are replicating the experiment 10,000 times, we don't expect the results to change much if the simulation is re-ran. However, you may see small changes in the proportion of rejects/fail to rejects for the same input values. 


</div>

<!-- frameBorder="0" -->
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/ceward/textbook/shinyApps/type12Err" width = "100%" height = "550"> 
</iframe>

<div class="exercise-container">
<ol>
  <li>Set the true status of the coin to be fair and start with a threshold of 5%.
  <ol style="list-style-type: lower-alpha; ">
  <li>In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain.</li>
  <li>At this threshold, how many heads would cause an experiment to reject the null hypothesis</li>
  <li>At this threshold, what proportion of the experiments resulted in the null hypothesis being rejected?  </li>
  <li>In terms of $\alpha$ and $\beta$ as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it's value?</li>
  <li>As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected </li>
  <li>Did we investigate type 1 or type 2 errors in this problem?</li>
  </ol>
  </li>
  <li>Now change the true status of the coin to be unfair with a 70% chance of heads and set the rejection threshold to 15%.
  <ol style="list-style-type: lower-alpha; ">
  <li>In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain.</li>
  <li>At this threshold, how many heads would cause an experiment to reject the null hypothesis</li>
  <li>At this threshold, what proportion of the experiments resulted in the null hypothesis *not* being rejected?  </li>
  <li>In terms of $\alpha$ and $\beta$ as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it's value?</li>
  <li>As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected? Explain why. </li>
  <li>Did we investigate type 1 or type 2 errors in this problem?</li>
  </ol>
  </li>
  <li>Now change the true status of the coin to be unfair with a 20% chance of heads and set the rejection threshold to 20% 
  <ol style="list-style-type: lower-alpha; ">
  <li>In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain.</li>
  <li>At this threshold, how many heads would cause an experiment to reject the null hypothesis</li>
  <li>At this threshold, what proportion of the experiments resulted in the null hypothesis *not* being rejected?  </li>
  <li>In terms of $\alpha$ and $\beta$ as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it's value?</li>
  <li>As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected?</li>
  <li>Did we investigate Type 1 or Type 2 errors in this problem?</li>
  </ol>
  </li>
  <li>What can you conclude about the relationship between type 1 and type 2 errors? </li>
</ol>
</div>



## P-values { #ch8_s3}

All hypothesis tests are based on quantifying the probability of the study results assuming the null hypothesis is true. This probability is so important that it has a special name, the **p-value**. In technical terms, the p-value gives the probability of obtaining results as extreme or more extreme than the ones observed in the sample, *given* that the null hypothesis is true. A less technical way to describe a p-value is that assuming there is truly nothing going on, what's the chances of obtaining results similar in opposition to the null hypothesis as our study? 

If we are thinking about hypothesis testing as a court case, p-values are the way that we can quantify the evidence against the defendant. Recall, the prosecution wants to prove beyond a reasonable doubt that the defendant is not innocent. So what does the scientific community consider sufficient evidence? There is a generally agreed-upon scale for interpreting p-values with regards to the strength of evidence that they represent.

```{r, echo = F}
p <- c('0.1', '0.05', '0.025', '0.01', '0.001')
strength <- c('Borderline', 'Moderate', 'Substantial', 'Strong', 'Overwhelming')
toPrint <- cbind.data.frame('p-value'=p, 'Evidence against null' = strength)
knitr::kable(toPrint)
```

Often, the term "**statistically significant**" is used to describe p-values below 0.05, possibly with a descriptive modifier.

* "Borderline significant" (p < 0.1)  
* "Highly significant" (p < 0.01)

However, don't let these clearly arbitrary cutoffs distract you from the main idea that p-values represent - how far off is the data from what you would expect under the null hypothesis. A p-value of 0.04 and 0.000000001 are not at all the same thing, even though both are "statistically significant."

In general, a p-value cutoff is chosen and if a p-value below the cutoff is observed, the null hypothesis is rejected. The investigators choose this cutoff, which is equivalent to the type I error rate and thus denoted by $\alpha$, before analyzing the data. Most of the time $\alpha$ is set to 0.05, which means there is a 5\% chance of a type I error (false alarm). The smaller the value of $\alpha$, the greater the "burden of proof" required to reject the null hypothesis. $\alpha$ is also commonly called the **significance level**.

A fundamental property of p-values is that if we use $p < \alpha$ as cutoff for rejecting the null hypothesis, the type I error rate is guaranteed to be no more than $\alpha$. However, the $p < \alpha$ cutoff guarantees us nothing about the type II error rate. This is because p-values are calculated assuming the null hypothesis is true, so they don't give us any information about what to expect when the null hypothesis is false.

While p-values are widely used, have a distinct purpose, and can be informative they also have a number of limitations. 
























