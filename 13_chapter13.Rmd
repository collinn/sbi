---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction to Simulations {#ch13}

<div class="objective-container">
<div class="objectives"> Learning objectives </div>
1. Understand what a simulation is
2. Understand why simulations are useful
3. Learn the types of illustrations that can be helpful to summarize simulations
4. Conceptual understanding of randomness and a simulation
</div>

## What is a simulation? {#ch13_s1}

Throughout this text, we will be describing simulations in terms of coins and dice. While these examples may be contrived, they do serve to illustrate the components of probability, randomness, and simulation in a way that is free from unnecessary complication. For example, flipping a coin has exactly two outcomes with equal probability. The goal here is not to bore you with unmotivating examples ("When am I ever going to flip 10,000 coins?"), but rather allow the focus of the discussion to be on the conceptual elements -- experiments, events, outcomes, and probabilities. Keeping this in mind will help the reader keep their focus on the relevant information, if at the slight expense of [idk it being boring?]


Prior to the advent of computers, nearly all of statistics were done in an analytic fashion, with the use of concise formulas and tedious calculations. However, with the ubiquity of

Throughout this text, we will be making heavy use of a tool known as simulation. In the broadest sense, computer simulations allow us to quickly and consistently repeat a random experiment that we might use to form conclusions about a process. Before going into technical details, let's begin with a simple random experiment that has a single event, the act of flipping a fair coin one time:

[[app]]

As in a real-life coin flip, we see that we do not get the same result each time. Because we know this is a fair coin, we might say that the probability of landing on heads is 50%, and the probability of landing on tails is 50%. And while we may know for certain that the probability is the same for each outcome, we are no more able to predict what the result of the next flip will be. This is a consequence of **randomness**, where the outcome is uncertain until it is observed. Processes (such as flipping a coin) in which randomness occurs are known as **stochastic processes**. In contrast, a process that is not random is known as **deterministic**, where the outcome of the process is known in advance. An example of a determinsitic process may include writing the numbers 1-10 from largest to smallest.


Let's return now to the coin flip simulation, and investigate ways in which we can make our experiment slightly more complex. In the first implementation, we observed a single **event**, or possible outcome, from flipping a coin. Letting $n$ represent the number of events, we would say that this simulation was carried out with $n = 1$. In the example below, we repeat the simulation above, but this time, we are able to change the value of $n$:

[[app]]

Just as before, while having defined $n$ allows us to control the behavior of the simulation, it does not interfere with the effects of randomness. In other words, each time the simulation is run, a different outcome appears. Values such as $n$ that change the behavior of the simulation are known as **simulation parameters**. We will consider a number of parameters that are commonly used to direct the simulation.

Once an outcome is complete and the coins have been flipped, we say that it is **observed**, or that *we have observed the outcome of a random process*. Here, it is important to note that an observed outcome is no longer random -- once the data have been observed, we know what they are. Data collected through real life experiments, such as testing the efficacy of new drugs, or measuring the height of people in a population, are all observed data. As is often lamented by researchers and statisticians alike, real word experiments can often only be conducted a single time. Once the data is observed, it's observed. This is in stark contrast to the power of simulations, in which experiments can be conducted and data observed an arbitrary number of times. 

To see this in action here, consider the previous example in which we introduced the simulation parameter $n$, which allowed us to dictate the number of events, or coin flips, that were done in our experiment. Suppose the experiment that we are interested in involves flipping a coin five times: in this case, we have $n = 3$. When we ran the simulation above, the experiment was done once, as is often the case in the real world, and we were left with a single collection of *observed data*. It now makes sense to introduce a new simulation parameter, $N$, which dictates *how many times the experiment is to be done*. In this last instance, our simulation was done with $N = 1$. In the simulation below, we will continue to run the experiment with five coin flips ($n = 3$), but we will change the nubmer of times $N$ that the experiment is run.

[[app]]

It is important to note that the experiment itself is unchanged -- just as before, we are continuing to investigate what happens when a coin is flipped $n = 3$ times. By adjusting the value of $N$, we are now able to perform this same experiment as often as we wish. Even more importantly, one sees that even though the exact same experiment is performed a number of times, the *observed* values of the experiment are different. This is a consequence of randomness, and demonstrates that even when we know all of the details of an experiment, the outcome is uncertain until it is observed.

## Why Simulation

In the previous section, we learned that a simulation is a random process (of $n$ events) that can be performed an arbitrary ($N$) number of times. Our next step will be to understand how this process can empower us to better understand key concepts in statistics. This next section will contain a number of terms and concepts that will be formally introduced in later chapters, but for now, will serve to give context to the usefulness of simulations.

We will continue with the experiment above, where we are interested in flipping a fair coin $n = 3$ times. However, along with the experiment, we will also bring a question: if a coin is flipped three times, what is the probability that exactly two of the coins will be heads? Before we attempt answering this via simulation, it will be instructive to consider first the ways we might answer it without.

[[might be nice to "name" these methods, like pdf method and something else.]]

### Other Methods

<ol type = "1">
<li> As $n$ is small enough, we could simply list all of the possible outcomes of this experiment and count how many of these outcomes have exactly two heads. The space of outcomes for this experiment, denoted $\mathcal{S}$, is given by 

$$
\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\}
$$
    where $H$ represents a heads and $T$ a tails. Having listed them out, we see there are eight possible outcomes.     Next, we count how many of those have exactly two heads:

$$
\mathcal{S} = \{HHH, \color{red}{HHT}, \color{red}{HTH}, \color{red}{THH}, TTH, THT, HTT, TTT\}
$$
Dividing the events of interest by the total number of outcomes, we see that the probability of getting exactly two heads is $P(\text{# Heads = 2}) = \frac38$. </li>
<br>
<li> Often times, a statistical or stochastic process has an associated *probability function* that allows us to determine the probability of a set of outcomes. In this case, our coin flipping example follows what is known as a *binomial distribution*, which is given as 

$$
f(k; n,p) = \binom{n}{k}p^k(1-p)^{n-k}
$$
with probability of heads being $p = 0.5$, $n = 3$ total flips, and for our event, $k = 2$ heads. Substituting these numbers, we have

$$
\begin{align*}
P(\text{# Heads = 2}) &= \binom{3}{2} (0.5)^2 (0.5)^{3-2} \\
&= 3 \times (0.5)(0.5)(0.5) \\
&= 0.375.
\end{align*}
$$
Just as in the first method, we find that $P(\text{# Heads = 2}) = 0.375 = \frac38$ </li>
</ol>

While each of these methods is satisfactory (and mathematically equivalent), there are a number of situations in which they would be impractical to perform:

1. For example, in the first method, we determined the probability by counting the total number of events of interest (getting two heads) and dividing it by the total number of possible outcomes, in this case eight. For $n = 4$, there are 16 possible outcomes, and for $n = 10$, this number grows to 1024. In fact, the number of possible outcomes grows exponentially with the number of flips: with only $n = 25$ coin flips, there would be more possible outcomes in our solution space than the number of hours between today and the construction of the Great Pyramid of Giza, c.2580-2560 BC! Clearly, we need a method that is a bit more concise

2. Our second method, determining probability via the probability function, is far and away the most popular method, as the computation is simple and immediate. Unfortunatley, we are limited by the crucial fact that we have to know what the probability function is in order to compute it. For many real life processes, from flipping coins to estimating the number of phone calls a call center will receive in an hour, this function is readily known. For processes that are less simple, these functions can be nearly impossible to construct, leaving statisticians with no other way to determine the probability of events.


### Simulation Method

Before moving forward, let's quickly recap where we are right now:

1. Simulation is a tool that we have, governed by a set of parameters (in our case, $n$ and $N$), that can be used to replicate a random experiment an arbitrary number of times. Because this process is *stochastic*, or random, it will give a different outcome each time

2. We have specified an experiment we would like to perform, flipping a fair coin $n = 3$ times, and we would like to know the probability that exactly two of the flips will land on heads

3. We investigated two methods besides simulation for answering this question: enumerating the event space and calculating the probability function. These methods gave identical answers: in other words, they are *deterministic*

4. We saw that even though enumeration and probability functions worked fine for our problem, they can quickly become impractical or even impossible once the problem grows or becomes more complex


A careful reader might note that for a given random experiment (flipping a coin three times) and a specified outcome (exactly two heads), there is only a single correct answer to the question "What is the probability of getting exactly two heads when flipping a coin three times?", and this correct answer is precisely what was found using the deterministic enumeration and probability function methods. And if this is true, how could we possibly expect simulation, which gives us *random* outcomes, to give us the same correct result? The answer lies in one of the most profound results in all of statistics, the **law of large numbers**. We will cover this in much greater detail in later chapters, but for now, it suffices to know that the law of larger numbers states: "the average of the probabilities obtained from a large number of trials should be close to the true probability, and will tend to become closer are more trials are performed." In the context of our problem, we can restate this as "as the number of experiments performed $N$ becomes larger, the average of all the results will get closer and closer to the true result."


















