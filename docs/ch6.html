<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Probability Distributions | An Intuitive, Interactive, Introduction to Biostatistics</title>
  <meta name="description" content="First template!" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Probability Distributions | An Intuitive, Interactive, Introduction to Biostatistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="First template!" />
  <meta name="github-repo" content="ceward/introTextbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Probability Distributions | An Intuitive, Interactive, Introduction to Biostatistics" />
  
  <meta name="twitter:description" content="First template!" />
  

<meta name="author" content="Caitlin Ward and Collin Nolte" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch5.html"/>
<link rel="next" href="ch7.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch1.html"><a href="ch1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch1.html"><a href="ch1.html#ch1_s1"><i class="fa fa-check"></i><b>1.1</b> Statistics and Evidence-based Research</a></li>
<li class="chapter" data-level="1.2" data-path="ch1.html"><a href="ch1.html#ch1_s2"><i class="fa fa-check"></i><b>1.2</b> Scientific Method</a></li>
<li class="chapter" data-level="1.3" data-path="ch1.html"><a href="ch1.html#ch1_s3"><i class="fa fa-check"></i><b>1.3</b> Statistical framework</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch2.html"><a href="ch2.html"><i class="fa fa-check"></i><b>2</b> Data Summaries and Presentation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch2.html"><a href="ch2.html#ch2_s1"><i class="fa fa-check"></i><b>2.1</b> Introduction to Data</a></li>
<li class="chapter" data-level="2.2" data-path="ch2.html"><a href="ch2.html#ch2_s2"><i class="fa fa-check"></i><b>2.2</b> Categorical Data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch2.html"><a href="ch2.html#ch2_s2_ss1"><i class="fa fa-check"></i><b>2.2.1</b> Basic Categorical Summaries</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch2.html"><a href="ch2.html#ch2_s2_ss2"><i class="fa fa-check"></i><b>2.2.2</b> Advanced Categorical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch2.html"><a href="ch2.html#ch2_s3"><i class="fa fa-check"></i><b>2.3</b> Continuous Data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ch2.html"><a href="ch2.html#ch3_centrality"><i class="fa fa-check"></i><b>2.3.1</b> Measures of Centrality</a></li>
<li class="chapter" data-level="2.3.2" data-path="ch2.html"><a href="ch2.html#ch3_dispersion"><i class="fa fa-check"></i><b>2.3.2</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch2.html"><a href="ch2.html#ch2_s4"><i class="fa fa-check"></i><b>2.4</b> Advanced Data Visualizations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch3.html"><a href="ch3.html"><i class="fa fa-check"></i><b>3</b> Study Design</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch3.html"><a href="ch3.html#ch3_s1"><i class="fa fa-check"></i><b>3.1</b> Importance of Study Design</a></li>
<li class="chapter" data-level="3.2" data-path="ch3.html"><a href="ch3.html#ch3_s2"><i class="fa fa-check"></i><b>3.2</b> Pre-trial Bias</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch3.html"><a href="ch3.html#ch3_s2_ss1"><i class="fa fa-check"></i><b>3.2.1</b> Selection Bias</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch3.html"><a href="ch3.html#ch3_s2_ss2"><i class="fa fa-check"></i><b>3.2.2</b> Confirmation Bias/Placebo Effect</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch3.html"><a href="ch3.html#ch3_s2_ss3"><i class="fa fa-check"></i><b>3.2.3</b> Observer Bias</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch3.html"><a href="ch3.html#ch3_s3"><i class="fa fa-check"></i><b>3.3</b> During-trial Bias</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch3.html"><a href="ch3.html#ch3_s3_ss1"><i class="fa fa-check"></i><b>3.3.1</b> Non-response/Participant Bias</a></li>
<li class="chapter" data-level="3.3.2" data-path="ch3.html"><a href="ch3.html#ch3_s3_ss2"><i class="fa fa-check"></i><b>3.3.2</b> Compliance Bias</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch3.html"><a href="ch3.html#ch3_s4"><i class="fa fa-check"></i><b>3.4</b> Post-trial Bias</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch3.html"><a href="ch3.html#ch3_s4_ss1"><i class="fa fa-check"></i><b>3.4.1</b> Confounding</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch3.html"><a href="ch3.html#ch3_s4_ss2"><i class="fa fa-check"></i><b>3.4.2</b> Extrapolation Bias</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch3.html"><a href="ch3.html#ch3_s4_ss3"><i class="fa fa-check"></i><b>3.4.3</b> Publication Bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch4.html"><a href="ch4.html"><i class="fa fa-check"></i><b>4</b> Introduction to Probability and Simulation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch4.html"><a href="ch4.html#ch4_s1"><i class="fa fa-check"></i><b>4.1</b> Randomness and Simulation</a></li>
<li class="chapter" data-level="4.2" data-path="ch4.html"><a href="ch4.html#ch4_s2"><i class="fa fa-check"></i><b>4.2</b> Probability</a></li>
<li class="chapter" data-level="4.3" data-path="ch4.html"><a href="ch4.html#ch4_s3"><i class="fa fa-check"></i><b>4.3</b> Methods for Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch5.html"><a href="ch5.html"><i class="fa fa-check"></i><b>5</b> Advanced Probability</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch5.html"><a href="ch5.html#ch5_s1"><i class="fa fa-check"></i><b>5.1</b> Probability Operations</a></li>
<li class="chapter" data-level="5.2" data-path="ch5.html"><a href="ch5.html#ch5_s2"><i class="fa fa-check"></i><b>5.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="5.3" data-path="ch5.html"><a href="ch5.html#ch5_s4"><i class="fa fa-check"></i><b>5.3</b> Probabilities from tables (needs a different example)</a></li>
<li class="chapter" data-level="5.4" data-path="ch5.html"><a href="ch5.html#ch5_s5"><i class="fa fa-check"></i><b>5.4</b> Bayes’ Rule</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch6.html"><a href="ch6.html"><i class="fa fa-check"></i><b>6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch6.html"><a href="ch6.html#introduction-to-probability-distributions"><i class="fa fa-check"></i><b>6.1</b> Introduction to Probability Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="ch6.html"><a href="ch6.html#binomial-distribution"><i class="fa fa-check"></i><b>6.2</b> Binomial Distribution</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch6.html"><a href="ch6.html#plotting-the-pmf-delete"><i class="fa fa-check"></i><b>6.2.1</b> Plotting the PMF (delete?)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch6.html"><a href="ch6.html#normal-distribution"><i class="fa fa-check"></i><b>6.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="6.4" data-path="ch6.html"><a href="ch6.html#other-common-distributions"><i class="fa fa-check"></i><b>6.4</b> Other Common Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch6.html"><a href="ch6.html#poisson-distribution"><i class="fa fa-check"></i><b>6.4.1</b> Poisson Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch7.html"><a href="ch7.html"><i class="fa fa-check"></i><b>7</b> Sampling Distributions and the <br/> Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch7.html"><a href="ch7.html#introduction-to-sampling"><i class="fa fa-check"></i><b>7.1</b> Introduction to Sampling</a></li>
<li class="chapter" data-level="7.2" data-path="ch7.html"><a href="ch7.html#sampling-distributions"><i class="fa fa-check"></i><b>7.2</b> Sampling Distributions</a></li>
<li class="chapter" data-level="7.3" data-path="ch7.html"><a href="ch7.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch8.html"><a href="ch8.html"><i class="fa fa-check"></i><b>8</b> Introduction to Inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch8.html"><a href="ch8.html#ch8_s0"><i class="fa fa-check"></i><b>8.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="8.2" data-path="ch8.html"><a href="ch8.html#ch8_s1"><i class="fa fa-check"></i><b>8.2</b> Point Estimation and Confidence Intervals</a></li>
<li class="chapter" data-level="8.3" data-path="ch8.html"><a href="ch8.html#ch8_s2"><i class="fa fa-check"></i><b>8.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="8.4" data-path="ch8.html"><a href="ch8.html#ch8_s3"><i class="fa fa-check"></i><b>8.4</b> P-values</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Intuitive, Interactive, Introduction to Biostatistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch6" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Probability Distributions</h1>
<blockquote>
<p>“Statistics is the grammar of science.”</p>
<p>— Karl Pearson</p>
</blockquote>
<div class="objective-container">
<div class="objectives">
Learning Objectives
</div>
<ol style="list-style-type: decimal">
<li>Understand how a distribution represents a random process that creates data that is then observed</li>
<li>Understand how the parameters of a distribution govern how the data is generated [and with what probability]</li>
<li>Be able to identify which distributions underlying a given real world random process.</li>
</ol>
</div>
<div id="introduction-to-probability-distributions" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Introduction to Probability Distributions</h2>
<p>In Chapter 4, we introduced the idea of random processes, i.e. situations
in which the outcome can not be determined perfectly in advance. Random processes
are defined in terms of the <em>collection of possible events</em> (sample space) and their
<em>associated probabilities</em>. In that chapter, we saw three methods for calculating
probabilities - the enumeration method, the probability function method, and the
simulation method. In this chapter we will expand on the probability function
method, which uses a known function called a <strong>probability distribution</strong> to
determine the probability of each event.</p>
<p>Probability distributions are closely related to <strong>random variables</strong>, a numeric
variable that can take on different values depending on the outcome of a random
process. In previous mathematics courses you may have seen variables such as
<span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> used as placeholder values which are then solved for. For example,
you can solve for the variable <span class="math inline">\(x\)</span> in <span class="math inline">\(4x + 5 = 25\)</span>, to determine <span class="math inline">\(x = 5\)</span>. By
contrast, the outcome of a <em>random</em> variable cannot be predetermined. Instead,
we talk probabilistically about the likelihood of observing each possible outcome.
Random variables are typically denoted with capital letters, e.g., <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>,
whereas the observed outcome of the random process is denoted with lowercase letters
<span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>. For example, flipping a coin three times is a random process. We can
define the random variable <span class="math inline">\(X\)</span> to represent the number of heads we observe
between the three flips. <span class="math inline">\(X\)</span> can take on four possible values: 0, 1, 2, or 3. If
we observe 2 heads, we have <span class="math inline">\(x = 2\)</span>.</p>
<p>Most simply, a probability distribution (often just called a distribution)
is a method for taking a possible event as input, and giving us the corresponding
probability as output; the corresponding probability tells us how likely it is
that the specific event will occur, out of all of the possible events. We often
say random variables have or follow a probability distribution, as the
distribution quantifies the probability of observing the possible values a random
variable can take on. There are many useful probability distributions that have
been defined by mathematicians and statisticians to describe a variety of scenarios:</p>
<ul>
<li><p>Counting the number of successes in a fixed number of trials that can results
in either success or failure</p></li>
<li><p>Counting the number of failures before the first success in a series of success/failure trials</p></li>
<li><p>Describing the length of time between events that occur at a constant rate</p></li>
<li><p>Describing the blood pressure of adults</p></li>
</ul>
<p>One can represent a probability distribution visually using a <strong>probability histogram</strong>.
On the x-axis, we have the possible outcomes of the random process - the values
the random variable could take on. For each outcome, the bar height represents
the probability of observing that value. For the coin flipping example, the
probabilities of observing each possible number of heads can be represented as:</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The beauty of using probability distributions to describe the likelihood of all
outcomes of a random process is its simplicity. Probability distributions rely
on a small number of <strong>parameters</strong> which determine the distribution’s ``shape".</p>
<p>Something about expected values.</p>
<div class="definition-container">
<div class="definition">
<span id="def:unlabeled-div-1" class="definition"><strong>Definition 6.1  </strong></span> 
</div>
<p><strong>Probability Distribution: </strong> <em> A method for assigning probabilities to all possible events </em></p>
<p><strong>Random Variable: </strong> <em> A method for assigning probabilities to all possible events </em></p>
<p><strong>Probability histogram: </strong> <em> Values associated with a probability distribution that determine the distributions shape </em></p>
<p><strong>Parameters: </strong> <em> Values associated with a probability distribution that determine the distributions shape </em></p>
</div>
</div>
<div id="binomial-distribution" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Binomial Distribution</h2>
<p>One common probability distribution is the <strong>binomial distribution</strong>, which
describes the number of successes in a fixed number of independent trials
that can result in one of two outcomes (success or failure), when each trial has
the same probability of success. We have already seen one example of the
binomial distribution - flipping a coin three times counting the number of flips
that result in heads (success). Each flip has two possible outcomes (heads or
tails), the same probability of heads (50%), and we have predetermined the number
of trials (3 flips). Another example of the binomial distribution is rolling a
six-sided die 10 times and counting the number of rolls that result in a 5 or 6.
In this example, each die roll has a 1/3 chance of turning up a 5 or 6 and the
die will be rolled 10 times.</p>
<p>As you may have noticed in these binomial distribution examples, the distribution
can be used for various probabilities of success and numbers of trials. In fact,
these quantities define the two parameters of the binomial distribution. These
parameters are denoted as <span class="math inline">\(n\)</span> = the number of trials and <span class="math inline">\(p\)</span> = the probability of
success. For any valid value of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, we can use the binomial distribution
to compute the probability of observing any possible outcome. To illustrate this,
we will consider a binomial distribution with <span class="math inline">\(n = 10\)</span> trials. The following
figure shows the probability histograms for four different values of <span class="math inline">\(p\)</span>.</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-2-1.png" width="960" /></p>
<p>When <span class="math inline">\(p\)</span> is 0.2, we are more likely to observe a lower number of successes, than
to observe 8 or more successes. When <span class="math inline">\(p\)</span> is 50%, we are most likely to observe
5 of the 10 trials resulting in a success, with the probability decreasing as the
number of successes gets closer to 0 or 10. For a large success probability of 90%,
there is a very small chance we observe less than 6 successes and a much higher
probability of observing 9 or 10 successes.</p>
<p>In the previous chapter, we examined the possible events and associated probabilities with flipping a coin three times, where each side could land with equal probability. In particular, we noted the collection of possible events was given by</p>
<p><span class="math display">\[\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\},\]</span>
and the respective probabilities for the number of heads were</p>
<table>
<thead>
<tr class="header">
<th align="center"># Heads</th>
<th align="center">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">3/8</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">3/8</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1/8</td>
</tr>
</tbody>
</table>
<p>Might we consider this table a probability distribution? Perhaps, as it does give us a method for determining a probability for any given event. However, the usefulness of such a method comes into question as our experiment changes. What if instead we had flipped our coin 50 times? Or 100? What is needed instead is a method that is more robust to changes in the experiment.</p>
<p>Here, we introduce the <em>binomial distribution</em>, a probability distribution in which:</p>
<ol style="list-style-type: decimal">
<li>The total number of trials (or flips) is fixed in advance at some value, <span class="math inline">\(n\)</span></li>
<li>There are two possible events or outcomes within each trial</li>
<li>Each trial has the same probability of success (heads), which we will denote <span class="math inline">\(p\)</span></li>
<li>Each trial is independent of all other trials. Whatever result occurs on the first flip will have no impact on the second, and so on.</li>
</ol>
<p>Notationally, the binomial distribution is expressed as <span class="math inline">\(X \sim Bin(n,p)\)</span>, or “The random variable <span class="math inline">\(X\)</span> follows a binomial distribution with <span class="math inline">\(n\)</span> trials and probability of success, <span class="math inline">\(p\)</span>.” We can also express this with the following formula:</p>
<p><span class="math display">\[
P(X = x) = \binom{n}{x} p^x (1-p)^{n-x}
\]</span>
where <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> are the <em>parameters</em> of our distribution, and <span class="math inline">\(X\)</span> can take any of the values <span class="math inline">\(x = 0, 1, \dots, n\)</span>. Perhaps unfamiliar to us here is the leading term in the expression above, <span class="math inline">\(\binom{n}{x}\)</span>, called the <em>binomial coefficient</em>, which can be written as</p>
<p><span class="math display">\[
\binom{n}{x} = \frac{n!}{x!(n-x)!}
\]</span>
where <span class="math inline">\(n! = n \times (n-1) \times \dots \times 2 \times 1\)</span> (known as a factorial). By convention, we have that <span class="math inline">\(0! = 1\)</span>. In words, we might say <span class="math inline">\(\binom{n}{x}\)</span> as “<span class="math inline">\(n\)</span> choose <span class="math inline">\(x\)</span>” or “given that we have <span class="math inline">\(n\)</span> total trials, in how many ways might the outcome <span class="math inline">\(x\)</span> appear?” While this may seem daunting at first, the need for it is quite reasonable. Consider again our coin flipping experiment, where the possible outcomes were listed as</p>
<p><span class="math display">\[
\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\}.
\]</span></p>
<p>If we are interested in determining the probability of observing two heads, we note that there are multiple ways in which this occurs. We might ask ourselves, “If we have <span class="math inline">\(n = 3\)</span> flips, how many ways might we <em>choose</em> <span class="math inline">\(x = 2\)</span> heads?” Writing this with our binomial coefficient, we find that</p>
<p><span class="math display">\[
\binom{3}{2} = \frac{3!}{2!(3-2)!} = \frac{3 \times 2 \times 1}{2 \times 1 \cdot(1 \times 1)} = \frac62 = 3,
\]</span>
and indeed, 3 is precisely the number of outcomes in <span class="math inline">\(\mathcal{S}\)</span> in which two heads occur.</p>
<p>Now that we have <em>the number</em> of ways in which we might have two heads, we move on to determining the probability of such an event. Being a fair coin, we know that the probability of heads is <span class="math inline">\(p = 0.5\)</span>, and because these events are independent, we know that the probability <span class="math inline">\(P(HHT) = P(HTH) = P(THH)\)</span>. Consequently, we need only determine the probability for one of them and then multiply this number by 3, the number we found from the binomial coefficient. See that</p>
<p><span class="math display">\[
\begin{align*}
P(HHT) &amp;= P(H) \cdot P(H) \cdot P(T) \\
&amp;= \underbrace{(0.5)(0.5)}_{P(H) P(H)} \cdot \underbrace{(1 - 0.5)}_{{P(T)}} \\
&amp;= 0.125
\end{align*}
\]</span>
Putting these pieces together, we arrive precisely at the binomial distribution:</p>
<p><span class="math display">\[
\begin{align*}
P(X = 2) &amp;= \binom{3}{2} (0.5)^2 (1-0.5)^{3-2} \\
&amp;= 3 \times 0.125 \\
&amp;= 0.375
\end{align*}
\]</span>
which we note is precisely the value we computed [(in the table above)/(in chapter 4)]</p>
<p>Finally, we consider how we might use the probability distribution for the binomial to answer questions like, “If we have <span class="math inline">\(n = 3\)</span> coin flips, what is the probability that we get <em>at least</em> 2 heads?” We can express this as <span class="math inline">\(P(X \leq 2)\)</span>, and the solution becomes immediate once we recognize that</p>
<p><span class="math display">\[
\begin{align*}
P(X \leq 2) &amp;= P(X = 0) + P(X = 1) + P(X=2) \\
&amp;= 0.125 + 0.375+0.375\\
&amp;= 0.875.
\end{align*}
\]</span>
In general, the solution to the binomial probability <span class="math inline">\(P(X \leq x)\)</span> for <span class="math inline">\(n\)</span> events can be written</p>
<p><span class="math display">\[
P(X \leq x) = \sum_{k=0}^x \binom{n}{k} p^k (1-p)^{n-k}.
\]</span></p>
<div class="exercise-container">
<div class="exercise">
<span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 6.1  </strong></span> 
</div>
<ol>
<li>
Using the binomial probability distribution above, verify the correct values for <span class="math inline">\(X = 0,1,2,3\)</span> in table [number]
</li>
<li>
Assume that we have a bag of red and blue marbles, with 75% of them being blue and 25% being red. Each time we draw a marble, we will place it back in the bag before drawing again (this is known as <em>sampling with replacement</em>)
<ol>
<li>
If we draw 10 marbles from the bag, what is the probability that at least 4 of them will be blue?
</li>
<li>
If we draw 6 marbles from the bag, what is the probability that we will draw an odd number of red marbles?
</li>
<li>
What does the binomial distribution function look like assuming that <span class="math inline">\(n = 10\)</span>, when drawing a blue marble is considered a “success?”
</ol>
</li>
</ol>
</div>
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/binomial/" width="100%" height="800">
</iframe>
<div id="plotting-the-pmf-delete" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Plotting the PMF (delete?)</h3>
<p>It is often useful to create a visual representation of a pmf as well. Doing so quickly gives us an idea of where data tend to aggregate and how the data are dispersed. Below are two plots representing two different sets of parameters for the binomials distribution. What do you notice in how they differ? How are they similar? What impacts do the different parameters have on the distribution of the data?</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-4-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Of particular note here, we recall from the previous chapter that the sum of all possible probabilities must be equal to one. Visually, this is represented by the total area of the bars in our plot. Given that our bars our rectangles, we can find the area by considering that the width of each bar is equal to 1, and it’s height is given by the probability of <span class="math inline">\(X = x\)</span>, which can be found using the PMF. On the left hand side, for example, from left to right, we have</p>
<p><span class="math display">\[
\begin{align}
\text{Total Area } &amp;= \text{ \{Area of Heads = 0\} + \{Area of Heads = 1\} + } \\
&amp; \quad \ \ \text{\{Area of Heads = 2\} + \{Area of Heads = 3\}} \\
&amp;= (1 \times P(X = 0)) + (1 \times P(X = 1)) + (1 \times P(X = 2)) + (1 \times P(X = 3)) \\
&amp;= 0.125+0.375+0.375+0.125 \\
&amp;= 1
\end{align}
\]</span>
If, say, we are interested in the probability that <span class="math inline">\(X = 2\)</span> or <span class="math inline">\(X = 3\)</span>, we can add the area of the two corresponding bars. In this case, we find <span class="math inline">\(P(X = 2, 3) = 0.75\)</span>.</p>
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/binomial/" width="100%" height="800">
</iframe>
<p>[Include binomial app here. See that clicking bars adds to something. Explain changing settings, clicking bars, selecting different things. Come up with exercises below, i.e., prob that even if n = 8, prob if even n = 4, etc.]</p>
</div>
</div>
<div id="normal-distribution" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Normal Distribution</h2>
<p>Included below are histograms of the depth of Lake Huron from 1875-1972, the annual flow of the Nile river in cubic meters from 1871-1970, and the height in feet of 31 black cherry trees. What do these histograms seem to have in common?</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-5-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>What we see here are examples of a <em>normal distribution</em> (also known as a bell curve), one of the most ubiquitous distributions in all of statistics. The normal distribution is characterized by the “bell shape” that is symmetric about it’s mean [but maybe don’t say mean].</p>
<p>Like the binomial, the normal distribution is characterized by two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, representing the mean and the variance, respectively. The mean value, <span class="math inline">\(\mu\)</span>, indicates the location of the peak on the x-axis, whereas the variance, <span class="math inline">\(\sigma^2\)</span>, indicates the amount of dispersion about the mean. A random variable <span class="math inline">\(X\)</span> that follows a normal distribution can be expressed <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, or, “The random variable <span class="math inline">\(X\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.” The formula for the normal distribution is given as</p>
<p><span class="math display">\[
\begin{align*}
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}}.
\end{align*}
\]</span></p>
<p>Consider the two normal distributions below, with different values for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. Although they are centered at different locations and have different amounts of dispersion around the mean, they are both bell-shaped curves characteristic of the normal distribution:</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Given that the normal distribution appears so frequently in statistics, it is common practice to <em>standardize</em> a normal distribution so that it has a mean value of <span class="math inline">\(\mu = 0\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>. A normal random variable that has been standardized is called a <em>standard normal distribution</em> and is often written <span class="math inline">\(Z \sim N(0,1)\)</span>. We can consider again the histograms above, once they’ve been standardized:</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-7-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Unlike the binomial distribution, in which there are <span class="math inline">\(n\)</span> possible values that our random variable can take, the normal distribution represents a random variable that is <em>continuous</em> over a range of values. Instead of asking the probability of a specific value, say, <span class="math inline">\(Z = 0\)</span>, probabilities are given as the area under the curve for a certain interval. We might ask, “What is the probability that <span class="math inline">\(Z\)</span> is one standard deviation (<span class="math inline">\(\sigma\)</span>) away from 0?” pr perhaps, “What is the probability that <span class="math inline">\(Z &lt; 0\)</span>?”</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Because probabilities for continuous distributions are described as areas under the curve, their values are computed with integrals. [do we introduce probability tables or just go with the app?]</p>
<p>also maybe include</p>
<ul>
<li>subtract cdf?</li>
<li>probability that |Z| &gt; z?</li>
<li>what else?</li>
</ul>
<p>[Using app below, explore different parameter values. Use slider to select a range of probabilities. Note that the area of interest is highlighted. Do exercises with it]</p>
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/normal/" width="100%" height="800">
</iframe>
<div class="definition-container">
<div class="definition">
<span id="def:unlabeled-div-3" class="definition"><strong>Definition 6.2  </strong></span> 
</div>
<p><strong>Binomial Distribution: </strong> <em> A discrete distribution in which there are two possible outcomes, “events” and “non-events.” There parameters are <span class="math inline">\(n\)</span>, which dictate the number of trials, and <span class="math inline">\(p\)</span>, determining the probability of an event </em></p>
<p><strong>Normal Distribution: </strong> <em> A continuous distribution with two parameters that is symmetric about a mean value, <span class="math inline">\(\mu\)</span>, with a variance <span class="math inline">\(\sigma^2\)</span>. Many real world processes follow a normal distribution. </em></p>
<p><strong>Standard Normal Distribution: </strong> <em> A special case of the normal distribution, <span class="math inline">\(Z \sim N(0, 1)\)</span> </em></p>
<p><strong>Probability Mass Function: </strong> <em> A probability function used for discrete random variables. The probability of outcomes is given as a sum </em></p>
<p><strong>Probability Distribution Function: </strong> <em> A probability function used for continuous random variables. The probabilities of outcomes are taken over a range, given as an integral. </em></p>
</div>
</div>
<div id="other-common-distributions" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Other Common Distributions</h2>
<p>Having examined in detail both discrete and properties distributions, demonstrated with the binomial and normal distributions, respectively, we consider below a brief overview of other common distributions and their properites.</p>
<div id="poisson-distribution" class="section level3" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Poisson Distribution</h3>
<p>The Poisson distribution, like the binomial, is a <em>discrete</em> distribution, in that it concerns itself with count data. Specifically, a Poisson distribution describes the number of independent events that may occur within a fixed interval of time. For example, we may be interested in the number of cars that pass through a busy intersection from noon to 1pm every day, or the number of major floods that occur in an area every 100 years. Perhaps the most famous example of the Poisson distribution comes courtesy of Ladislaus Bortkiewicz, a Russian statistician who, in 1898, showed that the number of Prussian soldiers killed by being kicked by a horse in a twenty year period followed a Poisson distribution (also child suicides, but that’s less fun).</p>
<p>The Poisson distribution has a single parameter, <span class="math inline">\(\lambda\)</span>, which describes the rate at which events occur, and a random variable following a Poisson distribution may be expressed as <span class="math inline">\(X \sim Pois(\lambda)\)</span> (People who write <span class="math inline">\(X \sim Po(\lambda)\)</span> are heathens). A random variable following a Poisson distribution has the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>The value of <span class="math inline">\(X\)</span>, being a count, can be any non-negative integer, i.e., <span class="math inline">\(0, 1, 2, \dots\)</span> with no upper bound</li>
<li>The occurrence of one event in a time interval is independent of another event. One soldier being kicked by a horse has no impact on the probability of another solider being kicked by a horse.</li>
<li><span class="math inline">\(\lambda\)</span>, which may be any number greater than <span class="math inline">\(0\)</span>, describes the rate at which events occur</li>
<li>[Two events cannot occur at the exact same time, though they probably don’t need this]</li>
</ol>
<p>The distribution function of a Poisson random variable with rate <span class="math inline">\(\lambda\)</span> can be expressed</p>
<p><span class="math display">\[
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}
\]</span>
One surprisingly detail about the Poisson distribution is the relationship between the mean and the variance. For both, we have that <span class="math inline">\(E(X) = Var(X) = \lambda\)</span>.</p>
<div id="plots-for-poisson" class="section level4" number="6.4.1.1">
<h4><span class="header-section-number">6.4.1.1</span> Plots for Poisson</h4>
<p>As we look at the plot for the Poisson, we will notice one aspect in particular that distinguishes it from the plots of both the binomial and normal distributions: it is no longer symmetric. This is a consequence of the range of values that a Poisson random variable can take on. Whereas a binomial random variable was bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>, the number of trials conducted, and where the normal distribution allowed any real number, the Poisson is bounded below by <span class="math inline">\(0\)</span>, while having no theoretical upper bound. Given below is a plot of the distribution with <span class="math inline">\(\lambda = 2\)</span> and <span class="math inline">\(\lambda = 4\)</span> (it’s obvious here that choosing a specific value is inadequate. We can replace these plots with distribution exploration apps)</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-9-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Just as with the binomial distribution, we can determine the probability of an event or collection of events by determining the area of the bars in our plot. Below is an interactive app to do stuff. Exercises</p>
<iframe src="https://ph-ivshiny.iowa.uiowa.edu/collin/textbook/distribution/poisson/" width="100%" height="800">
</iframe>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch7.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
