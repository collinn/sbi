[
["index.html", "Seriously Interesting Statistics Textbook Introduction 0.1 Acknowledgements", " Seriously Interesting Statistics Textbook Caitlin Ward and Collin Nolte Introduction Welcome! This resource offers an interactive learning experience for introductory biostatistics. This resource was designed with the course BIOS:4120 Introduction to Biostatistics at the University of Iowa in mind, but could be used in any introductory statistics or biostatistics course. We combine textual explanations and embedded simulation-based Shiny applications, to provide students with an engaging learning resource that offers an intuitive illustration of core statistical concepts. The applications and associated exercises are targeted at conceptional understanding, as opposed to calculations. At this point in time, we provide chapters on: Probability distributions (text and application) Sampling distributions and Central Limit Theorem (text, applications, exercises) All typos and errors contained within this text are intentional with the goal of keeping the reader vigilant This resource is in a Beta phase 0.1 Acknowledgements We would like to acknowledge the funding which made the creation of this resource possible, provided by a University of Iowa Libraries OpenHawks grant. "],
["ch5.html", "1 Probability Distributions 1.1 Introduction to Probability Distributions 1.2 Flipping Coins 1.3 Functional Representation of Probability Distributions 1.4 Measuring Heights 1.5 Other Common Distributions", " 1 Probability Distributions Learning Objectives Understand how a distribution represents a random process that creates data that is then observed Understand how the parameters of a distribution govern how the data is generated [and with what probability] Be able to identify which distributions underlying a given real world random process. In the previous chapter, we introduced the idea of random processes, which are situations where the outcome can not be determined perfectly in advance. We encounter random processes all the time in our lives, from the exact amount of time it takes to get to class from your home, to determining the winner of a football game. In any case, the random process is defined in terms of the collection of possible events and their associated probabilities. While the number of unique outcomes of a random process may be impossible to count, we will find that many of them have a very similar underlying structure dictating how these events occur. Formally recognizing the properties of these structures, as well as understanding how they can be used to make predictions, motivates the goals of this chapter. 1.1 Introduction to Probability Distributions Most simply, a probability distribution (often just called a distribution) is a method for taking a possible event as input, and giving us the corresponding probability as output. The corresponding probability tells us how likely it is that the specific event will occur, out of all of the possible events. A helpful metaphor is to consider a machine that produces these events at random frequencies corresponding to their associated probabilities. That is, if our machine only creates green marbles and red marbles, and the probability of producing a green marble is 75%, then on average, our machine will make 3 green marbles for each red one. In this sense, we can think of distributions as “data generating mechanisms” - the distributions govern how the machine which generates data works. To continue with the machine metaphor, it will be important for us to distinguish between two machines that are completely different, and two machines that are the same but tuned to different settings. These settings, which we call distribution parameters, dictate many aspects of how the machine will generate data, including the range of likely events, how likely these events are to occur, and how much variability we might expect in the events that we observe. To briefly illustrate, we might first consider a normal distribution, which is governed by two parameters: the mean value, \\(\\mu\\), and the amount of variability, \\(\\sigma^2\\). A plot of two normal distributions is given below: As we can see, these two curves are quite similar, and indeed, the underlying process that created each of them is the same. What is different, however, are the parameters governing how the data were generated. With this in mind, we have three goals for the present chapter Understand how a distribution represents a random process that creates data that is then observed Understand how the parameters of a distribution govern how the data is generated [and with what probability] Be able to identify which distributions underlying a given real world random process. Definitions Probability Distribution: A method for assigning probabilities to all possible events Distribution Parameters: Values associated with a probability distribution that determine how the data is generated 1.2 Flipping Coins In the previous chapter, we examined the possible events and associated probabilities with flipping a fair coin three times. In particular, we noted the collection of possible events was given by \\[\\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\},\\] and the respective probabilities for the number of heads were # Heads Probability 0 1/8 1 3/8 2 3/8 3 1/8 In what ways might we formalize this as a process? We might start by identifying a few details about this experiment. First, we know that we are interested in coin flips, where each flip could be either one of two outcomes. We might also recognize that in this experiment we flipped a coin three times, with each flip having an equal probability of being Heads as it did Tails. In light of our previous discussion, which of these properties seem characteristic of a more general underlying process, and which of these could be changed while retaining the more general form? If we had flipped the coin 50 times, would the process be fundamentally different? Would this process be different if the probability of Heads was twice that of Tails? What we have identified above is a data generating mechanism, or distribution, known as the binomial distribution, in which each outcome is one of two states. In this example, the two states were Heads and Tails, but it could just as easily be described as success vs failure, adverse reaction vs non-adverse reaction, death vs non-death, etc.,. Most generally, we will consider the outcome to be either an “event” or a “non-event.” We next turn our attention the parameters of the distribution. As you may have guessed, there are two parameters associated with the binomial distribution, namely the number of observations (or flips), denoted \\(n\\), and the probability of a particular outcome being classified as an event, denoted \\(p\\). In our coin flipping example, flipping the coin three times gives us a parameter value of \\(n = 3\\). As we were interested in counting the number of Heads, we will call this our “event,” and note that it occurs with probability \\(p = 0.5\\). Together, these pieces define everything we need to know about a random process that follows a binomial distribution. Notionally, we write \\[ X \\sim Bin(n, p)\\] or, “the random variable \\(X\\) follows a binomial distribution with \\(n = 3\\) and probability of event \\(p = 0.5\\).” Our coin flipping example would then be expressed as \\(X \\sim Bin(n = 3, p = 0.5)\\), where \\(X\\) is our experiment. 1.3 Functional Representation of Probability Distributions We turn our attention now to the practical problem of determining how we might relate the idea of a probability distribution to determine the actual probabilities of given outcomes. From our definition above, we see that at a minimum, all we need is a method to assign a probability to a given event; within these bounds, we have a number of options available. Perhaps the most direct method of doing so consists of counting each of the possible outcomes by hand and determining their probabilities, which is precisely what was done when we identified \\(\\mathcal{S}\\) and then counted the frequency in which different numbers of Heads occurred. This, of course, can become cumbersome quite quickly: with only \\(n = 3\\), we identified a total of 8 separate outcomes. If \\(n\\) were 4, this would increase to 16. One can quickly see the issue when considering an experiment in which the total number of coin flips was equal to \\(n = 50\\). This example was further made easier by the fact that each of the outcomes was equally likely; if the value of \\(p\\) was anything but \\(0.5\\), our task of assigning probabilities to these outcomes would have been significantly more challenging (see Chapter 4). Another possibility involves the use of simulation, the justification for which is covered in more detail in another chapter. By simulating this experiment a large number of times, we can determine the relative probabilities by counting the relative frequency of each outcome, which saves us the trouble of having to compute them mathematically. Here, we simulate this experiment \\(N = 10,000\\) times, and record the total number of heads in each experiment, dividing by the total number of experiments to get our desired probability Number of Heads Fraction Observed Probability True Probability 0 1,315/10,000 0.1315 0.125 1 3,712/10,000 0.3712 0.375 2 3,744/10,000 0.3744 0.375 3 1,229/10,000 0.1229 0.125 Of course as we can see, simulations have their own limitations: they often require a large number of replications, and because of randomness, the observed probabilities will rarely be exactly equal to the true probabilities. Nonetheless, simulations prove to be exceedingly useful when our experiment is complicated, or if a known distribution function for our problem does not exist. Our final method for specifying a probability distribution is with the use of a mathematical function, often referred to as a probability distribution function (pdf) or probability mass function (pmf), with the former reserved for continuous variables and the latter for those that are discrete. The binomial distribution, consisting of discrete outcomes, is specified with a pmf. For \\(X \\sim Bin(n, p)\\), we have the following formula: \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x} \\] where \\(n\\) and \\(p\\) are our distribution parameters, and \\(X\\) can take any of the values \\(x = 0, 1, \\dots, n\\). Perhaps new to us here is the leading term in the expression above, \\(\\binom{n}{x}\\), called the binomial coefficient, which can be written as \\[ \\binom{n}{x} = \\frac{n!}{x!(n-x)!} \\] where \\(n! = n \\times (n-1) \\times \\dots \\times 2 \\times 1\\) (known as a factorial). In words, we might say \\(\\binom{n}{x}\\) as “\\(n\\) choose \\(x\\).” While this may seem daunting at first, the need for it is quite reasonable. Consider again our coin flipping experiment, where the possible outcomes were listed as \\[ \\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}. \\] If we are interested in determining the probability of observing two Heads, it is of note that there are a number of instances above in which two Heads occurs. We might ask ourselves, “if we have \\(n = 3\\) flips, how many ways might we choose \\(x = 2\\) heads?” Writing this with our binomial coefficient, we find that \\[ \\binom{3}{2} = \\frac{3!}{2!(3-2)!} = \\frac{3 \\times 2 \\times 1}{2 \\times 1 \\cdot(1 \\times 1)} = \\frac62 = 3, \\] and indeed, 3 is precisely the number of outcomes in \\(\\mathcal{S}\\) in which two Heads occur. Finally, using the pmf above, we can substitute in our distribution parameters \\(n = 3\\) and \\(p = 0.5\\) to find the distribution function that describes our experiment: \\[ P(X = x) = \\binom{3}{x} (0.5)^x (1-0.5)^{3-x} \\] [Exercise: verify that the probabilities returned by the binomial pmf match the true probabilities in the table above by plugging in values for \\(X = \\{0, 1,2,3\\}\\). ] 1.3.1 Plotting the PMF It is often useful to create a visual representation of a pmf as well. Doing so quickly gives us an idea of where data tend to aggregate and how the data are dispersed. Below are two plots representing two different sets of parameters for the binomials distribution. What do you notice in how they differ? How are they similar? What impacts do the different parameters have on the distribution of the data? Of particular note here, we recall from the previous chapter that the sum of all possible probabilities must be equal to one. Visually, this is represented by the total area of the bars in our plot. Given that our bars our rectangles, we can find the area by considering that the width of each bar is equal to 1, and it’s height is given by the probability of \\(X = x\\), which can be found using the PMF. On the left hand side, for example, from left to right, we have \\[ \\begin{align} \\text{Total Area } &amp;= \\text{ \\{Area of Heads = 0\\} + \\{Area of Heads = 1\\} + } \\\\ &amp; \\quad \\ \\ \\text{\\{Area of Heads = 2\\} + \\{Area of Heads = 3\\}} \\\\ &amp;= (1 \\times P(X = 0)) + (1 \\times P(X = 1)) + (1 \\times P(X = 2)) + (1 \\times P(X = 3)) \\\\ &amp;= 0.125+0.375+0.375+0.125 \\\\ &amp;= 1 \\end{align} \\] If, say, we are interested in the probability that \\(X = 2\\) or \\(X = 3\\), we can add the area of the two corresponding bars. In this case, we find \\(P(X = 2, 3) = 0.75\\). [Include binomial app here. See that clicking bars adds to something. Explain changing settings, clicking bars, selecting different things. Come up with exercises below, i.e., prob that even if n = 8, prob if even n = 4, etc.] 1.4 Measuring Heights Often, our data will not fit nicely into a finite number of discrete categories, leaving us with continuous data that are described with a probability distribution function, or pdf. As our data does not fit neatly into categorical bins, many of the techniques described above will not work in the same way here. Rest assured, the idea is exactly the same. While we will spare the technical details here, interested readers may consider what follows to be analogous to the cases presented above when the number of “bins” becomes infinite. Our motivating example here will consider the process of determining the height of individuals within a population. Unlike the binomial distribution, where the underlying process and associated parameters were easily teased apart, the components here are less obvious, and some care will be needed to identify them. Height data, like many things in the natural world, tend to follow a symmetric distribution, where most observations tend to gather around a mean value, with observations deviating from the mean being equally likely to fall some distance above the mean as below, their frequencies becoming smaller as this distance increases. That is to say, if the mean height of a population is 68 inches, an individual is equally likely to be 67 inches tall as they are 69 inches. Similarly, an individual is equally likely to be 64 inches as they are 82. However, given their proximity to the mean value, an individual is far more likely to be either 67 or 69 inches (1 inch from the mean) than they are to be either 64 or 82 inches (4 inches from the mean). The process described above describes what is known as a normal distribution, colloquially referred to as a “bell curve.” There are a number of properties that together characterize a normal distribution There are two parameters for the normal distribution, the mean \\(\\mu\\) (pronounced “myu”) and the variance \\(\\sigma^2\\) (“sigma squared”) \\(\\mu\\) is the mean, or expected value, and represents the most probable value of the distribution. That is, observations from a normal distribution are more likely to be close to \\(\\mu\\) than away from it Observations are equally likely to be the same magnitude above \\(\\mu\\) as they are below it. In other words, the distribution is centered around \\(\\mu\\). We see this concept expressed in everyday language when we offer estimates of some value: “The cost is ‘x,’ plus or minus ‘y’” The second parameter, \\(\\sigma^2\\), describes how concentrated values are around the mean. The smaller the value of \\(\\sigma^2\\), the more observations that will be close to \\(\\mu\\). Likewise, larger values of \\(\\sigma^2\\) result in higher dispersion, or more values further away from \\(\\mu\\). Notationally, if a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we write \\(X \\sim N(\\mu, \\sigma^2)\\). A special case of this that will be explored in following chapters is known as a standard normal distribution, which arises when the mean value is \\(\\mu = 0\\), and the variances is \\(\\sigma^2 = 1\\). This distribution is often written with its own letter \\(Z\\), as in \\(Z \\sim N(0, 1)\\). Another critical difference between a continuous and discrete random variable is the way in which we determine probability. In the discrete case, we could enumerate all of the events and determine their relative frequency. Alternatively, we could run a simulation and simply count how often each outcome occurred. In the continuous case, however, there is no finite set of possibilities (i.e., somebody could be 68\" tall, 68.1\" tall, 68.01\", …), and any attempts to enumerate these will only terminate in frustration; we will determine the implications of this below. In the meantime, however, we will rejoice in knowing that a normal distribution can be mathematically represented by it’s probability function: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\ e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\] As we can see, the pdf contains both of the distribution parameters, \\(\\mu\\) and \\(\\sigma^2\\). As we saw at the beginning of the chapter, different values for these parameters gives us different curves: Of particular interest above, notice how the value of \\(\\mu\\) changes where the data are aggregated, and similarly, note how larger values of \\(\\sigma^2\\) results in a greater amount of dispersion. Let’s now return to the issue of determining the probability of a particular event. In the discrete case, we saw that we could examine the plot of the pmf and multiply the width of each bin (which was equal to 1), with the height of the bin, given by the pmf While the pdf here does indeed give us the “height,” we quickly run into an issue when considering the width: the only way we can have an “infinite” number of bins for each outcome is to assign each bin a width of 0. As such, the probability of any particular event is unintuitively assigned a probability of zero. This apparent shortcoming can thankfully be remedied with the tools of calculus. Where discrete observations allow us to take a sum, the analogous case for continuous intervals is satisfied by the use of the integral. As it no longer makes sense to consider the probability of specific events (all of which will be zero), we instead consider the probability that an observation falls within a range of events. For example, if we have a random variable with \\(X \\sim N(\\mu, \\sigma^2)\\), the probability that \\(X &gt; 3\\) can be written \\[ P(X &gt; 3) = \\int_3^{\\infty} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\ e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\ dx \\] Similarly, if we were curious to know the probability that \\(X\\) was between a range of values, say \\(2 &lt; X &lt; 5\\), we would write \\[ P(2 &lt; X &lt; 5) = \\int_2^{5} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\ e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\ dx \\] Fortunately for us today, this no longer need be computed by hand. A number of computational resources are able to compute this for us with minimal effort. [Using app below, explore different parameter values. Use slider to select a range of probabilities. Note that the area of interest is highlighted. Do exercises with it] Definitions Binomial Distribution: A discrete distribution in which there are two possible outcomes, “events” and “non-events.” There parameters are \\(n\\), which dictate the number of trials, and \\(p\\), determining the probability of an event Normal Distribution: A continuous distribution with two parameters that is symmetric about a mean value, \\(\\mu\\), with a variance \\(\\sigma^2\\). Many real world processes follow a normal distribution. Standard Normal Distribution: A special case of the normal distribution, \\(Z \\sim N(0, 1)\\) Probability Mass Function: A probability function used for discrete random variables. The probability of outcomes is given as a sum Probability Distribution Function: A probability function used for continuous random variables. The probabilities of outcomes are taken over a range, given as an integral. 1.5 Other Common Distributions Having examined in detail both discrete and properties distributions, demonstrated with the binomial and normal distributions, respectively, we consider below a brief overview of other common distributions and their properites. 1.5.1 Poisson Distribution The Poisson distribution, like the binomial, is a discrete distribution, in that it concerns itself with count data. Specifically, a Poisson distribution describes the number of independent events that may occur within a fixed interval of time. For example, we may be interested in the number of cars that pass through a busy intersection from noon to 1pm every day, or the number of major floods that occur in an area every 100 years. Perhaps the most famous example of the Poisson distribution comes courtesy of Ladislaus Bortkiewicz, a Russian statistician who, in 1898, showed that the number of Prussian soldiers killed by being kicked by a horse in a twenty year period followed a Poisson distribution (also child suicides, but that’s less fun). The Poisson distribution has a single parameter, \\(\\lambda\\), which describes the rate at which events occur, and a random variable following a Poisson distribution may be expressed as \\(X \\sim Pois(\\lambda)\\) (People who write \\(X \\sim Po(\\lambda)\\) are heathens). A random variable following a Poisson distribution has the following assumptions: The value of \\(X\\), being a count, can be any non-negative integer, i.e., \\(0, 1, 2, \\dots\\) with no upper bound The occurrence of one event in a time interval is independent of another event. One soldier being kicked by a horse has no impact on the probability of another solider being kicked by a horse. \\(\\lambda\\), which may be any number greater than \\(0\\), describes the rate at which events occur [Two events cannot occur at the exact same time, though they probably don’t need this] The distribution function of a Poisson random variable with rate \\(\\lambda\\) can be expressed \\[ P(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\] One surprisingly detail about the Poisson distribution is the relationship between the mean and the variance. For both, we have that \\(E(X) = Var(X) = \\lambda\\). 1.5.1.1 Plots for Poisson As we look at the plot for the Poisson, we will notice one aspect in particular that distinguishes it from the plots of both the binomial and normal distributions: it is no longer symmetric. This is a consequence of the range of values that a Poisson random variable can take on. Whereas a binomial random variable was bounded between \\(0\\) and \\(n\\), the number of trials conducted, and where the normal distribution allowed any real number, the Poisson is bounded below by \\(0\\), while having no theoretical upper bound. Given below is a plot of the distribution with \\(\\lambda = 2\\) and \\(\\lambda = 4\\) (it’s obvious here that choosing a specific value is inadequate. We can replace these plots with distribution exploration apps) Just as with the binomial distribution, we can determine the probability of an event or collection of events by determining the area of the bars in our plot. Below is an interactive app to do stuff. Exercises "],
["ch6.html", "2 Sampling Distributions and the Central Limit Theorem 2.1 Introduction to Sampling 2.2 Sampling Distributions 2.3 Central Limit Theorem", " 2 Sampling Distributions and the Central Limit Theorem “While nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” - Elizur Wright “Everything that can be counted does not necessarily count; everything that counts cannot necessarily be counted.” - Albert Einstein Learning objectives Learn to differentiate between statistics and parameters Understand the concept of sampling distributions Conceptual understanding of Central Limit Theorem and how it applies to sample means 2.1 Introduction to Sampling Suppose policy makers and public health experts in Iowa are concerned with Iowan’s fast food intake. To understand this further, the officials want to determine the average monthly spending on fast food for all Iowans. However, it would be very expensive and costly to have all 3.2 million Iowans track and report their total monthly spending on fast food. And certainly many Iowans would not be willing to share personal financial details with public health researchers or government officials. This means it is impossible for the officials to know the true mean amount of money that Iowans spend on fast food each month. The true numeric quantity about the population, such as this, is known as a population parameter. While we will never ever know the truth, there are still options. The officials can take a sample of Iowans and have those that consent to share their information report their monthly spending on fast food. Then, they can take the sample mean, which gives them an estimate of the average monthly spending on fast food. The sample mean is an example of a sample statistic, which is a numerical quantity about the sample. In other words, statistics are what investigators know, and parameters are what investigators want to know. The statistical framework (pictured below) allows us to make inference about population parameters using sample statistics. This means the researchers can use a random sample of Iowans’ monthly fast food spending to make generalizations about the average monthly spending for all Iowans. The numerical quantities of interest can be many different things - means, proportions, standard deviations, etc.. In the fast food example, the parameter of interest was a population mean and the sample statistic was the sample mean. In this chapter, we will specifically focusing on estimating means. Figure 2.1: Statistical Framework The statistical framework outlines the process of making inference using sample statistics to estimate population parameters. Later chapters in this textbook will cover the mathematical details of this process. In trying to make proper inference, we also want to ensure our estimated statistic is measuring the parameter well. To that end, there are two major statistical issues we concern ourselves with: On average, does our estimate tend to be centered around the true answer, or is it biased? How much variability is there likely to be in our sample? The difference between our estimate and our parameter is called bias. Variance describes the spread of our data, and is sometimes called noise. Sometimes we talk about spread in terms of precision, which is the inverse of variance. The more precision we have, the less variance, and vice versa. A good statistic will have little or no bias and would have minimal variability. We can visualize both bias and variability using a dart board analogy. Figure 2.2: Bias and Variability Illustration On the upper left target, the darts are hitting the bullseye with low bias and low variability, shown by the darts having minimal spread and all hitting the bullseye. This is the most desirable outcome when the goal is to hit the bullseye. In the upper right target, we see a lot more variability, as the darts are no longer highly concentrated on the bullseye. However, the dart locations are still centered around the bullseye, and this is why we consider this target to have low bias. This situation is less ideal than having low bias and low variance, but is still preferable to targets in the bottom row, where we have high bias. In the bottom left target, we have high bias and low variability, evidence by the high concentration of darts that are not hitting the bullseye. The consistency is why we saw there is low variability, but if our goal is to hit the bullseye, we are not going to achieve that goal. Finally, the bottom right target is the worst possible outcome - high bias and high variability. We are not hitting the bullseye and the darts are landing erratically. Statisticians work hard to develop inferential procedures that allow us to estimate population parameters with little to no bias and low variability. Definitions Population Parameter: The true numeric quantity about a population Sample Statistic: A numerical quantity about the sample Inference: The process of making generalizations about the population Bias: The difference between our estimate and our parameter Variability: The spread or error in the data Precision: Inverse of variance, how narrow the data is 2.2 Sampling Distributions In order to make inference, we must be able to quantify our uncertainty about the population parameter based on our sample data. To quantify our uncertainty, we must first establish where it comes from - the sampling process. Any time we take a sample from the population, we will get a different sample mean \\(\\overline{x}\\). In other words, \\(\\overline{x}\\) is a numeric quantity which assumes a value based on the outcome of a random experiment - the process of drawing a sample at random from the population. This should sound familiar - this is the definition of a random variable! We can use this logic for any statistic of interest. As we’ve described in the past, random variables have probability distributions. When the random variable is a statistic, this probability distribution is called the sampling distribution. Sampling distributions reflect which values of the statistic are likely and which values are improbable. As we will see later in this chapter, this uncertainty depends on how many individuals we sample and how variable the data we measure is. Exercises The applet below is designed to help you get familiar with the concept of random sampling. The plot on the left side gives the population distribution. Each subject in the population is represented by one box, and the population consist of 26 people. The x-axis represents the value of the outcome variable for each individual, and when multiple people have the same value, their boxes are stacked on top of each other. For example, we have one subject in the population with a value of 0, three subjects with values of 1, and so on. The plot on the right displays the values for the subjects in our sample, and these subjects are indicated by green coloring in both the population and sample distributions. We can take samples from our population of various sizes by changing the value of the slider at the top and clicking on the “Sample” button. Set the sample size to 2 and click the “Sample” button five times. Fill out the following table with the sample means observed in your five samples. Sample Number Sample mean 1 2 3 4 5 Did you get the same sample mean for any of your five samples? Was the sample mean ever equal to the population mean in any of your five samples? If not, how close were your sample means to the true population mean? Now set the sample size to 15 and click the “Sample” button five times. Fill out the following table with the sample means observed in your five samples. Sample Number Sample mean 1 2 3 4 5 Did any of your five simulations give you a sample mean equal to the population mean? how close were your sample means to the true population mean? Compare your results from problem 1 and problem 2. In either case were you able to obtain a sample mean equal to the population mean? Which one had sample means with more variability? What does this indicate about the relationship between the sample size and the distribution of sample means in the context of repeated sampling? As we’ve seen earlier in this book, we will use upper-case letters to denote random variables, and lower-case letters to denote possible values they can take on. When it comes to the sampling distribution of the mean, \\(\\overline{X}\\) is the random variable, which arises from repeated random sampling from the population and then taking a sample mean, and \\(\\overline{x}\\) is an observed sample mean we might see. In describing a sampling distribution, we are often interested in the mean and standard deviation - this gives information about typical values the statistic might take on as well as how spread out the distribution is. We refer to the mean of a statistic as the expected value and the standard deviation as the standard error. For the random variable describing the sample mean, these are denoted as \\(E(\\overline{X})\\) and \\(SE(\\overline{X}) = \\sqrt{Var(\\overline{X})}\\), respectively. Let’s return to the fast food scenario to motivate our development, but for now let’s assume there are only five individuals in the entire population. The monthly spending for those five individuals (rounded to the nearest dollar) is as follows: Person Monthly Spending A 8 B 22 C 22 D 36 E 50 The population mean can be found by taking the average monthly spending of these five individuals, as they are the only people in this population. This means the population mean is 27.6. Now suppose we are taking samples of three individuals from this population. There are \\(\\binom{5}{3}\\) = 10 ways we can sample three people from this population. Since this is a small population, we can enumerate all possible samples, the values we would obtain for the monthly spending in each sample, and then the sample mean monthly fast food spending for each sample: Sample Values \\(\\overline{x}\\) (A, B, C) (8, 22, 22) 17.33 (A, B, D), (A, C, D) (8, 22, 36), (8, 22, 36) 22.00 (A, B, E), (A, C, E), (B, C, D) (8, 22, 50), (8, 22, 50), (22, 22, 36) 22.00 (A, D, E), (B, C, E) (8, 36, 50), (22, 22, 50) 26.67 (B, D, E), (C, D, E) (22, 36, 50), (22, 36, 50) 36.00 One thing to note is that none of the sample means are actually equal to the population mean. This is often the case! Based on this table, we can construct the sampling distribution of \\(\\overline{X}\\) for a sample of size three. Why is our sample mean often not equal to the population mean? Probability P(\\(\\overline{x}\\) = 17.33) 0.1 P(\\(\\overline{x}\\) = 22) 0.2 P(\\(\\overline{x}\\) = 26.67) 0.3 P(\\(\\overline{x}\\) = 31.33) 0.2 P(\\(\\overline{x}\\) = 36) 0.2 With this probability distribution, we can get the expected value of the sampling distribution. Whenever we think about probability, we are thinking about long-run frequencies, so when we think about the expected value, we are thinking about the average sample mean if we were to take repeated samples of size three from this population. The probability distribution indicates that if we took samples of size three from this population over and over again (say 1,000 times), we would end up with 10% with a mean of 17.33, 20% with a mean of 22, 30% with a mean of 26.67; and so on. So the expected value (average) we would observed if we kept taking samples over and over would be: (0.1 \\(\\times\\) 17.33) + (0.2 \\(\\times\\) 22) + (0.3 \\(\\times\\) 26.67) + (0.2 \\(\\times\\) 31.33) + (0.2 \\(\\times\\) 36) = 27.6 But wait! That is the population mean. This illustrates an extremely powerful property of the sample mean - the expected value of the sample mean (\\(\\overline{X}\\)) is equal to the population parameter (\\(\\mu\\))! Because of this, we say that the sample mean is an unbiased estimator of the population mean. While we illustrated the property with a small example, this holds regardless of the population or sample size. Definitions Expected Value: The mean of a statistic in repeated sampling Standard Error: The standard deviation of a statistic (including the mean) in repeated sampling Unbiased Estimator: The statistic with an expected value equal to the true population parameter 2.3 Central Limit Theorem In the last section, we used a toy example to conceptualize repeated sampling and the property of the sample mean being an unbiased estimator. Now we will formalize these properties. For any population distribution which can be described by a random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the sample distribution of the mean based on sample of size \\(n\\) (\\(\\overline{X}_n\\)) has the following properties: The expected value of the sampling distribution is \\(\\mu\\) \\[E(\\overline{X}_n) = \\mu\\] The variance of the sampling distribution is \\[Var(\\overline{X}_n) = \\frac{\\sigma^2}{n}\\] Using these formulas we are able to describe the typical value of the sample mean and how variable the sample mean is in repeated sampling. Notice that the expected value does not depend on the sample size, \\(n\\), but the variance does. This means regardless of the sample size, the sample mean is an unbiased estimator of the population mean. But, as the sample size increases, the variance decreases, and we get a more precise estimate of the population mean. As we’ve seen in the past, it is more common to talk about the standard deviation than the variance and the standard deviation is the square root of the variance. The quantity \\(\\sqrt{Var(\\overline{X}_n)} = \\sigma / n\\) is primarily called the standard error of the mean, and is denoted by \\(SE(\\overline{X}_n)\\). We use the term standard error to make it really clear that it is referencing the sampling distribution of the mean, but all standard errors could be referred to as standard deviations. Conversely, not all standard deviations could be referred to as a standard error - the term is only used in describing the distribution of the sample mean. In addition to knowing the mean and standard error of the sample mean, we can also describe the distribution of sample means in repeated sampling. As we discussed in the last chapter on probability distributions, a distribution for the sample mean provides us with a way to quantify which sample mean values are likely and which values are unlikely. As it turns out, when we repeatedly sample from a population, and continue to calculate the sample mean from each sample, the distribution of the sample means will be normally distributed. This brings us what is largely considered the “fundamental theorem of statistics” Central Limit Theorem If the sample size is “large,” the sampling distribution of \\(\\overline{X}\\) is approximately normal, regardless of the characteristics of the underlying population. \\[\\overline{X}_n \\sim N(\\mu, \\sigma^2/n)\\] Generally, we consider a sample to be “large” enough if the sample size, \\(n \\geq 30\\). Because the variance of \\(\\overline{X}\\) is inversely related to the sample size, as the \\(n\\) increases, the variance decreases and the distribution becomes more concentrated. This is one of the most important, remarkable, and powerful results in all of statistics. In the real world, we rarely know the underlying population distribution of our data, however, the Central Limit Theorem (CLT) says: we don’t have to! Exercises The applet below is designed to help your conceptual understanding of the CLT. You can change the underlying population distribution, the size of the samples taken (\\(n\\)), and the number of experiments performed. Each experiment consists of taking a sample of the specified size from the population and calculating the sample mean. The top left panel (blue) shows the underlying population distribution. The top right panel (red) shows the data from the most recent sample, with the sample mean from that specific sample indicated by the dashed black line. The bottom panel (gold) shows us the distribution of sample means from all the experiments, with the observed mean of the sample means indicated by the dashed black line and the true underlying population mean indicated by the solid blue line. Once you select the parameters to the values you want, click “Run Simulation” to tabulate the results. Set the population distribution to Normal, the sample size to 30, and perform 100 experiments to collect 100 sample means. Describe the histogram of the the data from the last experiment. Comment on the modality and skew. Describe the histogram of the distribution of sample means from all experiments. What is the range of values observed for the distribution of sample means? How does this compare the the range of values observed in the data from the last experiment? Now adjust the sample size to 100. Perform 100 experiments. What is the range of values observed for the distribution of sample means? How does this compare to the range observed when the sample size was 30? What about the range of values observed in the data from the last experiment - did this change much when the sample size was increased? Play around with various sample sizes. How does changing the sample size effect the spread of the distribution of sample means? Return to a sample size of 30, but change the population distribution to be right skewed. Perform 1,000 experiments. Describe the histogram of the the data from the last experiment. Comment on the modality and skew. Describe the histogram of the distribution of sample means from all experiments. Does it resemble the population distribution? Re-run the simulation using a sample size of 10. How does this effect the distribution of the sample means? How does this compare to what you observed when taking samples of size 10 with a normal population distribution? Change the population distribution to be left skewed, set the sample size to 80, and perform 1,000 experiments. Describe the histogram of the the data from the last experiment. Comment on the modality and skew. Describe the histogram of the distribution of sample means from all experiments. Does it resemble the population distribution? Run the simulation with these parameters a few times and pay attention to how the mean of the distribution of sample means compares to the population mean. Is the population mean similar or different from the mean of the sample means? What property does this illustrate? Change the parameters of the simulations as needed to answer the following true/false questions. Explain your answers. CLT tells us that the distribution of data from any experiment will be normally distributed. With a larger sample size, the data from the last experiment will resemble the population distribution. Performing 10 experiments is enough to see the effect of CLT. Regardless of the underlying population distribution and the sample size, the distribution of the sample means will be normally distributed. Increasing the sample size causes the data from a single experiment to look more and more normally distributed. "],
["ch7.html", "3 Introduction to Inference 3.1 Confidence Intervals 3.2 Hypothesis Tests 3.3 P-values", " 3 Introduction to Inference \" A useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in his experiment.\" - Gertrude Mary Cox Learning objectives Understand conceptually how confidence intervals are constructed Know correct interpretation of confidence intervals Hypothesis testing null and alternative hypothesis Type 1 and type 2 error Definition of p-value and how to interpret Up until this point, we have introduced probability, which allows us to quantify our uncertainty, we have discussed the concept of random variables and two widely used probability distributions (binomial and normal), and we have seen the most powerful result in statistics, the Central Limit Theorem which gives us the distribution of the sample mean in repeated samples. Let’s take a step back and remind ourselves of the goal of a statistical analysis. There is some population level numerical quantity we are interested in (parameter). This could be a mean or a proportion, or it could be something more complicated. Because we can (almost) never measure something in the entire population, we are restricted to taking a sample. Our goal is to use the data we collect in our sample to make inference, or generalizations, about the parameter, which is the unknown quantity of interest. In particular, we want to use probability distributions and the observed data to quantify what values of the parameter are plausible and which are not pluaible and we would like to determine if the trends we observe in our data are “significant” or if it is simply a result of chance. 3.1 Confidence Intervals We will first turn our attention to parameter estimation, of which there are two general approaches. These approaches are point estimation, which is a single statistic computed from the sample data used to estimate the parameter and interval estimation, which is an interval computed from the sample data to give a range of plausible values that is likely to contain the true population parameter. Point estimation is straightforward and is as simple as using the sample mean as an estimate of the population mean. While straightforward, this approach is problematic. In the last chapter we saw that every time we take a sample from a population and compute the sample mean, it changes. And we saw an example in which no matter how we took a sample, the sample mean could never possibly equal the population mean. This is because the act of taking a sample is a random process, which means that for any estimate from our sample it is highly unlikely (or straight up impossible) that we got the exact correct answer. Instead, it would be nice to have an interval that we could be reasonably confident contained the true parameter. What properties would make a good interval estimate? Well, on one hand we desire an interval that we can have a high degree of confidence of including the true parameter, but on the other hand, a narrow interval is much more informative. I can tell you with 100% confidence that the interval from \\(-\\infty\\) to \\(\\infty\\) covers the true population parameter, and I don’t even need to know anything about what you’re studying or what numerical quantity is being estimated and I don’t even need any data! I’ve achieved the first goal of having a high degree of confidence, but this interval is clearly not informative. I can make the interval smaller (I will need some data to do that), but the probability that the interval contains the true parameter will decrease. As a compromise, scientists and statisticians decided that intervals with a 95% probability of containing the true parameter was a satisfactory balance between the desire for high confidence and narrow intervals. This gives way to the most common confidence interval – the 95% confidence interval. Although we are getting a little bit ahead of ourselves here (we will have plenty of chances to calculate 95% confidence intervals). Let’s back up with some important definitions and notation. With an interval estimate, we are concerned with coverage probability, the probability that the interval contains or covers the parameter of interest. Typically, coverage probability is denoted by \\(1 - \\alpha\\). As the interval covers the parameter with probability \\(1 - \\alpha\\), the interval does not cover the parameter with probability \\(\\alpha\\) (because covering and not covering the parameter are complements and cover the entire sample space). In that sense, \\(\\alpha\\) is the probability that we are wrong. We will discuss \\(\\alpha\\) in more specific terms later in the chapter, but for now let’s just note that \\(\\alpha\\) is a small number, maybe 0.05 or 0.01. If we convert from decimal format to percentage format (multiplying by 100), we have \\(100(1 - \\alpha)\\)%, which is called the confidence level. The mathematical details of constructing a confidence interval depends on the parameter of interest and how the study was designed, and we will see various formulas for constructing confidence intervals throughout the rest of this book. In this chapter, we will focus on conceptual understanding and interpretation. To construct a \\(100(1 - \\alpha)\\)% confidence interval, we use sample data and sampling distributions as we know that sampling is a random process. For example, the Central Limit Theorem tells us the sampling distribution of the mean is normally distributed and gives us the mean and standard deviation of that distribution. Once we have a normal distribution for which we know the mean and standard deviation, we can calculate probabilistic quantities using software. It’s extremely important that we remember what is random and what the probability distribution is specifying when we are interpreting confidence intervals. The sampling distribution describes which values of the sample statistic are likely and which are not likely. The random process is drawing a sample. The population parameter is a fixed quantity – we don’t know what it is, but it does not change. Confidence intervals are constructed from the information we do know – the observed data from the sample. Because sampling is a random process and confidence intervals are constructed using the sample data, the confidence interval will vary from sample to sample. However, we formulate confidence intervals mathematically to ensure that the probability that the interval contains the true probability is \\(1 - \\alpha\\). And as we’ve seen probabilities are long-run frequencies. So any given confidence interval may or may not cover the true parameter, but in the long run if we take samples over and over and compute a confidence interval based on the data from each sample, we are guaranteed that \\(100(1 - \\alpha)\\)% of the confidence intervals constructed will cover the truth. This is quite appealing. Lets explore this concept further by using the following applet. Exercises The applet below is designed to help you get familiar with some of the important concepts related to confidence intervals. The inputs to the simulation are the sample size, number of experiments, and the confidence level. The simulation proceeds by taking a sample of the specified size and constructing a confidence interval based on that sample with the specified level of confidence. This process is one experiment. Each sample is drawn from the same population. For a specified number of experiments, the sampling and confidence interval construction is repeated either 10, 50, 100, or 1000 times. The plot shows all the confidence intervals constructed from each experiment - the number of intervals shown is equal to the “Number of Experiments.” The true population parameter in this case is 0 (only for simplicity). Intervals are shaded in blue if they cover the true population parameter and red if they do not contain the true parameter. The applet also reports the “observed coverage” from all experiments, this is the proportion of all the intervals shown that did contain the truth. Clicking “Run Simulation” will redo the specified number of experiments using the inputs provided. Set the sample size to 30, the confidence level to 95%, and simulate running 100 experiments (100 intervals total). How many intervals did not contain the true population parameter? In other words, how many red intervals are there? What was the observed coverage? Show how this was computed based on the number of intervals that did/did not contain the truth. Given the specifications use, What would you expect the coverage to be? Re-run the simulation under the same conditions (click “Run Simulation” again). What was the observed coverage for the second set of 100 experiments? Did you get the same observed coverage for both simulations? What does this indicates about the simulation? Re-run the simulation several times and compare the observed coverage from each run. What do you notice? Keep the confidence level at 95%, but change the sample size to 50. Simulate 100 experiments. How do the intervals differ from those constructed from experiments using a sample size of 30? What is the observed coverage? What would you expect the coverage to be based on the specifications used? Across the 100 confidence intervals shown, how do the widths of the confidence intervals compare? Is there any difference in the confidence interval widths between intervals that do contain the true parameter and those that do not? Now change the sample size to 100. How do these intervals compare to those using sample sizes of 30 or 50? Change the number of experiments to be 1000. Play around with various sample sizes. How does changing the sample size effect the observed coverage? Return to a sample size of 30. Now we will simulate 1000 experiments. Simulating more experiments makes things harder to visualize (we can no longer easily count the intervals that did not contain the truth), but the coverage is more stable when the simulation is re-ran. With a confidence level of 95%, what is the observed coverage? Re-run the simulation with a sample size of 100. How do these intervals compare to those using sample sizes of 30 or 50? Re-run the simulation using a confidence level of 50%. How do these intervals compare to those using a confidence level of 95%? What is the observed coverage? Change the parameters of the simulations as needed to answer the following true/false questions. Explain your answers. As the sample size increases, the coverage probability increases. As the confidence level decreases, the width of the confidence intervals decreases, As the sample size increases, the width of the confidence intervals increases. If a researcher wants a narrower confidence interval, they should obtain a larger sample. If a researcher wants a narrower confidence interval, they should decrease the confidence level. 3.2 Hypothesis Tests Along with parameter estimation, a cornerstone of statistical inference is significance testing. Confidence intervals allow us to construct a range of plausible values for the parameter, and significance tests allow us to determine the likelihood of a parameter taking a certain value. A hypothesis test uses sample data to assess the plausibility of each of two competing hypotheses regarding an unknown parameter (or set of parameters). A statistical hypothesis is a statement or claim about an unknown parameter. The null hypothesis generally represents what is assumed to be true before the experiment is conducted. This hypothesis is typically denoted \\(H_0\\). The alternative hypothesis represents what the investigator is interested in establishing. This hypothesis is typically denoted \\(H_A\\). Oftentimes when people refer to the “scientific hypothesis,” this in reference to the alternative hypothesis - it is what the investigators think will happen or what they want to show. The goal of a hypothesis test is to determine which hypothesis is the most plausible - the null or the alternative. As an example, consider researchers that have developed a new drug to treat cancer. In order for the drug to be approved for use, the investigators must prove that it is more effective in treating cancer than the current gold standard treatment. To do this, the investigators gather a sample of cancer patients and randomize half of them to receive their new drug and half to receive the current treatment. Then they determine how many patients improved in both groups. In this scenario, the null hypothesis would be that the new drug and the current drug result in the same improvement. Why? Well, the null hypothesis is what is believed before the data was collected. The key is whose beliefs we are talking about. While the scientists that developed the drug most likely believe that their new drug is more effective, the rest of the scientific community remains in a state of uncertainty. The null hypothesis reflects the general beliefs of the scientific community. The alternative hypothesis in this scenario is that the drugs differ in their effectiveness on treating cancer. This is what the researchers hope to show, specifically, they hope to show that their drug is more effective, however, when the study is conducted there is no evidence in either direction, so we leave the alternative hypothesis to also encompass the possibility of the new drug being worse. In general, we can think about the null hypothesis as being the “baseline,” “boring,” “nothing to see here” hypothesis. The exact specification will depend on the study context and the type of data being measured (categorical or continuous): \\(H_0\\): the average cholesterol for hypertensive smokers’ is no different than the general population \\(H_0\\): no difference between the treatment and control groups \\(H_0\\): men and women have identical probabilities of colorectal cancer \\(H_0\\): observing a ``success\" in a population is identical to flipping a coin The hypothesis testing procedure uses probability to quantify the amount of evidence against the null hypothesis. Since the null hypothesis is the baseline, we start by assuming that it is true. Then, we conduct the study and collect data to quantify the likelihood that the the null is true. The reason for this approach is rooted in the scientific method. As we introduced in Chapter 1, the scientific method has 7 steps: Ask a question Do background research Construct a hypothesis Test your hypothesis with a study or an experiment Analyze data and draw conclusions How do the results align with your hypothesis? Communicate results We are really focusing on steps 3-5. In step 3, we construct the “scientific hypothesis” and in step 4, we test that hypothesis. In order to produce rigorous scientific results we cannot assume that the scientific hypothesis is true, as that is the goal of our study. We must assume the current state of knowledge (null hypothesis) and then if we are to prove that our hypothesis is correct, we would show that if the current knowledge was true, it would be really unlikely that our experiment would have ended up how it did. A great analogy to the concept of hypothesis testing is our judicial system. In court, the legal principle is that everyone is “innocent until proven guilty” and the prosecution must prove that the accused is guilty beyond a reasonable doubt. In many cases, there may not be definitive evidence if the defendant is actually innocent or guilty. But, if the prosecution can show that the likelihood of the accused individual being guilty is high (or equivalently, that the likelihood of the accuwsed individual being innocent is low), then the defendant will be convicted. In hypothesis testing researchers are like the prosecution and must use data to prove that the null hypothesis (current state of knowledge) is false beyond a reasonable doubt. The next several chapters will go through how we can use different types of data to quantify our evidence against the null hypothesis. With this set up in mind, we have two possible outcomes of a hypothesis test. Either we conclude that we do not have a lot of evidence against the null hypothesis, i.e. the null hyptohesis looks reasonable, and we fail to reject the null or we conclude that we have enough evidence against the null and we reject the null. It is extremely important to note here that we NEVER accept the null or accept the alternative. Many people find this annoying because we can never say anything with 100% certainty. But this is exactly the point! Remember that statistics is all about quantifying our uncertainty. Think back to our drug development example. There will never ever ever be a drug that works exactly the same in every person that takes it. People are too variable and many aspects of a person’s life impacts how a drug works in their body. So it would be completely unreasonable to say that a new drug works all the time. However, there can be a drug that improves outcomes for the average person or that this drug is likely to improve outcomes in a randomly selected person who takes it. We can create a two by two table for the results of any hypothesis test. In the rows we have the two possible outcomes from our test - fail to reject \\(H_0\\) and reject \\(H_0\\). In the columns we have the true underlying state of nature - either \\(H_0\\) is true or false. \\(H_0\\) true \\(H_0\\) false Fail to reject \\(H_0\\) Correct (1 - \\(\\alpha\\)) Incorrect (\\(\\beta\\)) Reject \\(H_0\\) Incorrect \\(\\alpha\\) Correct (1 - \\(\\beta\\)) In the upper-left and bottom-right cells of the table we are making the correct decision based on our test. When the null hypothesis is true, failing to reject \\(H_0\\) is the correct decision and when the null hypothesis is false, rejecting \\(H_0\\) is the correct decision. However, the other two cells correspond to a mistake being made. Because statisticians are not creative, these mistakes are referred to as type 1 error and type 2 error. A type 1 error is equivalent to a false alarm or a false positive - the null hypothesis was rejected, when in fact it was true. A type 2 error can be thought of as a missed opportunity or a false negative - the null hypothesis was false, but it was not rejected. Typically, the type 1 error rate is symbolically denoted with \\(\\alpha\\) and the type 2 error rate is denoted by \\(\\beta\\) (again the statisticians of the past were not creative). If we were looking to create a good hypothesis test, we would want to minimize type 1 and type 2 errors. In other words, if we were to conduct our study over and over again and there was something to be found, we would want to reject the null hypothesis (find something) at a high rate and fail to reject the null hypothesis at a low rate. If there was truly nothing to be found, we wouldn’t want to find anything and if there is something to be found we want to find it. However, there is a trade-off between type 1 and type 2 error. Let us illustrate this point with a simulation. Exercises In this experiment, we are looking to determine if a coin is fair, i.e. whether or not the probability of heads is 50%. A type 1 error in this context would be concluding that the coin is not fair, when it actually is. A type 2 error in this context would be concluding that the coin is fair when it is actually not. Each experiment consists of flipping a coin 20 times and observing the proportion of the 20 flips that result in heads. If the coin is fair, we would expect around 10 of the 20 flip to result in heads. However, if we observed 11 or 12 heads in one experiment, would you be convinced the coin isn’t fair? What about if you observed 19/20 of the flips resulting in heads? This applet allows the user to specify the “Rejection Threshold” to determine the cutoff where the app will reject the null hypothesis. This is specified in terms of how far off the proportion of heads in the experiment is from 50%. Since we have no way to determine if the coin is more prone to heads or tails, we consider this distance in both directions. If the observed proportion of the 20 flips that result in heads is beyond the specified threshold, the null hypothesis will be rejected. For example, if you choose a threshold of 10%, then any experiment where there are 60% (12/20) or more flips resulting in heads OR 40% (8/20) or less flips resulting in heads, then the null hypothesis is rejected and the coin is determined to not be fair. The simulation involves replicating the 20 flip experiment 10,000 times. For each experiment, a coin is flipped 20 times and the proportion of heads is calculated. If that proportion exceeds the specified threshold it is counted as “Reject.” If the proportion does not exceed the threshold, that experiment is considered a “Fail to Reject.” The barchart shows the proportion of 10,000 experiments in which the null hypothesis was and was not rejected. Note: Since we are replicating the experiment 10,000 times, we don’t expect the results to change much if the simulation is re-ran. However, you may see small changes in the proportion of rejects/fail to rejects for the same input values. Set the true status of the coin to be fair and start with a threshold of 5%. In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain. At this threshold, how many heads would cause an experiment to reject the null hypothesis At this threshold, what proportion of the experiments resulted in the null hypothesis being rejected? In terms of \\(\\alpha\\) and \\(\\beta\\) as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it’s value? As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected Did we investigate type 1 or type 2 errors in this problem? Now change the true status of the coin to be unfair with a 70% chance of heads and set the rejection threshold to 15%. In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain. At this threshold, how many heads would cause an experiment to reject the null hypothesis At this threshold, what proportion of the experiments resulted in the null hypothesis not being rejected? In terms of \\(\\alpha\\) and \\(\\beta\\) as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it’s value? As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected? Explain why. Did we investigate type 1 or type 2 errors in this problem? Now change the true status of the coin to be unfair with a 20% chance of heads and set the rejection threshold to 20% In this setting, if we reject the null hypothesis are we making the correct conclusion or the incorrect conclusion? Explain. At this threshold, how many heads would cause an experiment to reject the null hypothesis At this threshold, what proportion of the experiments resulted in the null hypothesis not being rejected? In terms of \\(\\alpha\\) and \\(\\beta\\) as described above, which of those two can you specify under these circumstances (i.e., with the true status of the coin being fair and the threshold of 5%)? What is it’s value? As the threshold increases, what happens to the proportion of experiments in which the null hypothesis is rejected? Did we investigate Type 1 or Type 2 errors in this problem? What can you conclude about the relationship between type 1 and type 2 errors? 3.3 P-values All hypothesis tests are based on quantifying the probability of the study results assuming the null hypothesis is true. This probability is so important that it has a special name, the p-value. In technical terms, the p-value gives the probability of obtaining results as extreme or more extreme than the ones observed in the sample, given that the null hypothesis is true. A less technical way to describe a p-value is that assuming there is truly nothing going on, what’s the chances of obtaining results similar in opposition to the null hypothesis as our study? If we are thinking about hypothesis testing as a court case, p-values are the way that we can quantify the evidence against the defendant. Recall, the prosecution wants to prove beyond a reasonable doubt that the defendant is not innocent. So what does the scientific community consider sufficient evidence? There is a generally agreed-upon scale for interpreting p-values with regards to the strength of evidence that they represent. p-value Evidence against null 0.1 Borderline 0.05 Moderate 0.025 Substantial 0.01 Strong 0.001 Overwhelming Often, the term “statistically significant” is used to describe p-values below 0.05, possibly with a descriptive modifier. “Borderline significant” (p &lt; 0.1) “Highly significant” (p &lt; 0.01) However, don’t let these clearly arbitrary cutoffs distract you from the main idea that p-values represent - how far off is the data from what you would expect under the null hypothesis. A p-value of 0.04 and 0.000000001 are not at all the same thing, even though both are “statistically significant.” In general, a p-value cutoff is chosen and if a p-value below the cutoff is observed, the null hypothesis is rejected. The investigators choose this cutoff, which is equivalent to the type I error rate and thus denoted by \\(\\alpha\\), before analyzing the data. Most of the time \\(\\alpha\\) is set to 0.05, which means there is a 5% chance of a type I error (false alarm). The smaller the value of \\(\\alpha\\), the greater the “burden of proof” required to reject the null hypothesis. \\(\\alpha\\) is also commonly called the significance level. A fundamental property of p-values is that if we use \\(p &lt; \\alpha\\) as cutoff for rejecting the null hypothesis, the type I error rate is guaranteed to be no more than \\(\\alpha\\). However, the \\(p &lt; \\alpha\\) cutoff guarantees us nothing about the type II error rate. This is because p-values are calculated assuming the null hypothesis is true, so they don’t give us any information about what to expect when the null hypothesis is false. While p-values are widely used, have a distinct purpose, and can be informative they also have a number of limitations. "]
]
