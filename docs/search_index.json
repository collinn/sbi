[
["index.html", "Seriously Interesting Statistics Textbook Introduction 0.1 Stuff lifted that used to be in ch2 (may or may not be useful still?) 0.2 Shiny test 0.3 Regular UI 0.4 Editing text", " Seriously Interesting Statistics Textbook Caitlin Ward and Collin ‘C-Money’ Nolte 2021-01-27 Introduction First note - Having {-} in the heading skips numbering assignment (which can be customized in _bookdown.yml), so that Chapter 1 doesn’t read as 2 Chapter 1 this is a test hello This is a cool book that we are going to make, and it’s going to have information about statistics index.Rmd is the first file loaded, similar to HTML use of index.html Do note that This book uses markdown, so we can use markdown notation when writing, or html tags That includes sublists too forever but not too many It can be published in a lot of ways, but this book suggests we focus on HTML first since pdf can be wonky and change format frequently We can use math too if we want. sex \\(\\rightarrow\\) \\(\\int e^X\\) Since this is done in Rstudio with markdown, we can build changes by just pushing Knit at the top (Ctrl+Shift+K !). However, if we add a new file/chapter, we have to, from the console, input library(magrittr) bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::gitbook&#39;) (or whichever format we want. When closer to finshed, we can make a script to compile finished versions in multiple formats) Things you’ll need: install.packages(c(&quot;knitr&quot;, &quot;bookdown&quot;, &quot;servr&quot;, &quot;shiny&quot;, &quot;rmarkdown&quot;)) https://www.w3schools.com/css/css_selectors.asp https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html 0.1 Stuff lifted that used to be in ch2 (may or may not be useful still?) We can also have R code But that’s boring and dumb when we can also have Shiny, though a few things of note, I guess While rmarkdown can host a self contained shiny app, bookdown requires a url. This is not ideal, and I will investigate if there is something that can be done about this Ok, whatever, I’m sure the library will host it for us. That would require internet access, though. We can still include the actual directories in our repo, or I think we can embed shiny if it is created as a single document instead of chapters This is dumb, but the shiny.rstudio website stopped hosting all of their shiny examples (?), and everything else is too complicated Oh, I know, we can just use the shiny server for public health! “To protect your security, ph-shiny.iowa.uiowa.edu will not allow Firefox to display the page if another site has embedded it. To see this page, you need to open it in a new window.” Here is a way complicated one that takes too long to load. Things I don’t like: Small window (maybe can adjust?) Scrolly window Yihui recommends miniUI for embedded shiny apps, but still Not yet sure what other options might exist 0.2 Shiny test This authenticates your computer to load shiny apps rsconnect::setAccountInfo(name=&#39;ui-sbi&#39;, token=&#39;1C11998F5C3BED19B58AFD22D349483C&#39;, secret=&#39;DsgOVlwde3RKJzzu5Nd07xVFrr7gv56kZ8NAFKyP&#39;) library(rsconnect) ## Directory containing app, this uploads it to site #rsconnect::deployApp(&quot;../shinyApps/confidenceIntervals&quot;) 0.3 Regular UI 0.4 Editing text Here are some things that were written that seem fine - this is maybe something we&#39;re not sure about + this is an idea or proposal we can discuss? But the actual code above looks like this ```diff Here are some things that were written that seem fine - this is maybe something we&#39;re not sure about + this is an idea or proposal we can discuss? ``` i think "],
["ch4.html", "1 Probability 1.1 Idea 1.2 Understanding Randomness 1.3 Probability Operations 1.4 Conditional Probability 1.5 Probabilities from tables (needs a different example) 1.6 Bayes’ Rule", " 1 Probability “The most important questions of life are, for the most part, really only problems of probability.” - Pierre Simon, Marquis de Laplace Learning objectives Learn the scientific definition of probability Conceptual understanding of randomness Understand probability notation and operations Learn about conditional probability Use probabilities to calculate quantities of interest in diagnostic testing 1.1 Idea Not sure where else to write this - for running a simulation at home. Consider heating a pan, throwing on 10 corn kernels, and figuring out how many have popped after certain number of time. Pretend 3 minutes is 50%. Well, have them do ten for 3 minutes, then count how many popped. Repeat. 1.2 Understanding Randomness If statistics is the science of uncertainty, then probability is the mechanism that allows us to quantify our uncertainty. People talk loosely about probability all the time. For example, what’s the chance of rain tomorrow?\" or “how likely is it that drug A is better than drug B?” However, for scientific purposes, we need to be more specific in terms of defining and using probabilities. First let’s introduce some definitions to help us talk about randomness and probability. First, a random process is any act or process that results in an outcome that cannot be predicted with certainty. The sample space of a random process is the set of all its possible outcomes and is often denoted \\(\\mathcal{S}\\). Finally, an event is an outcome or collection of outcomes from a random process and will always be contained in the sample space. To illustrate, suppose we flip a fair coin three times. Each act of flipping the coin is random process - the coin might land on heads and it might land on tails. Letting \\(H\\) be shorthand for the flip resulting in heads and \\(T\\) be shorthand for the flip resulting in tails, the sample space can be enumerated as \\[\\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\},\\] giving us eight possible results from the three coin flips. Finally, an event could be any single outcome, or any collection of outcomes, based on this experiment. For example, we may have: The event of obtaining exactly two heads: \\(\\{HHT, HTH, THH\\}\\) The event of obtaining heads on the first toss: \\(\\{HHH, HHT, HTH, HTT\\}\\) The event of obtaining three tails \\(\\{TTT\\}\\). We would all agree that the probability of heads when flipping a fair coin is 50% and the probability of rolling a 2 on a 6-sided die is 1/6, but why is that true? Well, if we were to flip a coin many, many times, we would expect half of the flips to result in heads. Similarly, if we roll a 6-sided die over and over, 1/6 of the rolls should result in a value of 2. In both cases, we are thinking about a long-run frequency, which is why probability is defined as the fraction of time an event occurs if a random process is repeated over and over again under the same conditions. This means that probabilities are always between 0 and 1, since we can never observed more events than the number of times the process is repeated, e.g. we can never observed 12 heads on 10 coin flips. Clearly, an event with probability 0 is an event that can never occur. This leads us to some important properties of probabilities: The sum of probabilities for all outcomes in the sample space, \\(\\mathcal{S}\\), must equal 1 For any event, the probability of that event is the sum of the probabilities for all the outcomes in that event For the coin flipping example, there are eight possible outcomes. Since each outcome is equally likely (why?) The first property tells us that the probability of any specific outcome (say, \\(HHH\\)) is 1/8. The second tells us that the probability of heads on the first toss is \\(4/8 = 1/2\\), since four of the outcomes. These properties underlie a lot of the more complicated formulas and concepts we will cover in this chapter, although we don’t always think about them explicitly. We can explore this concept of long-run frequency more with a simulation. For this simulation, we will simulate rolling a 6-sided die 100 times, recording the result of each roll, and then tabulating the proportion of rolls where we observed each side of the die. rollDie &lt;- function(nRolls, seed) { set.seed(seed) sample(1:6, nRolls, replace = TRUE) } barplot(table(rollDie(100, 1))) prop.table(table(rollDie(100, 1))) ## ## 1 2 3 4 5 6 ## 0.19 0.18 0.12 0.15 0.15 0.21 As we see, with just 100 rolls, we don’t observe exactly the same proportion of rolls landing on each side of the die. The proportions are not exactly 1/6 = 16.7%, because of randomness. However, if we do 1,000,000 rolls, we see things even out: round(prop.table(table(rollDie(1e6, 1))), 3) ## ## 1 2 3 4 5 6 ## 0.167 0.167 0.167 0.167 0.167 0.166 Now let users play with a similar app illustrating coin flips (or something) and have associated exercises. Maybe a plot of the proportion over time. 1.3 Probability Operations It’s easy to talk about probability of one event, i.e. the probability of rolling a 2, but often we are interested in quantifying probabilities about more complicated combinations of multiple events. For example, if we consider a family with two parents and one child, instead of the probability of one parent getting the flu, we might be interested in the probability that both parents get the flu. Or we might be interested in the probability anyone in the family gets the flu. Finally, we might want to quantify the probability that one parent gets the flu and the other does not. In order to succinctly describe these probabilities, we use some mathematical notation. Before you get totally scared, this notation is just to simplify the writing of probability statements - the underlying concept does not change! Probabilities are denoted as \\(P(Event) = p\\), as in \\(P(Heads) = 0.5\\). To make things even shorter we can use a capital letter to denote an event of interest, i.e. let \\(H\\) be the event that the outcome of a fair coin flip is heads, then we have \\(P(H) = 0.5\\). Then, to talk about relationships between events, we define the operations of the intersection, union, and complement. Consider to arbitrary events \\(A\\) and \\(B\\): The intersection represents the event that \\(A\\) and \\(B\\) occur and is denoted \\(A \\cap B\\) The union represents the event that \\(A\\) or \\(B\\) occur and is denoted \\(A \\cup B\\) The complement represents the scenario in which an event does not occur; the complement of \\(A\\) is denoted as \\(A^C\\). As a corollary to this, for any event \\(A\\), we could say that \\(A\\) occurs or \\(A\\) does not occur \\((A^C)\\). As these are the only possible outcomes regarding \\(A\\), \\(A \\cup A^C\\) represents all possible events, or \\(A \\cup A^C = \\mathcal{S}\\) In the previous coin tossing example, define \\(A = \\text{obtaining exactly two heads} = \\{HHT, HTH, THH\\}\\) and \\(B = \\text{obtaining heads on the first toss} = \\{HHH, HHT, HTH, HTT\\}\\). \\(A \\cap B = \\text{obtaining exactly two heads AND a heads on the first toss} = \\{HHT, HTH\\}\\) \\(A \\cup B = \\text{obtaining exactly two heads OR a heads on the first toss}= \\{HHH, HHT, HTH, HTT, THH\\}\\) \\(B^C = \\text{obtaining tails on the first toss} = \\{TTH, THT, THH, TTT\\}\\) We can visualize these operations with Venn diagrams. A Venn diagram uses overlapping circles and shading to describe the relationship between two events. First, we will visualize the intersection operation. If the left circle denotes the event \\(A\\) and the right circle denotes the event \\(B\\), then the intersection is the overlapping region where both \\(A\\) and \\(B\\) occur. Figure 1.1: Venn diagram of intersection The union operation includes all outcomes in \\(A\\) or \\(B\\) (or both), so it would include the entire region. Figure 1.2: Venn diagram of union The complement of \\(B\\), includes all outcomes that are not part of event \\(B\\). Figure 1.3: Venn diagram of the complement of B Since the probability of all outcomes in the sample space must add to 1, and the complement Back to the flu example, let \\(A\\) denote the event that one parent gets the flu, let \\(B\\) denote the event that the other parent gets the flu, and let \\(C\\) denote the event that the child gets the flu. Using this new notation, we can represent the probability that both parents get the flu as \\(P(A \\cap B)\\) , the probability that anyone in the family gets the flu as \\(P(A \\cup B \\cup C)\\), and the probability that one parent gets the flu and the other does not as \\(P(A \\cap B^C)\\). In some cases, there is no overlap between the events, i.e. the events cannot happen simultaneously. When two events cannot happen at the same time, they are said to be mutually exclusive. Mathematically, this means the \\(P(A \\cap B) = 0\\). For example, consider the following two events based on your final course letter grade: \\(A\\) is the event of getting an \\(A\\) and \\(B\\) is the event of getting a \\(B\\). As only one grade can be given per course, clearly \\(A\\) and \\(B\\) are mutually exclusive. Obtaining the probability of at least one of two mutually exclusive events happening is straightforward as there is no overlap between events. Thus, the probability of \\(A \\cup B\\) is simply the sum of the probabilities of \\(A\\) and \\(B\\) happening separately. Important Formula! For mutually exclusive events: \\[P(A \\cup B) = P(A) + P(B)\\] Figure 1.4: Venn diagram of mutually exclusive events If two events are not mutually exclusive, there is overlap in the events and \\(P(A \\cap B) \\neq 0\\) and we cannot get the probability of the union using the previous formula. If we did, we would be double counting the intersection. As a concrete illustration, suppose that for a married couple, the probability that one spouse contracts the flu (event \\(A\\)) is 0.25, the probability that the other spouse contracts the flu (event \\(B\\)) is 0.20, and the probability that both the spouses contract the flu (\\(A\\) and \\(B\\)) is 0.15. If we displayed this information in a Venn Diagram, we would have: Figure 1.5: Venn diagram of flu probabilities At first glance, this might not be what you would expect. If \\(P(A = 0.25)\\), why do we have \\(0.10\\) in that region? However, the event \\(A\\) is actually divided into two regions - the part that intersects \\(B\\) and the part that doesn’t. This is called the Law of Total Probability, and this means that the probability of \\(A\\) consists of both the probability that \\(A\\) and \\(B\\) both happen and the probability that \\(A\\) happens but \\(B\\) doesn’t. This is true since \\(B\\) and \\(B^C\\) are mutually exclusive and together contain all possible outcomes. Mathematically, we can write this as: \\[P(A) = P(A \\cap B) + P(A \\cap B^C)\\] And in a Venn diagram this looks like: Figure 1.6: Venn diagram of the law of total probability Getting back to finding the quantity of interest, \\(P(A \\cup B)\\), what all of this means is that if two events are not mutually exclusive, then \\(P(A \\cup B) \\neq P(A) + P(B)\\), because both \\(P(A)\\) and \\(P(B)\\) include the intersection regions \\(P(A \\cap B)\\). We do want to include the intersection in the union, but only one time. So we can get the correct probability by subtracting off one of the intersections: Important formula!! In general, \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] This formula actually includes the mutually exclusive case, since when two events are mutually exclusive \\(P(A \\cap B) = 0\\). Let’s use a coin flipping simulation to illustrate this formula with the two events of interest being obtaining exactly two heads (event \\(A\\)) and obtaining heads on the first toss (event \\(B\\)). We know there are three ways we can obtain exactly two heads, so \\(P(A) = 3/8 = 0.375\\). There are four ways we can obtain heads on the first toss, so \\(P(B) = 4/8 = 0.50\\). There are two ways we can obtain exactly two heads AND obtain heads on the first toss, so \\(P(A \\cap B) = 2/8 = 0.25\\). Finally, there are five ways we can we can obtain exactly two heads OR obtain heads on the first toss, so \\(P(A \\cup B) = 5/8 = 0.625\\). We could also find this last probability using the previous formula and the previous probabilities. \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.375 = 0.50 - 0.25 = 0.625\\] You can decide how many simulations to run, and the app will report the proportion of simulations that results in exactly two heads, the proportion where heads was obtained on the first toss, the proportion were both events occurred simultaneously, and the proportion of where either event occurred. Since probabilities are defined as long run frequencies, if enough simulations are run, these proportions should be identical to the probabilities defined previously. flipCoin3 &lt;- function() { dat &lt;- rbinom(3, 1, 0.5) event1 &lt;- sum(dat) == 2 event2 &lt;- dat[1] == 1 intersectionProb &lt;- event1 &amp; event2 unionProb &lt;- event1 | event2 c(event1, event2, intersectionProb, unionProb) } nSims &lt;- 1000 simRes &lt;- replicate(nSims, flipCoin3()) cbind(c(&#39;P(A)&#39;, &#39;P(B)&#39;, &#39;P(A AND B)&#39;, &#39;P(A OR B)&#39;), rowMeans(simRes)) ## [,1] [,2] ## [1,] &quot;P(A)&quot; &quot;0.373&quot; ## [2,] &quot;P(B)&quot; &quot;0.507&quot; ## [3,] &quot;P(A AND B)&quot; &quot;0.251&quot; ## [4,] &quot;P(A OR B)&quot; &quot;0.629&quot; We can run a similar experiment, but now using two mutually exclusive events - the event of obtaining exactly two heads, and the event of obtaining three tails. flipCoin3 &lt;- function() { dat &lt;- rbinom(3, 1, 0.5) event1 &lt;- sum(dat) == 2 event2 &lt;- sum(dat) == 0 intersectionProb &lt;- event1 &amp; event2 unionProb &lt;- event1 | event2 c(event1, event2, intersectionProb, unionProb) } nSims &lt;- 1000 simRes &lt;- replicate(nSims, flipCoin3()) cbind(c(&#39;P(A)&#39;, &#39;P(B)&#39;, &#39;P(A AND B)&#39;, &#39;P(A OR B)&#39;), rowMeans(simRes)) ## [,1] [,2] ## [1,] &quot;P(A)&quot; &quot;0.361&quot; ## [2,] &quot;P(B)&quot; &quot;0.124&quot; ## [3,] &quot;P(A AND B)&quot; &quot;0&quot; ## [4,] &quot;P(A OR B)&quot; &quot;0.485&quot; No matter how many times we repeat the experiment, the intersection is always 0, because we can never flip a coin three times and get exactly two heads and three tails. 1.4 Conditional Probability Many times we are interested in the probability of an event occurring, given that another event has occurred, such as “What is the probability of an individual getting lung cancer, given that they are a smoker?” If we didn’t know if the individual was a smoker or not, we would probably guess the probability of lung cancer is pretty low, maybe 1%. Once we gain the knowledge that the individual smokes, our estimate of the probability of lung cancer increases, maybe up to 20%. Conditional probability refers to the probability of one event occurring given that another event has already taken place. For events \\(A\\) and \\(B\\), the conditional probability that \\(B\\) will occur given that \\(A\\) has already taken place is denoted \\(P(B|A)\\). Important formula! Conditional probabilities \\[P(A|B) = \\dfrac{P(A \\cap B)}{P(B)}\\] or rearranging: \\[P(A \\cap B) = P(A|B) P(B)\\] This formulas may seem like it comes out of nowhere, but let’s see if we can understand the intuition behind the math. Recall that probabilities give us the fraction of time an event occurs, when repeated over and over. Conditional probabilities are calculated assuming we already have knowledge on whether a related event has occurred. In the formula above, we are given that \\(B\\) has occurred, so the denominator includes only probabilities associated with event \\(B\\). Additionally, since we know \\(B\\) occurred, \\(A\\) can only happen in conjunction with \\(B\\), so the numerator only includes probabilities associated with \\(A \\cap B\\). We can also think about this concept with our coin flipping example. Consider the events \\(A\\) to be obtaining exactly two heads and \\(B\\) to be obtaining a heads on the first toss (the same events we’ve previously defined as \\(A\\) and \\(B\\)) \\(A = \\{HHT, HTH, THH\\}\\) \\(B = \\{HHH, HHT, HTH, HTT\\}\\) If we condition on \\(B\\), that means we know that the first flip was heads. With conditional probability, \\(P(A|B)\\), we are thinking about the probability of obtaining exactly two heads on three flips, given that the first flip resulted in heads. Without knowledge of the first flip, we would have said the probability of exactly two heads is 3/8, because there are eight possible outcomes of flipping a coin three times, and three of those result in exactly two heads. After we condition on the first flip being heads, our sample space is reduced. Now, there are only four outcomes where the first of three flips results in heads. We also know that of those four outcomes, two of them result in exactly two heads: \\(B = \\{HHT, HTH\\}\\). So this conditional probability must be \\(1/2\\). Often, we are not able to enumerate all possible outcomes, which is why the formulas come in handy. In this case, if we had used the formula: \\[P(A|B) = \\dfrac{P(A \\cap B)}{P(B)} = \\dfrac{2/8}{4/8} = 1/2\\] The idea of conditional probability allows us to talk about the very important property of independence. When an event is not affected by another event, the two events are described as independent. Mathematically, we denote this as: \\[P(A|B) = P(A)\\] In other words, knowing that \\(B\\) has occurred does not change the probability of \\(A\\) occurring. If two events are not independent, they are said to be dependent. A simple example of independent events would be each flip of a coin. Whether the last flip was heads or not does not change the probability of heads on the next flip. If we are interested in the probability of two independent events occurring simultaneously (\\(P(A \\cap B)\\)), we can use simplify the previous result based on conditional probability: \\[P(A \\cap B) = P(A|B) P(B) = P(A)P(B)\\] This can be a helpful formula - and we’ve actually been implicitly using it when thinking about the probabilities of different outcomes from flipping a coin three times. If the probability of heads is \\(1/2\\) and each flip is independent, then the probability of getting three heads is \\[P(HHH) = (1/2)(1/2)(1/2) = (1/2)^3 = 1/8 = 12.5\\%\\] Since the probability of heads is identical to the probability of heads, this is the probability of any outcome from the three coin flips. It is important to keep in mind that independent and mutually exclusive do not mean the same thing. Consider a fair coin and a fair six-sided die and let \\(A\\) be the event of obtaining heads on one coin flip and \\(B\\) be the event of rolling a 2 on one roll. Clearly, \\(A\\) and \\(B\\) are independent, so \\[P(A \\cap B) = P(A)P(B) = (1/2)(1/6) = 1/12\\] Now consider a fair six-sided die, where even-numbered faces are blue and odd-numbered faces are red. Let \\(A\\) be the event of rolling a red face and \\(B\\) be the event of rolling a 2. \\(P(A) = 1/2)\\) and \\(P(B) = 1/6\\) as before, but \\(A\\) and \\(B\\) are mutually exclusive, i.e. \\(P(A \\cap B) = 0\\). To determine if events are mutually exclusive or independent, there are two questions you can ask yourself: Can both events happen at the same time? Does one event give me any information about the other event? If your answer to the first question is no, then the events must be mutually exclusive. If the answer to the first question is yes, but the answer to the second question is no, then the events are independent. 1.5 Probabilities from tables (needs a different example) When discussing and interpreting probability relationships between two or more events, it is often helpful to use tables. Consider the following table of representing Japanese men aged 45-69 (1975). The entries in the table are the probabilities of outcomes for a person selected randomly from the population. library(knitr) # fill by column probs &lt;- c(0.6, 0.2, 0.1, 0.1) probsMat &lt;- matrix(NA,ncol = 3, nrow = 3) probsMat[1:2, 1:2] &lt;- probs probsMat[,3] &lt;- rowSums(probsMat, na.rm = T) probsMat[3,] &lt;- colSums(probsMat, na.rm = T) colnames(probsMat) &lt;- c(&#39;Normal Blood Pressure (N)&#39;, &#39;High Blood Pressure (H)&#39;, &#39;Total&#39;) rownames(probsMat) &lt;- c(&#39;Reasonable Weight (R)&#39;, &#39;Overweight (O)&#39;, &#39;Total&#39;) kable(probsMat, align = &#39;c&#39;) Normal Blood Pressure (N) High Blood Pressure (H) Total Reasonable Weight (R) 0.6 0.1 0.7 Overweight (O) 0.2 0.1 0.3 Total 0.8 0.2 1.0 Here we use \\(N\\) to denote the event that a man has normal blood pressure, \\(H\\) to denote the event that a man has high blood pressure, \\(R\\) to denote the event that a man is a reasonable weight, and \\(O\\) to denote the event that a man is overweight (the letters \\(A\\) and \\(B\\) were getting old). Using this table we can get probabilities of events separately, we can get the probability of intersection, union, and complements, and we can get conditional probabilities. Let’s start by finding \\(P(R)\\), or the probability of man being a reasonable weight. This includes both men that have normal blood pressure and men that have high blood pressure, but not men that are overweight. Note that the events of being a reasonable weight and being overweight are mutually exclusive, as are the events of having normal blood pressure and high blood pressure - because a man cannot be in both categories at the same time. Thus, to find \\(P(R)\\) we actually just need to look at the right hand margin of the table, \\(P(R) = 0.7\\). Because of this, \\(P(R)\\), \\(P(O)\\), \\(P(N)\\), and \\(P(H)\\) are called marginal probabilities. The inner cells of the table directly give us the intersection probabilities, \\(P(R \\cap N) = 0.6\\), \\(P(R \\cap H) = 0.1\\), \\(P(O \\cap N)=0.2\\), and \\(P(O \\cap H)=0.1\\). To get the probabilities of at least one event happening (the union), we must add up all the cells that correspond to either event - but just like before we have to be careful about double counting the intersection. Let’s consider the probability of a man being overweight or having high blood pressure (\\(P(O \\cup H)\\)). The second row of the table corresponds to a man being overweight, and using the margin we have \\(P(O) = 0.3\\), the second column corresponds to a man having high blood pressure and \\(P(H) = 0.2\\). So if we want to get the probability of being overweight or having high blood pressure, we can add those together but notice that both of those probabilities include the probability of being overweight and having high blood pressure (\\(P(O \\cap H)\\)), so we have to subtract that value. Thus, \\[P(O \\cup H) = P(O) + P(H) - P(O \\cap H) = 0.3 + 0.2 - 0.1 = 0.4\\] This is the same formula we defined previously. When the data is in a table, we could also skip this formula and just add up all the cells where either one event or the other is present. The probability of being overweight or having high blood pressure is \\(0.2 + 0.1 + 0.1 = 0.4\\). Either way we do it, we get the same answer and you can use whichever method makes the most sense to you. Finally, we can get conditional probabilities from the table. For example, we might be interested in the probability of having high blood pressure for overweight men, or in other words, the probability of a man having high blood pressure, given that they are overweight (\\(P(H|O)\\)). When calculation conditional probabilities from a table, we only have to focus on the part of the table that we condition on - since that information is given to us. Given that a man is overweight, we know that we are only concerned with the second row in the table as that is the only part of the table concerning overweight men. Then to get \\(P(H|O)\\) we take the ratio of the probability of men with high blood pressure out of the probability that they are overweight \\(P(H|O) = 0.1 / 0.3 = 1/3\\). Similarly, if we wanted to get the probability of being a reasonable weight blood pressure, given a man has normal blood pressure, we would get \\(P(R|N) = 0.6 / 0.8 = 0.75\\). 1.6 Bayes’ Rule SHOULD PROBABLY THINK OF ANOTHER EXAMPLE We now know that conditional results allow us to incorporate already observed information into a probability calculation. However, conditional probabilities are often easier to reason through (or collect data for) in one direction or the other. For example, suppose a woman is having twins. Obviously, if she were having identical twins, the probability that the twins would be the same sex would be 1, and if her twins were fraternal, the probability would be 1/2. But what if the woman goes to the doctor, has an ultrasound performed, learns that her twins are the same sex, and wants to know the probability that her twins are identical. Both ways of looking at the probably are in terms of a conditional probability - if \\(SS\\) denotes the event that the twins are the same sex and \\(I\\) denotes the event that the twins are identical, we know \\(P(SS|I) = 1)\\) and \\(P(SS|I^C) = 1/2\\). But what we want now is \\(P(I|SS)\\), so the information we are given has changed. The tool we use to “flip” conditional probabilities is called Bayes’ rule. Important formula! Bayes’ rule Consider to events \\(A\\) and \\(B\\), and suppose we know the following probabilities: \\(P(B|A)\\), \\(P(B|A^C)\\), \\(P(A)\\), and \\(P(A^C)\\). If want to get \\(P(A|B)\\), we can use: \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^C)P(A^C)}\\] We can apply Bayes’ rule to the woman having twins scenario, but we need to know one other piece of information: the probability that a pair of twins will be identical (\\(P(I)\\)). Since the proportion of all twins that are identical is roughly \\(1/3\\), we will say \\(P(I = 1/3)\\). Therefore, \\[\\begin{aligned} P(I|SS) &amp;= \\frac{P(SS|I)P(I)}{P(SS|I)P(I) + P(SS|I^C)P(I^C)} \\\\ &amp;= \\frac{1 \\times \\frac{1}{3}}{(1 \\times \\frac{1}{3}) + (\\frac{1}{2} \\times \\frac{2}{3})} \\\\ &amp;= \\frac{1}{2} \\end{aligned}\\] Let’s think about what happened. Before the ultrasound, the probability that the twins were identical, \\(P(I)\\), was \\(1/3\\). This is called the prior probability. After we learned the results of the ultrasound, the probability that the twins were identical, \\(P(I|SS)\\), is \\(1/2\\). This is called the posterior probability. In fact, this prior/posterior way of thinking can be used to establish an entire statistical framework rather different in philosophy than the one we have presented so far in this course. In this way of thinking, we start out with the idea of the possible values of some unknown facet of the world \\(\\theta\\). This distribution of possibilities \\(P(\\theta)\\) is our prior belief about the unknown; then we observe data \\(D\\) and update those beliefs, arriving at our posterior beliefs about the unknown \\(P(\\theta|D)\\). Mathematically, this updating is derived from Bayes’ rule, hence the name for this line of inferential reasoning: Bayesian statistics. One clear advantage of Bayesian statistics is that it is a much more natural representation of human thought. Instead of thinking about the proportion of times an event would occur if it was repeated over and over, we think about the belief it will occur given all of the available information. For something like the outcome of a sports match or the weather this is more intuitive, since the game in only played once and as we are not stuck in the movie Groundhog’s Day, we can not experience a day over and over and record the fraction of times that it rains. However, the scientific community has not widely embraced the notion of subjective beliefs as the basis for science; the long-run frequency approach that we will cover in this course has generally proved more marketable. Bayesian statistics is certainly worth being aware of and is widely used and accepted in many fields. A common application of Bayes’ rule in biostatistics is in the area of diagnostic testing and routine screening, and this is the main application of Bayes’ rule we will focus on. For example, older women in the United States are recommended to undergo routine X-rays of breast tissue (mammograms) to look for cancer. Even though the vast majority of women will not have developed breast cancer in the year or two since their last mammogram, this routine screening is believed to save lives by catching cancer while it is relatively treatable. The application of a diagnostic to asympotmatic individuals in the hopes of catching a disease in its early stages is called screening. Note that this is different than someone experiencing flu-like symptoms and going to the doctor to get a rapid influenza diagnostic test (RIDT). First let’s think about what characteristics would make a good screening test. Given the following cross-classification table of test results vs. true disease status, do you think this would be a good screening test for diabetes? THERES A TABLE HERE THAT WONT SHOW UP When creating a good screening test, we want to correctly classify as many individuals with and without the disease as we can. This means if an individual truly has diabetes, we would want a positive test result and if an individual does not have diabetes, we would want a negative test result as often as possible. Another way of looking at it, is that we want to minimize the errors. We want to minimize the number of truly diabetic individuals that get a negative test result, as these individuals would then be missing out on the proper treatment. Similarly, we want to minimize the number of healthy individuals that receive a positive test, as these individuals could potentially get a treatment they don’t need. Which type of error is more important depends on the context of the disease and the potential treatment, although most often people are concerned with correctly classifying the individuals with the disease. We can talk about these events in terms of conditional probabilities. Consider an individual selected at random from a certain population who is administered a screening test. Define the following events: \\[\\begin{aligned} D &amp;= \\text{the individual has the disease} \\\\ D^C &amp;= \\text{the individual does not have the disease} \\\\ T^+ &amp;= \\text{the individual has a positive test result} \\\\ T^- &amp;= \\text{the individual has a negative test result} \\end{aligned}\\] Then, to assess how well a screening test performs, we consider two important conditional probabilities. Sensitivity is the probability of obtaining a positive test result, given that the individual has the disease: \\[\\text{Sensitivity} = P(T^+|D)\\] Specificity is the probability of obtaining a negative test result given that the individual does not have the disease: \\[\\text{Specificity} = P(T^-|D^C)\\] Sensitivity and specificity are both concerned with correctly classifying individuals, so ideally, both the sensitivity and specificity of a screening test would equal 1. However, diagnostic tests are not perfect, so there is always some misclassification. This leads us to two other conditional probabilities, which are directly related to sensitivity and specificity.A false positive occurs when a positive test result is obtained for an individual who does not have the disease: \\[\\text{False Positive} = P(T^+|D^C) = 1 - P(T^-|D^C) = 1 - \\text{Specificity}\\] A false negative occurs when a negative test result is obtained for an individual that has the disease: \\[\\text{False Negative} = P(T^-|D) = 1 - P(T^+|D) = 1 - \\text{Sensitivity}\\] Here we are using the rule of conditional probability \\(P(A^C) = 1 - P(A)\\) applied to conditional probabilities. In words, once we condition on the disease status the test result can only be positive or negative and since those are the only two events in the sample space the probability of both occurring must add up to 1. The previously defined quantities are all characteristics of the screening test, i.e. the probability of testing positive or negative given a certain disease status. Often, what is of more interest is the disease characteristics, conditional on the screening test results. In other words, we are interested in the probability of having the disease, given a positive test or the probability of not having the disease given a negative test. These two quantities are called the positive predictive value (PPV) and the negative predictive value (NPV) and can be calculated from the sensitivity and specificity using Bayes’ rule. Both the PPV and the NPV can be written in terms of the test’s sensitivity, specificity, and disease prevalence \\[\\begin{aligned} PPV &amp;= P(D|T^{+}) \\\\ &amp;= \\frac{P(T^+|D)P(D)}{P(T^+|D)P(D) + P(T^+|D^C)P(D^C)} \\\\ &amp;= \\frac{\\text{Sens}\\times \\text{Prev}}{\\text{Sens}\\times \\text{Prev} + (1 - \\text{Spec}) (1-\\text{Prev})} \\end{aligned} \\begin{aligned} NPV &amp;= P(D^C|T^{-}) \\\\ &amp;= \\frac{P(T^-|D^C)P(D^C)}{P(T^-|D^C)P(D^C) + P(T^-|D)P(D)} \\\\ &amp;= \\frac{\\text{Spec}(1-\\text{Prev})}{\\text{Spec}(1-\\text{Prev}) + (1 - \\text{Sens}) \\text{Prev}} \\end{aligned}\\] Need to add that we only use prevalence in the case of screenings. If you have symptoms than we have a different prior on the probability of you having the disease. "],
["ch5.html", "2 Probability Distributions 2.1 Introduction to Probability Distributions 2.2 Binomial Distribution 2.3 Normal Distribution 2.4 Poisson Distribution 2.5 t Distribution 2.6 Review of Distributions", " 2 Probability Distributions Learning objectives Understand the definition of a probability distribution Binomial distribution Normal distribution t distribution In the previous chapter, we introduced the idea of a random processes, situations with outcomes that we could not determine perfectly in advance. The idea of a random process can apply to most everything in our lives, from the exact amount of time it takes to go from home to class, to determining the winner of a football game. Further, recall that a random process is defined by the collection of possible events and their associated probabilities, [framed in terms of long run frequencies (discuss?)]. In the coin flipping example, where the coin was flipped three times, the collection of possible events was \\[\\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\},\\] and the associated probabilities for the number of heads were: ## Why does this need cbind? knitr::kable(cbind.data.frame(&#39;# Heads&#39; = 0:3, &#39;Probability&#39; = c(&#39;1/8&#39;, &#39;3/8&#39;, &#39;3/8&#39;, &#39;1/8&#39;)), align = &#39;c&#39;) # Heads Probability 0 1/8 1 3/8 2 3/8 3 1/8 After tabulating all of the possible events, we were able to determine probabilities by tabulating the number of ways each event could occur and dividing by the total number of possibilities (in this case, eight). Of course, this can quickly become cumbersome in general: if three flips resulted in eight possible events, and four flips would result in sixteen, imagine trying to determine all of the possible outcomes of flipping a coin fifty times! Clearly, we need a more efficient way to do this. [I wrote this when it was two. Not sure if we should keep or not, or reword] Further, suppose we were asked to anticipate the number of heads that would occur after three flips. The table above would lead us to conclude that our best guess would be two or three, as it has twice the probability of either of the other outcomes. In other words, the distribution of outcomes is not completely random: there appear to be some structure in the ways these outcomes unfold. This, along with the need for a more precise way of determining possible outcomes and probabilities, leads us to the topic of the current chapter. 2.1 Introduction to Probability Distributions Put simply, a probability distribution is a function that takes a possible event as input, and gives us the resulting probability as output. -In the last chapter, we discussed random processes, events, and probability. We noted that probability is framed in terms of long-run frequencies, or the fraction of time an event occurs if a random processes is repeated over and over. One example we looked at was flipping a coin three times and we were able to determine probabilities for specific events by tabulating the number of ways the event could occur and dividing by the number of possible outcomes of flipping the coin three times. This is a feasible approach when there are not very many possible outcomes, but otherwise can be quite cumbersome. Imagine trying to tabulate all the possible outcomes of flipping a coin 50 times! Yeah, we don&#39;t want to imagine that either. This is where the genius of probability distributions comes to our rescue. A **probability distribution** is a function that describes the probability of all possible outcomes for an experiment. The input to the function is the outcome of interest, and the output is the probability of observing that outcome. Probability distributions typically have one or two **parameters**, which describe the distribution. In the coin flipping example, the parameter would be the number of flips. -Before we get to some of the most commonly used probability distributions and the pre-defined functions, we will illustrate the concept by deriving a probability distribution for our toy example. Before we get to some of the most commonly used distributions, let’s illustrate the concept by formally deriving a probability distribution for our example above. Suppose the experiment consists of flipping a coin three times and recording the number of heads. We will let \\(X\\) denote the number of heads that appear. We know that the possible values \\(X\\) can take on are \\(\\{0, 1, 2, 3\\}\\), since we can’t see more heads than we have flips, and we can’t have less than zero. As we saw in the previous chapter, there are eight possible outcomes and the sample space for this experiment is \\[\\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}.\\] For each of these possible outcomes, we can count the number of heads - \\(\\{3, 2, 2, 2, 1, 1, 1, 0\\}\\). To find the probability distribution, we calculate the probability that \\(X\\) takes on each possible value library(knitr) tmpTab &lt;- cbind.data.frame(x = 0:3, &#39;P(X=x)&#39; = c(&#39;1/8&#39;, &#39;3/8&#39;, &#39;3/8&#39;, &#39;1/8&#39;)) kable(tmpTab, align = &#39;c&#39;) x P(X=x) 0 1/8 1 3/8 2 3/8 3 1/8 Since there is one possible event where 0 heads were obtained out of the eight possible events, \\(P(X = 0) = 1/8\\). Similarly, there are three possible events where 1 head was obtained, so \\(P(X = 1) = 3/8\\). If we were to plot this probability distribution, it would look like this x &lt;- 0:3 y &lt;- dbinom(x, 3, 0.5) barplot(y, names.arg = x, xlab = &#39;Number of Heads Observed on Three Flips&#39;, ylab = &#39;Probability&#39;) We can also think about these probabilities from a simulation perspective. In this case, we are repeating the experiment of flipping the coin 3 times over and over. So to examine these probabilities from a simulation, we must simulate flipping three coins repeatedly, each time recording the total number of heads observed out of the three flips. If we do enough simulations, we should see the proportion of times each number of heads occurs follows the probability distribution we just calculated. flipCoin3 &lt;- function() { dat &lt;- rbinom(3, 1, 0.5) sum(dat) } nSims &lt;- 1000 simRes &lt;- replicate(nSims, flipCoin3()) prop.table(table(simRes)) ## simRes ## 0 1 2 3 ## 0.121 0.373 0.387 0.119 2.1.1 A final note There are three more characteristics of probability distributions that will be relevant to us going forward. The first of these is a concept known as independent and identically distributed, often written IID for short. Observations from a random experiment are independent when the value of one observation does not have an impact on the other. For example, if we flip a fair coin twice, the value of the first flip has absolutely no effect on the second. That is, if my first flip results in Heads, the second flip is equally likely to be Heads as it is Tails. A common fallacy related to this is the Gambler’s fallacy, in which an individual, having witnessed a fair coin flip land on Heads 20 times in a row falsely believes there is a higher probability of landing on heads for the 21st flip. In actuality, the probability of landing on heads continues to be 50%. Observations from a random experiment are identically distributed if each observation comes from the same probability function. 10 flips from a fair coin would be identically distributed, while 9 flips from a fair coin and one flip from an unfair coin would not. Next, most of the probability distributions that we will be working with are defined in terms of parameters. Often, distribution functions are defined in the most general way possible, specifying a general structure to a class of problems, using parameters to adjust the distribution to the problem at hand. For instance, all random experiments involving the flipping of coins have a similar structure; what changes betweeen experiments may include the number of times a coin is flipped or the probability of landing on heads. With the use of parameters, a single distribution function can be used to describe a wide variety of possible situations. The last thing we need to know about probability distributions is that there are two types, discrete and continuous. This is a little different than the types of data discussed in Chapter 2 (categorical or continuous). Discrete distributions calculate probabilities for specific numeric values (most often counts). The coin flipping example we just looked at was a discrete distribution, because there were four distinct possibilities of the number of heads we could observe. On the other hand, continuous distributions describe probabilities over a continuum with a smooth curve, such as an individual’s height, weight, or systolic blood pressure. In these distributions, it makes less sense to ask the probability of achieving an exact value (i.e., probability of weighing exactly 153.489 lbs) and more sense to inquire about a range of values (probability that an individual weighs more than 185 lbs). For continuous distributions, probabilities are described by the area under the curve. This is a fourth thing, but maybe we put it closer to a definition of random variable vs observed, though currently that’s defined in a few places. We will let \\(X\\) describe a random variable that follows some distribution, with \\(x\\) being the value of \\(X\\) once it is observed. 2.2 Binomial Distribution The first distribution we will cover is one that we are already familiar with. The binomial distribution is a discrete distribution that describes the number of successes in a fixed number of independent trials in which exactly two outcomes are possible. The term “success” can be confusing depending on the situation; in our previous examples, we have considered “successes” to be the number of heads observed, when we just as well could have been counting the number of tails. Even more confusing, in biomedical trials a “success” could be used to describe an adverse reaction to a drug, or even death. To avoid this confusion, we will instead consider an outcome to be either an “event” or a “non-event.” In the context of our previous examples, counting the number of heads in a sequence of flips will be synonymous with counting the number of “events” that we have observed. There are three assumptions of the binomial distribution: There is fixed number of trials, \\(n\\), each of which has two possible outcomes Each trial is independent of the others Each trial has the same probability \\(p\\) of an event occurring We will let \\(X\\) describe an experiment which follows a binomial distribution with parameters for the number of trials (\\(n\\)), with probability of an event \\(p\\). Syntactically, we write this as \\(X \\sim Bin(n, p)\\). The probability distribution function is described by the following formula: \\[P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}\\] and the possible observed values of \\(X\\) are \\(x = 0, 1, ..., n\\). Our coin flipping example above is a case of a random experiment following a binomial distribution, with distribution parameters \\(n = 3\\) and \\(p = 0.5\\). Substituting these values into our distribution function, we may describe the random experiment with the formula: \\[P(X = x) = \\binom{3}{x} (0.5)^x (1-0.5)^{3-x}\\] Perhaps new to us here is the leading term in the expression above, \\(\\binom{n}{x}\\), which is called a binomial coefficient, and can be written with the formuila \\[ \\binom{n}{x} = \\frac{n!}{x!(n-x)!} \\] where \\(n! = n \\times (n-1) \\times (n-2) \\times \\ ... \\ \\times 2 \\times 1\\) (known as a factorial). In words, we might say \\(\\binom{n}{x}\\) as “n choose x.” While this may seem daunting at first, the need for it is quite reasonable. Consider our current experiment, with \\(n = 3\\), and the list of the possible outcomes, given above: \\[\\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}.\\] When we are investigating the probability of observing two heads after flipping a coin three times, we are asking: “with \\(n = 3\\) flips, how many ways can we choose \\(x = 2\\) heads,” or, “three choose two.” Counting the number of scenarios above in which two heads appear tells us that it is three, and indeed, we find that \\[ \\binom{3}{2} = \\frac{3!}{2!(3-2)!} = \\frac{3 \\times 2 \\times 1}{2 \\times 1 \\cdot(1 \\times 1)} = \\frac62 = 3 \\] Using the binomial distribution function for \\(n = 3\\) and \\(p = 0.5\\), confirm the values here that we given in the table above tmpTab &lt;- cbind.data.frame(x = 0:3, &#39;P(X=x)&#39; = c(&#39;1/8&#39;, &#39;3/8&#39;, &#39;3/8&#39;, &#39;1/8&#39;)) kable(tmpTab, align = &#39;c&#39;) x P(X=x) 0 1/8 1 3/8 2 3/8 3 1/8 The mean of a binomial distribution in \\(E(X) = np\\) and the variances is \\(Var(X) = np(1-p)\\) 2.2.1 Plot of Binomial We can visualize a binomial distribution with a plot: each of the values on the \\(x\\)-axis represent a number of events observed, while the values on the \\(y\\)-axis represent the probabilities x &lt;- 0:3 y &lt;- dbinom(x, 3, 0.5) barplot(y, names.arg = x, xlab = &#39;Number of Heads Observed on Three Flips&#39;, ylab = &#39;Probability&#39;, yaxt=&#39;n&#39;) axis(side = 2, at = seq(0, 0.45, by = 0.05), pos = c(0, 0), labels = seq(0, 0.45, by = 0.05)) Of course, as the number of flips \\(n\\) changes, so does the associated plot x &lt;- 0:6 y &lt;- dbinom(x, max(x), 0.5) barplot(y, names.arg = x, xlab = &#39;Number of Heads Observed on Six Flips&#39;, ylab = &#39;Probability&#39;) 2.3 Normal Distribution The normal distribution, also known as a Gaussian distribution, is a continuous distribution, and is perhaps the most well recognized of the statistical distributions, often informally referred to as a “bell curve.” curve(dnorm(x), from = -3, to = 3, main = &#39;maybe delete this since we do plots below&#39;) There are a number of properties that together characterize the normal distribution: There are two parameters for the normal distribution, the mean \\(\\mu\\) (pronounced “myu”) and the variance \\(\\sigma^2\\) (“sigma squared”) \\(\\mu\\) is the mean, or expected value, and represents the most probably value of the distribution. That is, observations from a normal distribution are more likely to be close to \\(\\mu\\) than away from it Observations are equally likely to be the same magnitude above \\(\\mu\\) as they are below it. In other words, the distribution is centered around \\(\\mu\\). We see this concept expressed in every day language when we offer estimates of some value: “The cost is ‘x,’ plus or minus ‘y’” The second parameter, \\(\\sigma^2\\), describes how concentrated values are around the mean. The smaller the value of \\(\\sigma^2\\), the more observations that will be close to \\(\\mu\\). Likewise, larger values of \\(\\sigma^2\\) result in higher dispersion, or more values further away from \\(\\mu\\). If a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we may express it syntactically with \\(X \\sim N(\\mu, \\sigma^2)\\). A special case of this known as the standard normal distribution arises when \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). We usually write this with the letter \\(Z\\), or \\(Z \\sim N(0, 1)\\). Many things with which we are familiar can be described with a normal distribution, including the height of individuals in a population, grades on an exam, and average shoe size. Amazingly, as we will see in the following chapters, even the number of events that we observe from a binomial distribution will follow as normal distribution, as the binomial parameter \\(n\\) increases towards infinity. In addition to most things that we measure following a normal distribution, we will also see that the errors we make when measuring are often normal. When measuring the arc distance between two stars in the sky, astronomers are just as likely to record a measurement that is just over the true value as they are to record one that is just under it as well. [Maybe need to describe difference between pdf and pmf since we don’t use P(X = x)] \\[ p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\ e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\] The mean of a normal distribution is \\(E(X) = \\mu\\) and the variance is \\(Var(X) = \\sigma^2\\) 2.3.1 Plot for Normal In constrast to the binomial distribution, which had discrete probabilities, the normal distribution is continuous, requiring that probabilities be calculated via integration (this is somewhat analagous to the trapazoidal rule, for those who remember from calculus). As a consequence of this, we are not able to ask meaningful questions about the probability of a specific point (the intergral of a point is always zero, maybe getting into weeds here). We won’t go into detail here, mostly because I’m not quite sure what details we might need, but supposing that \\(Z \\sim N(0, 1)\\), we might ask, what is the probability that \\(Z\\) is greater than 1? This probability is represented by the red region in the plot below x &lt;- seq(-3, 3, 0.01) y &lt;- dnorm(x) plot(x, y, type = &#39;l&#39;) polygon(c(x[x &gt; 1], max(x), 1), c(y[x &gt; 1], 0, 0), col = &#39;red&#39;) Things in normal that I didn’t include but we may want calculating the probabilities more detail on how/where/when it comes up we haven’t introduced estimators yet, so somewhat limited 2.4 Poisson Distribution The Poisson distribution, like the binomial, is a discrete distribution, in that it concerns itself with count data. Specifically, a Poisson distribution describes the number of independent events that may occur within a fixed interval of time. For example, we may be interested in the number of cars that pass through a busy intersection from noon to 1pm every day, or the number of major floods that occur in an area every 100 years. Perhaps the most famous example of the Poisson distribution comes courtesy of Ladislaus Bortkiewicz, a Russian statistician who, in 1898, showed that the number of Prussian soldiers killed by being kicked by a horse in a twenty year period followed a Poisson distribution (also child suicides, but that’s less fun). The Poisson distribution has a single parameter, \\(\\lambda\\), which describes the rate at which events occur, and a random variable following a Poisson distribution may be expressed as \\(X \\sim Pois(\\lambda)\\) (People who write \\(X \\sim Po(\\lambda)\\) are heathens). A random variable following a Poisson distribution has the following assumptions: The value of \\(X\\), being a count, can be any non-negative integer, i.e., \\(0, 1, 2, \\dots\\) The occurence of one event in a time interval is independent of another event. One soldier being kicked by a horse has no impact on the probability of another solider being kicked by a horse. \\(\\lambda\\), which may be any number greater than \\(0\\), describes the rate at which events occur [and is independent of the occurence of events] [Two events cannot occur at the exact same time, though they probably don’t need this] The distribution function of a Poisson random variable with rate \\(\\lambda\\) can be expressed \\[ P(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\] One surprisingly detail about the Poisson distribution is the relationship between the mean and the variance. For both, we have that \\(E(X) = Var(X) = \\lambda\\). 2.4.1 Plots for Poisson As we look at the plot for the Poisson, we will notice one aspect in particular that distinguishes it from the plots of both the binomial and normal distributions: it is no longer symmetric. This is a consequence of the range of values that a Poisson random variable can take on. Whereas a binomial random variable was bounded between \\(0\\) and \\(n\\), the number of trials conducted, and where the normal distribution allowed any real number, the Poisson is bounded below by \\(0\\), while having no theoretical upper bound. Given below is a plot of the distribution with \\(\\lambda = 4\\) (it’s obvious here that choosing a specific value is inadequate. We can replace these plots with distribution exploration apps) x &lt;- 0:15 y &lt;- dpois(x, lambda = 4) barplot(y, names.arg = x, xlab = &#39;Number of Events Observed in Interval&#39;, ylab = &#39;Probability&#39;, main = expression(paste(&quot;Poisson distribution with &quot;, lambda, &quot; = 4&quot;))) Need to do maybe: relate \\(\\lambda\\) to a specific interval, say, an hour Interactive (in general not just poisson) 2.5 t Distribution Actually not sure on this here, as we haven’t introduced estimators. Will come back to this 2.5.1 Plot for t distribution 2.6 Review of Distributions [[table]] "],
["ch6.html", "3 Sampling Distributions and the Central Limit Theorem 3.1 Sampling Distributions", " 3 Sampling Distributions and the Central Limit Theorem “While nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” - Elizur Wright “Everything that can be counted does not necessarily count; everything that counts cannot necessarily be counted.” - Albert Einstein https://iwant2study.org/lookangejss/math/ejss_model_GaltonBoardwee/GaltonBoardwee_Simulation.xhtml Learning objectives Learn to differentiate between statistics and parameters Understand the concept of sampling distributions Conceptual understanding of Central Limit Theorem and how it applies to sample means Questions we can answer at the end of this chapter What is a statistic? What is a parameter? What is the relationship between the two? (Draw a picture would be a cool exercise. Idk how you’d grade it) Why are sampling distributions random? Suppose we wanted to ask the state of Iowa which university had the best football team, and we only ask in Iowa City. Would this be a random sample? Could we use this sample to make reliable predictions about the entire state? Why or why not? Prove the Central Limit Theorem using characteristic functions for the iid case, assuming finite mean and variance. Ideas to make things interesting - The Wall TV show. 3.1 Sampling Distributions Definition Population Parameter: The true numeric quantity about a population Suppose policy makers and public health experts in Iowa are concerned with Iowan’s fast food intake. To understand this further, the officials want to determine the average monthly spending on fast food for all Iowans. However, it would be very expensive and costly to have all 3.2 million Iowans track and report their total monthly spending on fast food. And certainly many Iowans would not be willing to share personal financial details with public health researchers or government officials. This means it is impossible for the officials to know the true mean amount of money that Iowans spend on fast food each month. The true numeric quantity about the population, such as this, is known as a population parameter. While we will never ever know the truth, there are still options. The officials can take a sample of Iowans and have those that consent to share their information report their monthly spending on fast food. Then, they can take the sample mean, which gives them an estimate of the average monthly spending on fast food. The sample mean is an example of a sample statistic, which is a numerical quantity about the sample. In other words, statistics are what investigators know, and parameters are what investigators want to know. The statistical framework allows us to make inference about population parameters using sample statistics. This means the researchers can use a random sample of Iowans’ monthly fast food spending to make generalizations about the average monthly spending for all Iowans. Figure 3.1: Statistical Framework We also want to know whether or not our estimated statistic is measuring the parameter well. To that end, there are two major issues we concern ourselves with: * On average, does our estimate tend to be centered around the true answer, or is it biased? * How much variability is there likely to be in our sample? The difference between our estimate and our parameter is called bias. Variance describes the spread of our data (I DON\"T LIKE THIS DEFINITION), and is sometimes called noise. Sometimes we talk about spread in terms of precision, which is the inverse of variance. The more precision we have, the less variance, and vice versa. For some reason, it doesn’t like pdfs, and it doesn’t like `out.width=“0.75\\linewidth”’ Figure 3.2: Some cool caption PRACTICE SAFE STATISTICS Don’t drink and derive In order to make inference, we must be able to quantify our uncertainty about the population parameter. As we will see later in this chapter, this uncertainty depends on how many individuals we sample and how variable the data we measure is. In our statistical framework, there is some unknown population parameter which we estimate with a sample statistic. In the fast food example, the parameter of interest was a population mean and the sample statistic was the sample mean. In this chapter, we will specifically focusing on estimating means, however, there are other possible numerical quantities we could describe - proportions, standard deviations, etc. . Any time we take a sample from the population, we will get a different sample mean \\(\\bar{x}\\). In other words, \\(\\bar{x}\\) is a numeric quantity which assumes a value based on the outcome of a random experiment - the process of drawing a sample at random from the population. This should sound familiar to you - this is the definition of a random variable! We can use this logic for any statistic of interest. As we’ve described in the past, random variables have probability distributions. When the random variable is a statistic, this probability distribution is called the sampling distribution. Sampling distributions reflect which values of the statistic are likely and which values are improbable. A VISUALIZATION ABOUT SAMPLING DISTRIBUTIONS WOULD BE COOL HERE. SOMETHING THAT SHOWS ALL THE UNITS IN THE POPULATION WITH NUMERIC VALUES ON THEM, THEN TAKE SAMPLE AND CALCULATE MEAN. I think what would be neat is doing dot plots for a population and have them be all one color, say, red. Once the samples are chosen, they will be presented below with their own distribution in blue. BUT! The ones selected in the random sample would also turn blue, indicating that they were picked. Maybe this is like what that shiny app you showed me was? As we’ve seen earlier in this book, we will use upper-case letters to denote random variables, and lower-case letters to denote possible values they can take on. When it comes to the sampling distribution of the mean, \\(\\bar{X}\\) is the random variable, which arises from repeated random sampling from the population and then taking a sample mean, and \\(\\bar{x}\\) is an observed sample mean we might see. In describing a sampling distribution, we are often interested in the mean and standard deviation - this gives information about typical values the statistic might take on as well as how spread out the distribution is. We refer to the mean of a statistic as the expected value and the standard deviation as the standard error. For the random variable describing the sample mean, these are denoted as \\(E(\\bar{X})\\) and \\(SE(\\bar{X}) = \\sqrt{Var(\\bar{X})}\\), respectively. Another good way to address the \\(\\bar{X}\\) vs \\(\\bar{x}\\) is to have something like (&amp;= isn’t making them align?) \\[ \\begin{align*} \\bar{X} &amp;= \\{\\text{Mean height of} \\textit{ Sex and the City } \\text{protagonists}\\} \\\\ \\bar{x} &amp;= \\{\\text{Mean height of Carrie and Samantha}\\} \\end{align*} \\] Example We will use the fast food scenario to motivate our sample, but for let’s assume there are only five individuals in the entire population. The monthly spending for those five individuals (rounded to the nearest dollar) is as follows: Person Monthly Spending A 8 B 22 C 22 D 36 E 50 The population mean can be found by taking the average monthly spending of these five individuals, as they are the only people in this population. This means the population mean is 27.6. Now suppose we are taking samples of three individuals from this population. There are \\(\\binom{5}{3}\\) = 10 ways we can sample three people from this population. Since this is a small population, we can enumerate all possible samples, the values we would obtain for the monthly spending in each sample, and then the sample mean monthly fast food spending for each sample: Sample Values \\(\\bar{x}\\) (A, B, C) (8, 22, 22) 17.33 (A, B, D), (A, C, D) (8, 22, 36), (8, 22, 36) 22.00 (A, B, E), (A, C, E), (B, C, D) (8, 22, 50), (8, 22, 50), (22, 22, 36) 22.00 (A, D, E), (B, C, E) (8, 36, 50), (22, 22, 50) 26.67 (B, D, E), (C, D, E) (22, 36, 50), (22, 36, 50) 36.00 One thing to note is that none of the sample means are actually equal to the population mean. This is often the case! Based on this table, we can construct the sampling distribution of \\(\\bar{X}\\) for a sample of size three. Why is our sample mean often not equal to the population mean? Probability P(\\(\\bar{x}\\) = 17.33) 0.1 P(\\(\\bar{x}\\) = 22) 0.2 P(\\(\\bar{x}\\) = 26.67) 0.3 P(\\(\\bar{x}\\) = 31.33) 0.2 P(\\(\\bar{x}\\) = 36) 0.2 With this probability distribution, we can get the expected value of the sampling distribution. Whenever we think about probability, we are thinking about long-run frequencies, so when we think about the expected value, we are thinking about the average sample mean if we were to take repeated samples of size three from this population. The probability distribution indicates that if we took samples of size three from this population over and over again (say 1,000 times), we would end up with 10% with a mean of 17.33, 20% with a mean of 22, 30% with a mean of 26.67; and so on. So the expected value (average) we would observed if we kept taking samples over and over would be: (0.1 \\(\\times\\) 17.33) + (0.2 \\(\\times\\) 22) + (0.3 \\(\\times\\) 26.67) + (0.2 \\(\\times\\) 31.33) + (0.2 \\(\\times\\) 36) = 27.6 But wait! That is the population mean. This illustrates an extremely powerful property of the sample mean - the expected value of the sample mean (\\(\\bar{X}\\)) is equal to the population parameter (\\(\\mu\\))! Because of this, we say that the sample mean is an unbiased estimator of the population mean. While we illustrated the property with a small example, this holds regardless of the population or sample size. Important Formula! For any population distribution which can be described by a random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the sample distribution of the mean based on sample of size \\(n\\) (\\(\\bar{X}_n\\)) has the following properties: The expected value of the sampling distribution is \\(\\mu\\) \\[E(\\bar{X}_n) = \\mu\\] The variance of the sampling distribution is \\[Var(\\bar{X}_n) = \\frac{\\sigma^2}{n}\\] Using these formulas we are able to describe the typical value of the sample mean and how variable the sample mean is in repeated sampling. Notice that the expected value does not depend on the sample size, \\(n\\), but the variance does. This means regardless of the sample size, the sample mean is an unbiased estimator of the population mean. But, as the sample size increases, the variance decreases, and we get a more precise estimate of the population mean. As we’ve seen in the past, it is more common to talk about the standard deviation than the variance and the standard deviation is the square root of the variance. The quantity \\(\\sqrt{Var(\\bar{X}_n)} = \\sigma / n\\) is primarily called the standard error of the mean, and is denoted by \\(SE(\\bar{X}_n)\\). We use the term standard error to make it really clear that it is referencing the sampling distribution of the mean, but all standard errors could be refered to as standard deviations. Conversely, not all standard deviations could be referred to as a standard error - the term is only used in describing the distribution of the sample mean. Example Let \\(X\\) denote the resting heart rate of a randomly selected U.S. adult. Assume the distribution of resting heart rates is approximately normally distributed with mean 70 bpm and standard deviation of 5 bpm. Suppose we sample 50 adults. Let \\(\\bar{X}\\) denote the mean resting heart rate of these 50 adults. What is the mean of \\(\\bar{X}\\)? (That is, in repeated sampling, what would be the average value of \\(\\bar{X}\\)?) What is the standard error of \\(\\bar{X}\\) "],
["ch7.html", "4 Introduction to Inference 4.1 Confidence Intervals 4.2 Hypothesis Tests 4.3 P-values", " 4 Introduction to Inference \" A useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in his experiment.\" - Gertrude Mary Cox Learning objectives Understand conceptually how confidence intervals are constructed Know correct interpretation of confidence intervals Hypothesis testing null and alternative hypothesis Type 1 and type 2 error Definition of p-value and how to interpret Up until this point, we have introduced probability, which allows us to quantify our uncertainty, we have discussed the concept of random variables and two widely used probability distributions (binomial and normal), and we have seen the most powerful result in statistics, the Central Limit Theorem which gives us the distribution of the sample mean in repeated samples. Let’s take a step back and remind ourselves of the goal of a statistical analysis. There is some population level numerical quantity we are interested in (parameter). This could be a mean or a proportion, or it could be something more complicated. Because we can (almost) never measure something in the entire population, we are restricted to taking a sample. Our goal is to use the data we collect in our sample to make inference, or generalizations, about the parameter, which is the unknown quantity of interest. In particular, we want to use probability distributions and the observed data to quantify what values of the parameter are plausible and which are not pluaible and we would like to determine if the trends we observe in our data are “significant” or if it is simply a result of chance. 4.1 Confidence Intervals We will first turn our attention to parameter estimation, of which there are two general approaches. These approaches are point estimation, which is a single statistic computed from the sample data used to estimate the parameter and interval estimation, which is an interval computed from the sample data to give a range of plausible values that is likely to contain the true population parameter. Point estimation is straightforward and is as simple as using the sample mean as an estimate of the population mean. While straightforward, this approach is problematic. In the last chapter we saw that every time we take a sample from a population and compute the sample mean, it changes. And we saw an example in which no matter how we took a sample, the sample mean could never possibly equal the population mean. This is because the act of taking a sample is a random process, which means that for any estimate from our sample it is highly unlikely (or straight up impossible) that we got the exact correct answer. Instead, it would be nice to have an interval that we could be reasonably confident contained the true parameter. What properties would make a good interval estimate? Well, on one hand we desire an interval that we can have a high degree of confidence of including the true parameter, but on the other hand, a narrow interval is much more informative. I can tell you with 100% confidence that the interval from \\(-\\infty\\) to \\(\\infty\\) covers the true population parameter, and I don’t even need to know anything about what you’re studying or what numerical quantity is being estimated and I don’t even need any data! I’ve achieved the first goal of having a high degree of confidence, but this interval is clearly not informative. I can make the interval smaller (I will need some data to do that), but the probability that the interval contains the true parameter will decrease. As a compromise, scientists and statisticians decided that intervals with a 95% probability of containing the true parameter was a satisfactory balance between the desire for high confidence and narrow intervals. This gives way to the most common confidence interval – the 95% confidence interval. Although we are getting a little bit ahead of ourselves here (we will have plenty of chances to calculate 95% confidence intervals). Let’s back up with some important definitions and notation. With an interval estimate, we are concerned with coverage probability, the probability that the interval contains or covers the parameter of interest. Typically, coverage probability is denoted by \\(1 - \\alpha\\). As the interval covers the parameter with probability \\(1 - \\alpha\\), the interval does not cover the parameter with probability \\(\\alpha\\) (because covering and not covering the parameter are complements and cover the entire sample space). In that sense, \\(\\alpha\\) is the probability that we are wrong. We will discuss \\(\\alpha\\) in more specific terms later in the chapter, but for now let’s just note that \\(\\alpha\\) is a small number, maybe 0.05 or 0.01. If we convert from decimal format to percentage format (multiplying by 100), we have \\(100(1 - \\alpha)\\)%, which is called the confidence level. The mathematical details of constructing a confidence interval depends on the parameter of interest and how the study was designed, and we will see various formulas for constructing confidence intervals throughout the rest of this book. In this chapter, we will focus on conceptual understanding and interpretation. To construct a \\(100(1 - \\alpha)\\)% confidence interval, we use sample data and sampling distributions as we know that sampling is a random process. For example, the Central Limit Theorem tells us the sampling distribution of the mean is normally distributed and gives us the mean and standard deviation of that distribution. Once we have a normal distribution for which we know the mean and standard deviation, we can calculate probabilistic quantities using software. It’s extremely important that we remember what is random and what the probability distribution is specifying when we are interpreting confidence intervals. The sampling distribution describes which values of the sample statistic are likely and which are not likely. The random process is drawing a sample. The population parameter is a fixed quantity – we don’t know what it is, but it does not change. Confidence intervals are constructed from the information we do know – the observed data from the sample. Because sampling is a random process and confidence intervals are constructed using the sample data, the confidence interval will vary from sample to sample. However, we formulate confidence intervals mathematically to ensure that the probability that the interval contains the true probability is \\(1 - \\alpha\\). And as we’ve seen probabilities are long-run frequencies. So any given confidence interval may or may not cover the true parameter, but in the long run if we take samples over and over and compute a confidence interval based on the data from each sample, we are guaranteed that \\(100(1 - \\alpha)\\)% of the confidence intervals constructed will cover the truth. This is quite appealing. Let’s explore this concept further by using the following applet. Exercises Set the sample size to 30, the confidence level to 95%, and simulate running 100 experiments and calculating a confidence interval for each experiment (100 intervals total). How many intervals contained the true population parameter? What was the observed coverage? What would you expect it to be? Re-run the simulation under the same conditions. What was the observed coverage for the second set of 100 experiments? Keep the confidence level at 95%, but change the sample size to 50. Simulate 100 experiments. How do the intervals differ from those constructed from experiments using a sample size of 30? What is the observed coverage? Re-run the simulation, now with a sample size of 100. How do these intervals compare to those using sample sizes of 30 or 50? Play around with various sample sizes. How does changing the sample size effect the observed coverage? Return to a sample size of 30. Now we will simulate 1000 experiments. With a confidence level of 95%, what is the observed coverage? Re-run the simulation, now with a sample size of 100. How do these intervals compare to those using sample sizes of 30 or 50? Re-run the simulation using a confidence level of 50%. How do these intervals compare to those using a confidence level of 95%? What is the observed coverage? 4.2 Hypothesis Tests Along with parameter estimation, a cornerstone of statistical inference is significance testing. Confidence intervals allow us to construct a range of plausible values for the parameter, and significance tests allow us to determine the likelihood of a parameter taking a certain value. A hypothesis test uses sample data to assess the plausibility of each of two competing hypotheses regarding an unknown parameter (or set of parameters). A statistical hypothesis is a statement or claim about an unknown parameter. The null hypothesis generally represents what is assumed to be true before the experiment is conducted. This hypothesis is typically denoted \\(H_0\\). The alternative hypothesis represents what the investigator is interested in establishing. This hypothesis is typically denoted \\(H_A\\). Oftentimes when people refer to the “scientific hypothesis,” this in reference to the alternative hypothesis - it is what the investigators think will happen or what they want to show. The goal of a hypothesis test is to determine which hypothesis is the most plausible - the null or the alternative. As an example, consider researchers that have developed a new drug to treat cancer. In order for the drug to be approved for use, the investigators must prove that it is more effective in treating cancer than the current gold standard treatment. To do this, the investigators gather a sample of cancer patients and randomize half of them to receive their new drug and half to receive the current treatment. Then they determine how many patients improved in both groups. In this scenario, the null hypothesis would be that the new drug and the current drug result in the same improvement. Why? Well, the null hypothesis is what is believed before the data was collected. The key is whose beliefs we are talking about. While the scientists that developed the drug most likely believe that their new drug is more effective, the rest of the scientific community remains in a state of uncertainty. The null hypothesis reflects the general beliefs of the scientific community. The alternative hypothesis in this scenario is that the drugs differ in their effectiveness on treating cancer. This is what the researchers hope to show, specifically, they hope to show that their drug is more effective, however, when the study is conducted there is no evidence in either direction, so we leave the alternative hypothesis to also encompass the possibility of the new drug being worse. In general, we can think about the null hypothesis as being the “baseline,” “boring,” “nothing to see here” hypothesis. The exact specification will depend on the study context and the type of data being measured (categorical or continuous): \\(H_0\\): the average cholesterol for hypertensive smokers’ is no different than the general population \\(H_0\\): no difference between the treatment and control groups \\(H_0\\): men and women have identical probabilities of colorectal cancer \\(H_0\\): observing a ``success\" in a population is identical to flipping a coin The hypothesis testing procedure uses probability to quantify the amount of evidence against the null hypothesis. Since the null hypothesis is the baseline, we start by assuming that it is true. Then, we conduct the study and collect data to quantify the likelihood that the the null is true. The reason for this approach is rooted in the scientific method. As we introduced in Chapter 1, the scientific method has 7 steps: Ask a question Do background research Construct a hypothesis Test your hypothesis with a study or an experiment Analyze data and draw conclusions How do the results align with your hypothesis? Communicate results We are really focusing on steps 3-5. In step 3, we construct the “scientific hypothesis” and in step 4, we test that hypothesis. In order to produce rigorous scientific results we cannot assume that the scientific hypothesis is true, as that is the goal of our study. We must assume the current state of knowledge (null hypothesis) and then if we are to prove that our hypothesis is correct, we would show that if the current knowledge was true, it would be really unlikely that our experiment would have ended up how it did. A great analogy to the concept of hypothesis testing is our judicial system. In court, the legal principle is that everyone is “innocent until proven guilty” and the prosecution must prove that the accused is guilty beyond a reasonable doubt. In many cases, there may not be definitive evidence if the defendant is actually innocent or guilty. But, if the prosecution can show that the likelihood of the accused individual being guilty is high (or equivalently, that the likelihood of the accuwsed individual being innocent is low), then the defendant will be convicted. In hypothesis testing researchers are like the prosecution and must use data to prove that the null hypothesis (current state of knowledge) is false beyond a reasonable doubt. The next several chapters will go through how we can use different types of data to quantify our evidence against the null hypothesis. With this set up in mind, we have two possible outcomes of a hypothesis test. Either we conclude that we do not have a lot of evidence against the null hypothesis, i.e. the null hyptohesis looks reasonable, and we fail to reject the null or we conclude that we have enough evidence against the null and we reject the null. It is extremely important to note here that we NEVER accept the null or accept the alternative. Many people find this annoying because we can never say anything with 100% certainty. But this is exactly the point! Remember that statistics is all about quantifying our uncertainty. Think back to our drug development example. There will never ever ever be a drug that works exactly the same in every person that takes it. People are too variable and many aspects of a person’s life impacts how a drug works in their body. So it would be completely unreasonable to say that a new drug works all the time. However, there can be a drug that improves outcomes for the average person or that this drug is likely to improve outcomes in a randomly selected person who takes it. I THINK THIS EXAMPLE MIGHT BE STUPID. We can create a two by two table for the results of any hypothesis test. In the rows we have the two possible outcomes from our test - fail to reject \\(H_0\\) and reject \\(H_0\\). In the columns we have the true underlying state of nature - either \\(H_0\\) is true or false. \\[ \\begin{tabular}{|l|c|c|} \\hline &amp; \\multicolumn{2}{c|}{True State of Nature} \\\\ \\cline{2-3} Test Result &amp; $H_0$ True &amp; $H_0$ False \\\\ \\hline Fail to reject $H_0$ &amp; \\begin{tabular}[c]{@{}c@{}}Correct\\\\ ($1 - \\alpha$)\\end{tabular} &amp; \\begin{tabular}[c]{@{}c@{}}Incorrect\\\\ Type II Error ($\\beta$)\\end{tabular} \\\\ \\hline Reject $H_0$ &amp; \\begin{tabular}[c]{@{}c@{}}Incorrect\\\\ Type I Error ($\\alpha$)\\end{tabular} &amp; \\begin{tabular}[c]{@{}c@{}}Correct\\\\ $(1 - \\beta)$\\end{tabular} \\\\ \\hline \\end{tabular} \\] In the upper-left and bottom-right cells of the table we are making the correct decision based on our test. When the null hypothesis is true, failing to reject \\(H_0\\) is the correct decision and when the null hypothesis is false, rejecting \\(H_0\\) is the correct decision. However, the other two cells correspond to a mistake being made. Because statisticians are not creative, these mistakes are referred to as type 1 error and type 2 error. A type 1 error is equivalent to a false alarm or a false positive - the null hypothesis was rejected, when in fact it was true. A type 2 error can be thought of as a missed opportunity or a false negative - the null hypothesis was false, but it was not rejected. Typically, the type 1 error rate is symbolically denoted with \\(\\alpha\\) and the type 2 error rate is denoted by \\(\\beta\\) (again statisticians are not creative). If we were looking to create a good hypothesis test, we would want to minimize type 1 and type 2 errors. In other words, if we were to conduct our study over and over again and there was something to be found, we would want to reject the null hypothesis (find something) at a high rate and fail to reject the null hypothesis at a low rate. If there was truly nothing to be found, we wouldn’t want to find anything and if there is something to be found we want to find it. However, there is a tradeoff between type 1 and type 2 error. Let us illustrate this point with a simulation. In this experiment, we are looking to determine if a coin is fair, i.e. the probability of heads is 50%. A type 1 error in this context would be concluding that the coin is not fair, when it actually is. A type 2 error in this context would be concluding that the coin is fair when it is actually not. We simulate flipping a coin 20 times, and you can select a cutoff for how far away from 50% the proportion of heads would need to be for you to not believe that the coin is fair. If the observer proportion of the 20 flips that result in heads is beyond your threshold, the null hypothesis will be rejected. For example, if you choose a threshold of 10%, then any experiment where there are 60% or more flips resulting in heads OR 40% or less flips resulting in heads, then the null hypothesis is rejected and the coin is determined to not be fair. In our first simulation, the coin is truly fair, i.e. \\(H_0\\) is true, so we want to fail to reject \\(H_0\\). The larger we make our cutoff, the more often we fail to reject when the coin is fair. This is intuitive because we are saying that we need more evidence to reject the null hypothesis. flipCoinFair &lt;- function( cutoff) { flipRes &lt;- rbinom(20, 1, 0.5) as.numeric(abs(mean(flipRes) - 0.5) &gt;= cutoff) } tmp &lt;- replicate(1000, flipCoinFair(0.05)) mean(1-tmp) ## [1] 0.325 tmp &lt;- replicate(1000, flipCoinFair(0.1)) mean(1-tmp) ## [1] 0.769 tmp &lt;- replicate(1000, flipCoinFair(0.3)) mean(1-tmp) ## [1] 0.989 However, if the coin is actually not fair then we do want to reject the null hypothesis. But increasing our cutoff makes it less likely that we do so. flipCoinNotFair &lt;- function( cutoff) { flipRes &lt;- rbinom(20, 1, 0.7) as.numeric(abs(mean(flipRes) - 0.5) &gt;= cutoff) } tmp &lt;- replicate(1000, flipCoinNotFair(0.05)) mean(tmp) ## [1] 0.963 tmp &lt;- replicate(1000, flipCoinNotFair(0.1)) mean(tmp) ## [1] 0.777 tmp &lt;- replicate(1000, flipCoinNotFair(0.3)) mean(tmp) ## [1] 0.253 As we have seen, type I and type II errors are not independent. Changing the rejection cutoff to minimize type I errors comes at the cost of more type II errors. 4.3 P-values All hypothesis tests are based on quantifying the probability of the study results assuming the null hypothesis is true. This probability is so important that it has a special name, the p-value. In technical terms, the p-value gives the probability of obtaining results as extreme or more extreme than the ones observed in the sample, given that the null hypothesis is true. A less technical way to describe a p-value is that assuming there is truly nothing going on, what’s the chances of obtaining results similar in opposition to the null hypothesis as our study? Let’s think about this in terms of the coin flipping example. Our null hypothesis is that the coin is fair or, equivalently, that the probability of flipping a heads is 50%. If our experiment involves flipping a coin 10 times, a p-value would give us the probability of observing a proportion of heads as far away or farther away from 50% if the coin was truly fair. Clearly, even if we do have a fair coin, there’s a decent chance of getting 4/10 or 6/10 heads, but what about the chances of getting 0/10 or 10/10 heads? For this simple example, we can actually plot these probabilities. We also could actually calculate the probabilities using what we’ve already learned about the binomial distribution, but we will leave those details to a later chapter. x1 &lt;- 0:4 x2 &lt;- 6:10 prob1 &lt;- 2 * pbinom(x1, 10, 0.5) prob2 &lt;- 2 * pbinom(x2 - 1, 10, 0.5, lower.tail = F) plot(x1, prob1, type = &#39;l&#39;, xlim = c(0, 10), main = &#39;P-value&#39;) lines(x2, prob2) As we can see, and what is hopefully intuitive, the further away our number of heads is, the lower the probability of observing something as extreme or more extreme assuming the coin is truly fair. If we observed 0/10 heads, the p-value is 0.0019. This means if the coin was truly fair, the probability of observing something as extreme or more extreme than 0/10 heads is 0.19%. This is pretty strong evidence that the coin is not fair. By comparison, if we observed 6 heads, the p-value would be 0.7539. If the coin was fair, we have a pretty high chance of observing something as extreme or more extreme than 6 heads - thus we don’t have evidence that the coin is not fair. If we are thinking about hypothesis testing as a court case, p-values are the way that we can quantify the evidence against the defendant. Recall, the prosecution wants to prove beyond a reasonable doubt that the defendant is not innocent. So what does the scientific community consider sufficient evidence? There is a generally agreed-upon scale for interpreting p-values with regards to the strength of evidence that they represent. p-value Evidence against null 0.1 Borderline 0.05 Moderate 0.025 Substantial 0.01 Strong 0.001 Overwhelming Often, the term “statistically significant” is used to describe p-values below 0.05, possibly with a descriptive modifier. “Borderline significant” (p &lt; 0.1) “Highly significant” (p &lt; 0.01) However, don’t let these clearly arbitrary cutoffs distract you from the main idea that p-values represent - how far off is the data from what you would expect under the null hypothesis. A p-value of 0.04 and 0.000000001 are not at all the same thing, even though both are “statistically significant.” In general, a p-value cutoff is chosen and if a p-value below the cutoff is observed, the null hypothesis is rejected. The investigators choose this cutoff, which is equivalent to the type I error rate and thus denoted by \\(\\alpha\\), before analyzing the data. Most of the time \\(\\alpha\\) is set to 0.05, which means there is a 5% chance of a type I error (false alarm). The smaller the value of \\(\\alpha\\), the greater the “burden of proof” required to reject the null hypothesis. \\(\\alpha\\) is also commonly called the significance level. A fundamental property of p-values is that if we use \\(p &lt; \\alpha\\) as cutoff for rejecting the null hypothesis, the type I error rate is guaranteed to be no more than \\(\\alpha\\). However, the \\(p &lt; \\alpha\\) cutoff guarantees us nothing about the type II error rate. This is because p-values are calculated assuming the null hypothesis is true, so they don’t give us any information about what to expect when the null hypothesis is false. While p-values are widely used, have a distinct purpose, and can be informative they also have a number of limitations. DO WE NEED THIS? IS THERE AN APP WE CAN DO ABOUT P_VALUE? "],
["ch13.html", "5 Introduction to Simulations 5.1 What is a simulation? 5.2 Why Simulation 5.3 Chapter Summary", " 5 Introduction to Simulations Learning objectives Understand what a simulation is Understand why simulations are useful Learn the types of illustrations that can be helpful to summarize simulations Conceptual understanding of randomness and a simulation 5.1 What is a simulation? Throughout this text, we will be describing simulations in terms of coins and dice. While these examples may be contrived, they do serve to illustrate the components of probability, randomness, and simulation in a way that is free from unnecessary complication. For example, flipping a coin has exactly two outcomes with equal probability. The goal here is not to bore you with unmotivating examples (“When am I ever going to flip 10,000 coins?”), but rather allow the focus of the discussion to be on the conceptual elements – experiments, events, outcomes, and probabilities. Keeping this in mind will help the reader keep their focus on the relevant information, if at the slight expense of [idk it being boring?] Prior to the advent of computers, nearly all of statistics were done in an analytic fashion, with the use of concise formulas and tedious calculations. However, with the ubiquity of Throughout this text, we will be making heavy use of a tool known as simulation. In the broadest sense, computer simulations allow us to quickly and consistently repeat a random experiment that we might use to form conclusions about a process. Before going into technical details, let’s begin with a simple random experiment that has a single event, the act of flipping a fair coin one time: [[app]] As in a real-life coin flip, we see that we do not get the same result each time. Because we know this is a fair coin, we might say that the probability of landing on heads is 50%, and the probability of landing on tails is 50%. And while we may know for certain that the probability is the same for each outcome, we are no more able to predict what the result of the next flip will be. This is a consequence of randomness, where the outcome is uncertain until it is observed. Processes (such as flipping a coin) in which randomness occurs are known as stochastic processes. In contrast, a process that is not random is known as deterministic, where the outcome of the process is known in advance. An example of a determinsitic process may include writing the numbers 1-10 from largest to smallest. Let’s return now to the coin flip simulation, and investigate ways in which we can make our experiment slightly more complex. In the first implementation, we observed a single event, or possible outcome, from flipping a coin. Letting \\(n\\) represent the number of events, we would say that this simulation was carried out with \\(n = 1\\). In the example below, we repeat the simulation above, but this time, we are able to change the value of \\(n\\): [[app]] Just as before, while having defined \\(n\\) allows us to control the behavior of the simulation, it does not interfere with the effects of randomness. In other words, each time the simulation is run, a different outcome appears. Values such as \\(n\\) that change the behavior of the simulation are known as simulation parameters. We will consider a number of parameters that are commonly used to direct the simulation. Once an outcome is complete and the coins have been flipped, we say that it is observed, or that we have observed the outcome of a random process. Here, it is important to note that an observed outcome is no longer random – once the data have been observed, we know what they are. Data collected through real life experiments, such as testing the efficacy of new drugs, or measuring the height of people in a population, are all observed data. As is often lamented by researchers and statisticians alike, real word experiments can often only be conducted a single time. Once the data is observed, it’s observed. This is in stark contrast to the power of simulations, in which experiments can be conducted and data observed an arbitrary number of times. To see this in action here, consider the previous example in which we introduced the simulation parameter \\(n\\), which allowed us to dictate the number of events, or coin flips, that were done in our experiment. Suppose the experiment that we are interested in involves flipping a coin three times: in this case, we have \\(n = 3\\). When we ran the simulation above, the experiment was done once, as is often the case in the real world, and we were left with a single collection of observed data. It now makes sense to introduce a new simulation parameter, \\(N\\), which dictates how many times the experiment is to be done. In this last instance, our simulation was done with \\(N = 1\\). In the simulation below, we will continue to run the experiment with three coin flips (\\(n = 3\\)), but we will change the number of times \\(N\\) that the experiment is run. [[app]] It is important to note that the experiment itself is unchanged – just as before, we are continuing to investigate what happens when a coin is flipped \\(n = 3\\) times. By adjusting the value of \\(N\\), we are now able to perform this same experiment as often as we wish. Even more importantly, one sees that even though the exact same experiment is performed a number of times, the observed values of the experiment are different. This is again a consequence of randomness, and demonstrates that even when we know all of the details of an experiment, the outcome is uncertain until it is observed. 5.2 Why Simulation In the previous section, we learned that a simulation is a random process (of \\(n\\) events) that can be performed an arbitrary number of (\\(N\\)) times. Our next step will be to understand how this process can empower us to better understand key concepts in statistics. This next section will contain a number of terms and concepts that will be formally introduced in later chapters, but for now, will serve to give context to the usefulness of simulations. We will continue with the experiment above, where we are interested in flipping a fair coin \\(n = 3\\) times. However, along with the experiment, we will also bring a question: if a coin is flipped three times, what is the probability that exactly two of the coins will be heads? Before we attempt answering this via simulation, it will be instructive to consider first the ways we might answer it without. [[might be nice to “name” these methods, like pdf method and something else.]] 5.2.1 Non-simulation Methods As \\(n\\) is small enough, we could simply list all of the possible outcomes of this experiment and count how many of these outcomes have exactly two heads. The space of outcomes for this experiment, where the coin is flipped \\(n = 3\\) times, denoted \\(\\mathcal{S}\\), is given by \\[ \\mathcal{S} = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\} \\] where \\(H\\) represents a heads and \\(T\\) a tails. Having listed them out, we see there are eight possible outcomes. Next, we count how many of those have exactly two heads: \\[ \\mathcal{S} = \\{HHH, \\color{red}{HHT}, \\color{red}{HTH}, \\color{red}{THH}, TTH, THT, HTT, TTT\\} \\] Dividing the events of interest by the total number of outcomes, we see that the probability of getting exactly two heads is \\(P(\\text{# Heads = 2}) = \\frac38\\). Often times, a statistical or stochastic process has an associated probability function that allows us to determine the probability of a set of outcomes. In this case, our coin flipping example follows what is known as a binomial distribution, which is given as \\[ f(k; n,p) = \\binom{n}{k}p^k(1-p)^{n-k} \\] with probability of heads being \\(p = 0.5\\), \\(n = 3\\) total flips, and for our event, \\(k = 2\\) heads. Substituting these numbers, we have \\[ \\begin{align*} P(\\text{# Heads = 2}) &amp;= \\binom{3}{2} (0.5)^2 (0.5)^{3-2} \\\\ &amp;= 3 \\times (0.5)(0.5)(0.5) \\\\ &amp;= 0.375. \\end{align*} \\] Just as in the first method, we find that \\(P(\\text{# Heads = 2}) = 0.375 = \\frac38\\) While each of these methods is satisfactory (and mathematically equivalent), there are a number of situations in which they would be impractical to perform: For example, in the first method, we determined the probability by counting the total number of events of interest (getting two heads) and dividing it by the total number of possible outcomes, in this case eight. For \\(n = 4\\), there are 16 possible outcomes, and for \\(n = 10\\), this number grows to 1024. In fact, the number of possible outcomes grows exponentially with the number of flips: with only \\(n = 25\\) coin flips, there would be more possible outcomes in our solution space than the number of hours between today and the construction of the Great Pyramid of Giza, c.2580-2560 BC! Clearly, we need a method that is a bit more concise Our second method, determining probability via the probability function, is far and away the most popular method, as the computation is simple and immediate. Unfortunatley, we are limited by the crucial fact that we have to know what the probability function is in order to compute it. For many real life processes, such as flipping coins or estimating the number of phone calls a call center will receive in an hour, this function is readily known. For processes that are less simple, these functions can be nearly impossible to construct, leaving statisticians and researchers with no clear way to determine the probability of events. 5.2.2 Simulation Method Before moving forward, let’s quickly recap where we are right now: Simulation is a tool that we have, governed by a set of parameters (in our case, \\(n\\) and \\(N\\)), that can be used to replicate a random experiment an arbitrary number of times. Because this process is stochastic, or random, it will give a different outcome each time We have specified an experiment we would like to perform, flipping a fair coin \\(n = 3\\) times, and we would like to know the probability that exactly two of the flips will land on heads We investigated two methods besides simulation for answering this question: enumerating the event space and calculating the probability function. These methods gave identical answers, and will continue to do so each time they are performed: in other words, they are deterministic We saw that even though enumeration and probability functions worked fine for our problem, they can quickly become impractical or even impossible once the problem grows or becomes more complex A careful reader might note that for a given random experiment (flipping a coin three times) and a specified outcome (exactly two heads), there is only a single correct answer to the question “What is the probability of getting exactly two heads when flipping a coin three times?” and this correct answer is precisely what was found using the deterministic enumeration and probability function methods. And if this is true, how could we possibly expect simulation, which gives us random outcomes, to give us the same correct result? The answer lies in one of the most profound results in all of statistics, the law of large numbers. We will cover this in much greater detail in later chapters, but for now, it suffices to know that the law of larger numbers states: “the average of the probabilities obtained from a large number of trials should be close to the true probability, and will tend to become closer are more trials are performed.” In the context of our problem, we can restate this as “as the number of experiments performed \\(N\\) becomes larger, the average of all the results will get closer and closer to the true result.” Or more clearly still, “our simulation will match determinstic methods when \\(N = \\infty\\).” That’s pretty neat. We will end this chapter with a demonstration of the claims made above. Verify that as \\(N\\) increases, the calculated probability from the simulation matches the analytic results made above. In addition, try changing the experiment and the outcome of interest. This can be done by changing the number of flips \\(n\\), as well as the number of heads we expect to see after \\(n\\) flips. [[app]] 5.3 Chapter Summary In this chapter, we introduced a powerful tool known as simulation. Randomness describes the phenomenon in which a process may give different outcomes each time it is observed In real life, an random experiment is specified, often performed once, and the results are observed A simulation, along with a number of parameters (\\(n\\), \\(N\\)), can be used to perform the same random experiment an arbitrary number of times Probabilities of events can sometimes be computed directly, or with the aid of a probability function. These are deterministic methods of computing probability Often, deterministic methods of computing probability are either impractical or outright impossible By increasing the size of \\(N\\), simulations, which are stochastic, can be used as a very close approximation to the true results "]
]
