<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Probability Distributions | Seriously Interesting Statistics Textbook</title>
  <meta name="description" content="First template!" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Probability Distributions | Seriously Interesting Statistics Textbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="First template!" />
  <meta name="github-repo" content="ceward/introTextbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Probability Distributions | Seriously Interesting Statistics Textbook" />
  
  <meta name="twitter:description" content="First template!" />
  

<meta name="author" content="Caitlin Ward and Collin Nolte" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>
<link rel="next" href="ch6.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>1</b> Acknowledgements</a></li>
<li class="chapter" data-level="2" data-path="ch5.html"><a href="ch5.html"><i class="fa fa-check"></i><b>2</b> Probability Distributions</a><ul>
<li class="chapter" data-level="2.1" data-path="ch5.html"><a href="ch5.html#introduction-to-probability-distributions."><i class="fa fa-check"></i><b>2.1</b> Introduction to Probability Distributions.</a></li>
<li class="chapter" data-level="2.2" data-path="ch5.html"><a href="ch5.html#flipping-coins"><i class="fa fa-check"></i><b>2.2</b> Flipping Coins</a></li>
<li class="chapter" data-level="2.3" data-path="ch5.html"><a href="ch5.html#appropriate-heading-for-section"><i class="fa fa-check"></i><b>2.3</b> appropriate heading for section</a><ul>
<li class="chapter" data-level="2.3.1" data-path="ch5.html"><a href="ch5.html#plotting-the-pmf"><i class="fa fa-check"></i><b>2.3.1</b> Plotting the PMF</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch5.html"><a href="ch5.html#measuring-heights"><i class="fa fa-check"></i><b>2.4</b> Measuring Heights</a></li>
<li class="chapter" data-level="2.5" data-path="ch5.html"><a href="ch5.html#other-common-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Common Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="ch5.html"><a href="ch5.html#poisson-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch5.html"><a href="ch5.html#review-of-distributions"><i class="fa fa-check"></i><b>2.6</b> Review of distributions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch6.html"><a href="ch6.html"><i class="fa fa-check"></i><b>3</b> Sampling Distributions and the <br/> Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.1" data-path="ch6.html"><a href="ch6.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ch6.html"><a href="ch6.html#sampling-distributions"><i class="fa fa-check"></i><b>3.2</b> Sampling Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="ch6.html"><a href="ch6.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.3</b> Central Limit Theorem</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Seriously Interesting Statistics Textbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch5" class="section level1">
<h1><span class="header-section-number">2</span> Probability Distributions</h1>
<div class="objective-container">
<div class="objectives">
Learning Objectives
</div>
<ol style="list-style-type: decimal">
<li>Understand how a distribution represents a random process that creates data that is then observed</li>
<li>Understand how the parameters of a distribution govern how the data is generated [and with what probability]</li>
<li>Be able to identify which distributions underlying a given real world random process.</li>
</ol>
</div>
<p>In the previous chapter, we introduced the idea of a random processes, situations with outcomes that could not be determined perfectly in advance. The idea of a random process can apply to most everything in our lives, from the exact amount of time it takes to go from home to class, to determining the winner of a football game. In each of these cases, the random process is defined in terms of the <em>collection of possible events</em> and their <em>associated probabilities</em>. While the number of unique instances of a random process may be inumerable, we will find that many of them have a very similar underlying structure dictating how these events occur. Formally recognizing the properties of these strucutres, as well as understanding how they can be used to make predictions, motivates the goal of the current chapter.</p>
<div id="introduction-to-probability-distributions." class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction to Probability Distributions.</h2>
<p>Most simply, a <strong>probability distribution</strong> is a method for taking a possible event as input, and giving us the corresponding probability as output. A helpful metaphor is to consider a machine that produces these events at random frequencies corresponding to their associated probabilities. [That is, if our machine only creates green marbles and red marbles, and the probability of producing a green marble is 75%, then <em>on average</em>, our machine will make 3 green marbles for each red one.] In this sense, we can think of distributions as “data generating mechanisms”.</p>
<p>To continue with the machine metaphor, it will be important for us to distinguish between two machines that are completely different, and two machines that are the same but tuned to different settings. These settings, which we call <em>distribution parameters</em>, dicatate many aspects of how the machine will generate data, including the range of likely events, how likely these events are to occur, and how much variability we might expect in the events that we observe. To briefly illustrate, we might first consider a <em>normal distribution</em>, which is governed by two parameters: the mean value, <span class="math inline">\(\mu\)</span>, and the amount of variability, <span class="math inline">\(\sigma^2\)</span>. A plot of two normal distributions is given below:</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As we can see, these two curves are quite similar, and indeed, the underlying process that created each of them is the same. What is different, however, are the parameters governing how the data were generated. With this in mind, we have three goals for the present chapter</p>
<ol style="list-style-type: decimal">
<li>Understand how a distribution represents a random process that creates data that is then observed</li>
<li>Understand how the parameters of a distribution govern how the data is generated [and with what probability]</li>
<li>Be able to identify which distributions underlying a given real world random process.</li>
</ol>
<div class="definition-container">
<div class="definition">
<span id="def:unlabeled-div-1" class="definition"><strong>Definition 2.1  </strong></span>Definitions
</div>
<p><strong>Probability Distribution: </strong> <em> A method for assigning probabilities to all possible events </em></p>
<p><strong>Distribution Parameters: </strong> <em> Values associated with a probability distribution that determine how the data is generated </em></p>
</div>
</div>
<div id="flipping-coins" class="section level2">
<h2><span class="header-section-number">2.2</span> Flipping Coins</h2>
<p>In the previous chapter, we examined the possible events and associated probabilities with flipping a fair coin three times. In particular, we noted the collection of possible events was given by</p>
<p><span class="math display">\[\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\},\]</span>
and the respective probabilities for the number of heads were</p>
<table>
<thead>
<tr class="header">
<th align="center"># Heads</th>
<th align="center">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">3/8</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">3/8</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1/8</td>
</tr>
</tbody>
</table>
<p>In what ways might we formalize this as a process? We might start by identifying a few details about this experiment. First, we know that we are interested in coin flips, where each flip could be either one of two outcomes. We might also recognize that in this experiment we flipped a coin three times, with each flip having an equal probability of being Heads as it did Tails. In light of our previous discussion, which of these properites seem characteristic of a more general underlying process, and which of these could be changed while retaining the more general form? If we had flipped the coin 50 times, would the process be fundamentally different? Would this process be different if the probability of Heads was twice that of Tails?</p>
<p>What we have identified above is a data generating mechanism, or distribution, known as the <em>binomial distribution</em>, in which each outcome is one of two states. In this example, the two states were Heads and Tails, but it could just as easily be described as success vs failure, adverse reaction vs non-adverse reaction, death vs non-death, etc.,. Most generally, we will consider the outcome to be either an “event” or a “non-event”.</p>
<p>We next turn our attention the parameters of the distribution. As you may have guessed, there are two parameters associated with the binomial distribuiton, namely the number of observations (or flips), denoted <span class="math inline">\(n\)</span>, and the probability of a particular outcome being classified as an event, denoted <span class="math inline">\(p\)</span>. In our coin flipping example, flipping the coin three times gives us a parameter value of <span class="math inline">\(n = 3\)</span>. As we were interested in counting the number of Heads, we will call this our “event”, and note that it occurs with probability <span class="math inline">\(p = 0.5\)</span>. Together, these pieces define everything we need to know about a random process that follows a binomial distribution. Notationally, we write</p>
<p><span class="math display">\[ X \sim Bin(n, p)\]</span></p>
<p>or, “the random variable <span class="math inline">\(X\)</span> follows a binomial distribution with <span class="math inline">\(n = 3\)</span> and probability of event <span class="math inline">\(p = 0.5\)</span>”. Our coin flipping example would then be expressed as <span class="math inline">\(X \sim Bin(n = 3, p = 0.5)\)</span>, where <span class="math inline">\(X\)</span> is our experiment.</p>
</div>
<div id="appropriate-heading-for-section" class="section level2">
<h2><span class="header-section-number">2.3</span> appropriate heading for section</h2>
<p>We turn our attention now to the practical problem of determining how we might relate the idea of a probability distribution to determing the actual probabilities of given outcomes. From our definition above, we see that at a minimum, all we need is a method to assign a probability to a given event; within these bounds, we have a number of options available.</p>
<p>Perhaps the most direct method of doing so consists of counting each of the possible outcomes by hand and determining their probabilities, which is precisely what was done when we identified <span class="math inline">\(\mathcal{S}\)</span> and then counted the frequency in which different numbers of Heads occured. This, of course, can become cumbersome quite quickly: with only <span class="math inline">\(n = 3\)</span>, we identified a total of 8 separate outcomes. If <span class="math inline">\(n\)</span> were 4, this would increase to 16. One can quickly see the issue when considering an experiment in which the total number of coin flips was equal to <span class="math inline">\(n = 50\)</span>. This example was further made easier by the fact that each of the outcomes was equally likely; if the value of <span class="math inline">\(p\)</span> was anything but <span class="math inline">\(0.5\)</span>, our task of assigning probabilities to these outcomes would have been significantly more challenging (see Chapter 4).</p>
<p>Another possibility involves the use of simluation, the justification for which is covered in more detail in another chapter. By simulating this experiment a large number of times, we can determine the relative probabilities by counting the relative frequency of each outcome, which saves us the trouble of having to compute them mathematically. Here, we simluate this experiment <span class="math inline">\(N = 10,000\)</span> times, and record the total number of heads in each experiment, dividing by the total number of experiments to get our desired probability</p>
<table>
<thead>
<tr class="header">
<th align="left">Number of Heads</th>
<th align="left">Fraction</th>
<th align="left">Observed Probability</th>
<th align="left">True Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">1,270/10,000</td>
<td align="left">0.127</td>
<td align="left">0.125</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">3,775/10,000</td>
<td align="left">0.3775</td>
<td align="left">0.375</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">3,732/10,000</td>
<td align="left">0.3732</td>
<td align="left">0.375</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">1,223/10,000</td>
<td align="left">0.1223</td>
<td align="left">0.125</td>
</tr>
</tbody>
</table>
<p>Of course as we can see, simulations have their own limitations: they often require a large number of iterations, and because of randomness, the observed probabilities will rarely be exactly equal to the true probabilities. Nonetheless, simulations prove to be exceedingly useful when our experiment is complicated, or if a known distribution function for our problem does not exist.</p>
<p>Our final method for specifying a probability distribution is with the use of a mathematical function, often referred to as a <em>probability distribution function</em> (pdf) or <em>probability mass funciton</em> (pmf), with the former reserved for continuous variables and the latter for those that are discrete. The binomial distribuiton, consisting of discrete outcomes, is specified with a pmf. For <span class="math inline">\(X \sim Bin(n, p)\)</span>, we have the following formula:</p>
<p><span class="math display">\[
P(X = x) = \binom{n}{x} p^x (1-p)^{n-x}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> are our distribution parameters, and <span class="math inline">\(X\)</span> can take any of the values <span class="math inline">\(x = 0, 1, \dots, n\)</span>. Perhaps new to us here is the leading term in the expression above, <span class="math inline">\(\binom{n}{x}\)</span>, called the <em>binomial coefficient</em>, which can be written as</p>
<p><span class="math display">\[
\binom{n}{x} = \frac{n!}{x!(n-x)!}
\]</span>
where <span class="math inline">\(n! = n \times (n-1) \times \dots \times 2 \times 1\)</span> (known as a factorial). In words, we might say <span class="math inline">\(\binom{n}{x}\)</span> as “<span class="math inline">\(n\)</span> choose <span class="math inline">\(x\)</span>”. While this may seem daunting at first, the need for it is quite reasonable. Consider again our coin flipping experiment, where the possible outcomes were listed as</p>
<p><span class="math display">\[
\mathcal{S} = \{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\}.
\]</span></p>
<p>If we are interested in determining the probability of observing two Heads, it is of note that there are a number of instances above in which two Heads occurs. We might ask ourselves, “if we have <span class="math inline">\(n = 3\)</span> flips, how many ways might we <em>choose</em> <span class="math inline">\(x = 2\)</span> heads?” Writing this with our binomial coefficient, we find that</p>
<p><span class="math display">\[
\binom{3}{2} = \frac{3!}{2!(3-2)!} = \frac{3 \times 2 \times 1}{2 \times 1 \cdot(1 \times 1)} = \frac62 = 3,
\]</span>
and indeed, 3 is precisely the number of outcomes in <span class="math inline">\(\mathcal{S}\)</span> in which two Heads occur. Finally, using the pmf above, we can substitute in our distribution paramters <span class="math inline">\(n = 3\)</span> and <span class="math inline">\(p = 0.5\)</span> to find the distribution function that describes our experiment:</p>
<p><span class="math display">\[
P(X = x) = \binom{3}{x} (0.5)^x (1-0.5)^{3-x}
\]</span>
[Exercise: verify that the probabilities returned by the binomial pmf match the true probabilities in the table above by plugging in values for <span class="math inline">\(X = \{0, 1,2,3\}\)</span>. ]</p>
<div id="plotting-the-pmf" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Plotting the PMF</h3>
<p>It is often useful to create a visual representation of a pmf as well. Doing so quickly gives us an idea of where data tend to aggregate and how the data are dispersed. Below are two plots representing two different sets of parameters for the binomials distribution. What do you notice in how they differ? How are they similar? What impacts do the different parameters have on the distribution of the data?</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-6-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Of particular note here, we recall from the previous chapter that the sum of all possible probabilities must be equal to one. Visually, this is represented by the total area of the bars in our plot. Given that our bars our rectangles, we can find the area by considering that the width of each bar is equal to 1, and it’s height is given by the probability of <span class="math inline">\(X = x\)</span>, which can be found using the PMF. On the left hand side, for example, from left to right, we have</p>
<p><span class="math display">\[
\begin{align}
\text{Total Area } &amp;= \text{ \{Area of Heads = 0\} + \{Area of Heads = 1\} + } \\
&amp; \quad \ \ \text{\{Area of Heads = 2\} + \{Area of Heads = 3\}} \\
&amp;= (1 \times P(X = 0)) + (1 \times P(X = 1)) + (1 \times P(X = 2)) + (1 \times P(X = 3)) \\
&amp;= 0.125+0.375+0.375+0.125 \\
&amp;= 1
\end{align}
\]</span>
If, say, we are interested in the probability that <span class="math inline">\(X = 2\)</span> or <span class="math inline">\(X = 3\)</span>, we can add the area of the two corresponding bars. In this case, we find <span class="math inline">\(P(X = 2, 3) = 0.75\)</span>.</p>
<iframe src="https://ph-shiny.iowa.uiowa.edu/collin/textbook/distribution/binomial/" width="100%" height="800">
</iframe>
<p>[Include binomial app here. See that clicking bars adds to something. Explain changing settings, clicking bars, selecting different things. Come up with exercises below, i.e., prob that even if n = 8, prob if even n = 4, etc.]</p>
</div>
</div>
<div id="measuring-heights" class="section level2">
<h2><span class="header-section-number">2.4</span> Measuring Heights</h2>
<p>Often, our data will not fit nicely into a finite number of discrete categories, leaving us with <em>continuous data</em> that are described with a <em>probability distribution function</em>, or <em>pdf</em>. As our data does not fit neatly into categorical bins, many of the techniques described above will not work in the same way here. Rest assured, the idea is exactly the same. While we will spare the technical details here, interested readers may consider what follows to be analagous to the cases presented above when the number of “bins” becomes infinite.</p>
<p>Our motivating example here will consider the process of determining the height of individuals within a population. Unlike the binomial distribution, where the underlying process and associated parameters were easily teased apart, the components here are less obvious, and some care will be needed to identify them. Height data, like many things in the natural world, tend to follow a symmetric distribution, where most observations tend to gather around a mean value, with observations deviating from the mean being equally likely to fall some distance above the mean as below, their frequencies becoming smaller as this distance increases. That is to say, if the mean height of a population is 68 inches, an individual is equally likely to be 67 inches tall as they are 69 inches. Similarly, an individual is equally likely to be 64 inches as they are 82. However, given their proximity to the mean value, an individual is far more likely to be either 67 or 69 inches (1 inch from the mean) than they are to be either 64 or 82 inches (4 inches from the mean).</p>
<p>The process described above describes what is known as a <em>normal distribution</em>, colloquially referred to as a “bell curve”. There are a number of properties that together characterize a normal distribution</p>
<ol style="list-style-type: decimal">
<li>There are two parameters for the normal distribution, the mean <span class="math inline">\(\mu\)</span> (pronounced “myu”) and the variance <span class="math inline">\(\sigma^2\)</span> (“sigma squared”)</li>
<li><span class="math inline">\(\mu\)</span> is the mean, or expected value, and represents the most probable value of the distribution. That is, observations from a normal distribution are more likely to be close to <span class="math inline">\(\mu\)</span> than away from it</li>
<li>Observations are equally likely to be the same magnitude above <span class="math inline">\(\mu\)</span> as they are below it. In other words, the distribution is centered around <span class="math inline">\(\mu\)</span>. We see this concept expressed in everyday language when we offer estimates of some value: “The cost is ‘x’, plus or minus ‘y’”</li>
<li>The second parameter, <span class="math inline">\(\sigma^2\)</span>, describes how concentrated values are around the mean. The smaller the value of <span class="math inline">\(\sigma^2\)</span>, the more observations that will be close to <span class="math inline">\(\mu\)</span>. Likewise, larger values of <span class="math inline">\(\sigma^2\)</span> result in higher dispersion, or more values further away from <span class="math inline">\(\mu\)</span>.</li>
</ol>
<p>Notationally, if a random variable <span class="math inline">\(X\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we write <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. A special case of this that will be explored in following chapters is known as a <em>standard normal distribution</em>, which arises when the mean value is <span class="math inline">\(\mu = 0\)</span>, and the variances is <span class="math inline">\(\sigma^2 = 1\)</span>. This distribution is often writen with its own letter <span class="math inline">\(Z\)</span>, as in <span class="math inline">\(Z \sim N(0, 1)\)</span>.</p>
<p>Another critical difference between a continous and discrete random variable is the way in which we determine probability. In the discrete case, we could enumerate all of the events and determine their relative frequency. Alternatively, we could run a simulation and simply count how often each outcome occured. In the continuous case, however, there is no finite set of possibilities (i.e., somebody could be 68" tall, 68.1" tall, 68.01", …), and any attempts to enumerate these will only terminate in frustration; we will determine the implications of this below. In the meantime, however, we will rejoice in knowing that a normal distribution can be mathematically represented by it’s probability function:</p>
<p><span class="math display">\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}}
\]</span>
As we can see, the pdf contains both of the distribution parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. As we saw at the beginning of the chapter, different values for these parameters gives us different curves:</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Of particular interest above, notice how the value of <span class="math inline">\(\mu\)</span> changes where the data are aggregated, and similarly, note how larger values of <span class="math inline">\(\sigma^2\)</span> results in a greater amount of dispersion.</p>
<p>Let’s now return to the issue of determining the probability of a particular event. In the discrete case, we saw that we could examine the plot of the pmg and multiply the width of each bin (which was equal to 1), with the height of the bin, given by the pmf While the pdf here does indeed give us the “height”, we quickly run into an issue when considering the width: the only way we can have an “infinite” number of bins for each outcome is to assign each bin a width of 0. As such, the probabililty of any particular event is unintuitively assigned a probability of zero.</p>
<p>This apparent shortcoming can thankfully be remedied with the tools of calculus. Where discrete observations allow us to take a sum, the analagous case for continuous intervals is satisfied by the use of the integral. As it no longer makes sense to consider the probability of specific events (all of which will be zero), we instead consider the probability that an observation falls within a <em>range</em> of events. For example, if we have a random variable with <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, the probability that <span class="math inline">\(X &gt; 3\)</span> can be written</p>
<p><span class="math display">\[
P(X &gt; 3) = \int_3^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}} \ dx
\]</span>
Similarly, if we were curious to know the probability that <span class="math inline">\(X\)</span> was between a range of values, say <span class="math inline">\(2 &lt; X &lt; 5\)</span>, we would write</p>
<p><span class="math display">\[
P(2 &lt; X &lt; 5) = \int_2^{5} \frac{1}{\sqrt{2\pi \sigma^2}} \ e^{- \frac{(x-\mu)^2}{2\sigma^2}} \ dx
\]</span>
Fortunately for us today, this no longer need be computed by hand. A number of computational resouces are able to compute this for us with minimal effort.</p>
<p>[Using app below, explore different parameter values. Use slider to select a range of probabilities. Note that the area of interest is highlighted. Do exercises with it]</p>
<iframe src="https://ph-shiny.iowa.uiowa.edu/collin/textbook/distribution/normal/" width="100%" height="800">
</iframe>
<div class="definition-container">
<div class="definition">
<span id="def:unlabeled-div-2" class="definition"><strong>Definition 2.2  </strong></span>Definitions
</div>
<p><strong>Binomial Distribution: </strong> <em> A discrete distribution in which there are two possible outcomes, “events” and “non-events”. There parameters are <span class="math inline">\(n\)</span>, which dictate the number of trials, and <span class="math inline">\(p\)</span>, determining the probability of an event </em></p>
<p><strong>Normal Distribution: </strong> <em> A continuous distribution with two parameters that is symmetric about a mean value, <span class="math inline">\(\mu\)</span>, with a variance <span class="math inline">\(\sigma^2\)</span>. Many real world processes follow a normal distribution. </em></p>
<p><strong>Standard Normal Distribution: </strong> <em> A special case of the normal distribution, <span class="math inline">\(Z \sim N(0, 1)\)</span> </em></p>
<p><strong>Probability Mass Function: </strong> <em> A probability function used for discrete random variables. The probability of outcomes is given as a sum </em></p>
<p><strong>Probability Distribution Function: </strong> <em> A probability function used for continuous random variables. The probabilities of outcomes are taken over a range, given as an integral. </em></p>
</div>
</div>
<div id="other-common-distributions" class="section level2">
<h2><span class="header-section-number">2.5</span> Other Common Distributions</h2>
<p>Having examined in detail both discrete and continuous distributions, demonstrated with the binomial and normal distributions, respectively, we consider below a brief overview of other common distributions and their properites.</p>
<div id="poisson-distribution" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Poisson Distribution</h3>
<p>The Poisson distribution, like the binomial, is a <em>discrete</em> distribution, in that it concerns itself with count data. Specifically, a Poisson distribution describes the number of independent events that may occur within a fixed interval of time. For example, we may be interested in the number of cars that pass through a busy intersection from noon to 1pm every day, or the number of major floods that occur in an area every 100 years. Perhaps the most famous example of the Poisson distribution comes courtesy of Ladislaus Bortkiewicz, a Russian statistician who, in 1898, showed that the number of Prussian soldiers killed by being kicked by a horse in a twenty year period followed a Poisson distribution (also child suicides, but that’s less fun).</p>
<p>The Poisson distribution has a single parameter, <span class="math inline">\(\lambda\)</span>, which describes the rate at which events occur, and a random variable following a Poisson distribution may be expressed as <span class="math inline">\(X \sim Pois(\lambda)\)</span> (People who write <span class="math inline">\(X \sim Po(\lambda)\)</span> are heathens). A random variable following a Poisson distribution has the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>The value of <span class="math inline">\(X\)</span>, being a count, can be any non-negative integer, i.e., <span class="math inline">\(0, 1, 2, \dots\)</span> with no upper bound</li>
<li>The occurence of one event in a time interval is independent of another event. One soldier being kicked by a horse has no impact on the probability of another solider being kicked by a horse.</li>
<li><span class="math inline">\(\lambda\)</span>, which may be any number greater than <span class="math inline">\(0\)</span>, describes the rate at which events occur</li>
<li>[Two events cannot occur at the exact same time, though they probably don’t need this]</li>
</ol>
<p>The distribution function of a Poisson random variable with rate <span class="math inline">\(\lambda\)</span> can be expressed</p>
<p><span class="math display">\[
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}
\]</span>
One surprisingly detail about the Poisson distribution is the relationship between the mean and the variance. For both, we have that <span class="math inline">\(E(X) = Var(X) = \lambda\)</span>.</p>
<div id="plots-for-poisson" class="section level4">
<h4><span class="header-section-number">2.5.1.1</span> Plots for Poisson</h4>
<p>As we look at the plot for the Poisson, we will notice one aspect in particular that distinguishes it from the plots of both the binomial and normal distributions: it is no longer symmetric. This is a consequence of the range of values that a Poisson random variable can take on. Whereas a binomial random variable was bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>, the number of trials conducted, and where the normal distribution allowed any real number, the Poisson is bounded below by <span class="math inline">\(0\)</span>, while having no theoretical upper bound. Given below is a plot of the distribution with <span class="math inline">\(\lambda = 2\)</span> and <span class="math inline">\(\lambda = 4\)</span> (it’s obvious here that choosing a specific value is inadequate. We can replace these plots with distribution exploration apps)</p>
<p><img src="introTextbookTemplate_files/figure-html/unnamed-chunk-8-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Just as with the binomial distribution, we can determine the probability of an event or collection of events by determining the area of the bars in our plot. Below is an interactive app to do stuff. Exercises</p>
<iframe src="https://ph-shiny.iowa.uiowa.edu/collin/textbook/distribution/poisson/" width="100%" height="800">
</iframe>
</div>
</div>
</div>
<div id="review-of-distributions" class="section level2">
<h2><span class="header-section-number">2.6</span> Review of distributions</h2>
<p>Will finish this this weekend. It will include names, continuous/discrete, parmeters, pdf/pmf, mean and variance.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
